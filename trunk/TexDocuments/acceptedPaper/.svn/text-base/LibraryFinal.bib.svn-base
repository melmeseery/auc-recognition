% This file was created with JabRef 2.4.2.
% Encoding: Cp1252

@INPROCEEDINGS{DShonggang2005,
  author = {Zhang honggang and Chen guang and Liu gang and Guo jun},
  title = {Bank Check Image Binarization Based on Signal matching},
  booktitle = {Information, Communications and Signal Processing, 2005 Fifth International
	Conference on},
  year = {2005},
  pages = {1430 -1433},
  month = {0-0 },
  abstract = {In this paper, a method based on signal matching to binarize low signal-noise
	rate bank check image is proposed. This method can extract information
	from the check image interfered with both complex background and
	imprinted seal. With the prior knowledge, the image projection function
	without noise is the source signal, the projection function of image
	binarized by iterative threshold will match the source signal, and
	the threshold which projection function matches best is the optimum
	threshold. Experimental results showed that significant improvement
	in the binarization quality in comparison with other well-established
	algorithms},
  doi = {10.1109/ICICS.2005.1689294},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DShonggang2005.pdf:PDF},
  keywords = {bank check image binarization;image projection function;information
	extraction;iterative threshold;signal matching;signal source;cheque
	processing;feature extraction;image matching;iterative methods;signal
	sources;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{ARMello2008,
  author = {Carlos A.B.Mello and Edward Roe and Everton B.Lacerda},
  title = {Segmentation of Overlapping Cursive Handwritten Digits},
  booktitle = {DocEng '08: Proceeding of the eighth ACM symposium on Document engineering},
  year = {2008},
  pages = {271--274},
  address = {Sao Paulo, Brazil},
  publisher = {ACM},
  abstract = {In this paper, we describe an approach for the problem of segmenting
	overlapping characters. We are working with digit segmentation for
	bank check processing. Our method is based on the idea of a hypothetical
	ball traversing the number. The inertia of the movement segments
	the overlapping digits. Rules are defined for this movement. Our
	initial proposal achieved very good results with O(n2) complexity.},
  comment = {Last edited 2 march 2010},
  doi = {http://doi.acm.org/10.1145/1410140.1410199},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARMello2008.pdf:PDF},
  isbn = {978-1-60558-081-4},
  keywords = {Document processing, segmentation, overlapping digits.},
  location = {New York, NY, USA},
  owner = {TOSHIBA},
  review = {It discribe an segmentation algorithm that seperate two connected
	digits ( and in my opinion characters ) from each other. It make
	use of a hypothetical ball that moves inside the digit from one end
	to the other (makes use of concept that the pen movement is continoues
	and a digit is writen using single stroke). The intertia of movement
	tends to move to location that will minimize change. In other words
	if there is an intersection a ball will tends to stay in same direction
	is was moving before. If there is a change in direction then the
	next movement will be to the location that is nearer to the previous
	one. Experiments showed that the algorithm give good segmentation
	of highly segmented digits. The test were done on generated (artifitially
	) set from isolated digits database. No comparison with other systems.},
  timestamp = {2009.10.29}
}

@CONFERENCE{ARbdAlmageed2009,
  author = {Wael Abd-Almageed and Jayant Kumar and David Doermann},
  title = {Page Rule-Line Removal using Linear Subspaces in Monochromatic Handwritten
	Arabic Documents},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {In this paper we present a novel method for removing page rule lines
	in monochromatic handwritten Arabic documents using subspace methods
	with minimal effect on the quality of the foreground text. We use
	moment and histogram properties to extract features that represent
	the characteristics of the underlying rule lines. A linear subspace
	is incrementally built to obtain a line model that can be used to
	identify rule line pixels. We also introduce a novel scheme for evaluating
	noise removal algorithms in general and we use it to assess the quality
	of our rule line removal algorithm. Experimental results presented
	on a data set of 50 Arabic documents, handwritten by different writers,
	demonstrate the effectiveness of the proposed method},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARbdAlmageed2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@INCOLLECTION{ARAbdelRaouf2008,
  author = {Ashraf AbdelRaouf and Colin A Higgins and Mahmoud Khalil},
  title = {A Database for Arabic Printed Character Recognition},
  booktitle = {Image Analysis and Recognition},
  publisher = {Springer-Verlag Berlin Heidelberg},
  year = {2008},
  volume = {5112},
  series = {Lecture Notes in Computer Science},
  pages = {567-578},
  abstract = {Electronic Document Management (EDM) technology is being widely adopted
	as it makes for the efficient routing and retrieval of documents.
	Optical Character Recognition (OCR) is an important front end for
	such technology. Excellent OCR now exists for Latin based languages,
	but there are few systems that read Arabic, which limits the penetration
	of EDM into Arabicspeaking countries. In developing an OCR system
	for Arabic it is necessary to create a database of Arabic words.
	Such a database has many uses as well as in training and testing
	a recognition system. This paper provides a comprehensive study and
	analysis of Arabic words and explains how such a database was constructed.
	Unlike earlier studies, this paper describes a database developed
	using a large number of collected Arabic words (6 million). It also
	considers connected segments or Pieces of Arabic Words (PAWs) as
	well as Naked Pieces of Arabic Word (NPAWs); PAWS without diacritic
	Background information concerning the Arabic language is also presented.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARAbdelRaouf2008.pdf:PDF},
  keywords = {Character Recognition, Arabic Characters, Database},
  owner = {TOSHIBA},
  review = {Comments:
	
	Printed Text arabic database. 
	
	Discripe the proces of collecting the database and the analysis of
	the dataset.
	
	Details:
	
	Current databases 
	
	1. Collected printed text: 
	
	 a) Linguistic Data Consortium (LDC) at the University of Pennsylvania
	produced “Arabic Gigaword Second Edition” [4]. This is a huge database
	of 1,500 million Arabic words. Problems, only from news, from lebanese
	news only, paragraphs format. 
	
	 b)The Environmental Research Institute o f M ichigan ( ERIM) h as
	created a printed database of 750 pages collected from Arabic books
	and magazines. Problems ( small , hard to access). 
	
	2. Lexical database of printed arabic:
	
	 Arabic lexical database produced by the Euro-Mediterranean project.
	It comprises 119,693 lemmas distributed between nouns, verbs and
	adverbials and uses 6,546 roots
	
	3. Handwritten Arabic database:
	
	 a) In 2002 Al-Ma'adeed et al introduced AHDB. A database of 100 different
	writers which contains Arabic text and words.
	
	 b) In 2002 another handwritten database of town/village names was
	created by the Institute for Communications Technology (IFN).
	
	
	
	The Database of Arabic Words:
	
	The database contains 6 million Arabic words from a wide variety of
	selected sources covering old Arabic, religious texts, traditional
	language, modern language, different specializations and very modern
	material from “chat rooms.” These sources were:},
  timestamp = {2009.10.29},
  url = {http://www.springerlink.com/content/p567835u6l07k418/}
}

@INCOLLECTION{ARAbdulKader2008,
  author = {Ahmad AbdulKader},
  title = {A Two-Tier Arabic Offline Handwriting Recognition Based on Conditional
	Joining Rules},
  booktitle = {Arabic and Chinese Handwriting Recognition},
  publisher = {Springer-Verlag Berlin Heidelberg},
  year = {2008},
  volume = {Volume 4768},
  series = {Lecture Notes in Computer Science},
  pages = {70-81},
  abstract = {Abstract. In this paper we present a novel approach for the recognition
	of offline Arabic handwritten text motivated by the Arabic letters’
	conditional joining rules. A lexicon of Arabic words can be expressed
	in terms of a new alphabet of PAWs (Part of Arabic Word). PAWs can
	be expressed in terms of letters. The recognition problem is decomposed
	into two problems to solve simultaneously. To find the best matching
	word for an input image, a Two-Tier Beam search is performed. In
	Tier One, the search is constrained by a letter to PAW lexicon. In
	Tier Two, the search is constrained by a PAW to word lexicon. The
	searches are driven by a PAW recognizer. Experiments conducted on
	the standard IFN/ENIT database [6] of handwritten Tunisian town names
	show word error rates of about 11%. This result compares to the results
	of the commonly used HMM based approaches.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARAbdulKader2008.pdf:PDF},
  keywords = {Arabic Handwriting, Neural Networks,, IFN/INIT},
  owner = {TOSHIBA},
  review = {Comments:
	
	
	The error of 11%
	
	19% with combined classifier of Convolutional net and directional
	codes},
  timestamp = {2009.10.29}
}

@ARTICLE{ARAbdullah2008,
  author = {Shubair Abdulla and Amer Al Nassiri and Rosalina Abdul Salam},
  title = {Off-line Arabic Handwritten Word Segmentation Using Rational Invariant
	Segments Features},
  journal = {The international Jornal of Information Technology},
  year = {2008},
  volume = {5},
  pages = {200-210},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARAbdullah2008.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.02.23}
}

@INPROCEEDINGS{FEAbdullah2007,
  author = {Abdullah, Siti Norul Huda Sheikh and Khalid, Marzuki and Yusof, Rubiyah
	and Omar, Khairuddin},
  title = {Comparison of Feature Extractors in License Plate Recognition},
  booktitle = {AMS '07: Proceedings of the First Asia International Conference on
	Modelling \& Simulation},
  year = {2007},
  pages = {502--506},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  abstract = {Vehicle license plate recognition has been intensively studied in
	many countries. Due to the different types of license plates being
	used, the requirement of an automatic license plate recognition system
	is different for each country. In this paper, an automatic license
	plate recognition system is proposed for Malaysian vehicles with
	standard license plates using blob labeling and clustering for segmentation,
	seven popular and one proposed edge detectors for feature extraction
	and neural networks for classification.There were eight experiments
	conducted using eight different edge dectectors: Kirsch, Sobel, Laplacian,
	Wallis, Prewitt, Frei Chen and a proposed edge detector. The result
	had shown kirsch edge detectors is the best technique for feature
	exractor while the proposed achieved better results compared to Prewitt,
	Frei Chen and Wallis.},
  doi = {http://dx.doi.org/10.1109/AMS.2007.25},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\FEAbdullah2007.pdf:PDF},
  isbn = {0-7695-2845-7},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@CONFERENCE{ARElAbed2009,
  author = {Haikal El Abed and Volker Margner},
  title = {How to Improve a Handwriting Recognition System},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {The recognition of handwritten characters, words, and text arouses
	great interest today. To develop the best working system is subject
	of many papers published. With this paper, methods to improve the
	performance of existing word recognition systems are discussed. The
	availability of a sufficient data sets for training and testing the
	system assumed, optimization algorithms are presented. The usage
	of different feature sets and the combination of different recognizers
	are proposed. Tests with Arabic handwriting recognition systems using
	the reference IfN/ENIT-database show the usefulness of the proposed
	methods. An improvement of the recognition rate of up to 28% of the
	best single system is achieved.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARElAbed2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@INPROCEEDINGS{ARAbed2007,
  author = {Haikal El Abed and Volker Margner},
  title = {Comparison of Different Preprocessing and Feature Extraction Methods
	for Offline Recognition of Handwritten ArabicWords},
  booktitle = {Proceedings of the Ninth International Conference on Document Analysis
	and Recognition},
  year = {2007},
  volume = {2},
  pages = {974-978},
  abstract = {Preprocessing and feature extraction are very important steps in automatic
	cursive handwritten word recognition. Based on an offline recognition
	system for Arabic handwritten words which uses a semi-continuous
	1-dimensional Hidden Markov Model recognizer, different preprocessing
	combined with different feature sets are presented. The dependencies
	of the feature sets from preprocessing steps are discussed and their
	performances are compared using the IFN/ENIT-database of handwritten
	Arabic words. As the lower and upper baseline of each word are part
	of the ground truth of the database, the dependency of the feature
	set from the accuracy of the estimated baseline is evaluated},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARAbed2007.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARElAbed2007,
  author = {Haikal El Abed and Volker Margner},
  title = {The IFN/ENIT-database - a tool to develop Arabic handwriting recognition
	systems},
  booktitle = {9th International Symposium on Signal Processing and Its Applications,
	ISSPA 2007,},
  year = {2007},
  abstract = {Databases enclosing a huge amount of images of handwritten words together
	with detailed ground truth information are the most important precondition
	for the development of handwritten word recognition systems. The
	IFN/ENIT-database of handwritten Tunisian town names is used by many
	research groups working on recognition systems. This paper gives
	at first a short overview about the most important features of the
	IFN/ENIT-database. In the second part an example of using the data
	for developing baseline estimation methods is given. In the third
	part a recognition system is described and some results are show},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARElAbed2007.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.02.23}
}

@CONFERENCE{ARMargner2009,
  author = {Haikal El Abed and Volker Margner and Monji Kherallah and Adel M.
	Alimi},
  title = {ICDAR 2009 Online Arabic Handwriting Recognition Competition},
  booktitle = {2009 10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {This paper describes the Online Arabic handwriting recognition competition
	held at ICDAR 2009. This first competition uses the ADAB-database
	with Arabic online handwritten words. This year, 3 groups with 7
	systems are participating in the competition. The systems were tested
	on known data (sets 1 to 3) and on one test dataset which is unknown
	to all participants (set 4). The systems are compared on the most
	important characteristic of classification systems, the recognition
	rate. Additionally, the relative speed of the different systems were
	compared. A short description of the participating groups, their
	systems, the experimental setup, and the performed results are presented.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARMargner2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARAburas2008,
  author = {Aburas, Abdurazzag Ali and Gumah, Mohamed E.},
  title = {Arabic handwriting recognition: Challenges and solutions},
  booktitle = {ITSim 2008. International Symposium on Information Technology},
  year = {2008},
  volume = { 2},
  pages = { 1-6},
  address = {Kuala Lumpur, Malaysia},
  month = { 26-28 Aug.},
  organization = {IEEE},
  abstract = {Optical Characters Recognition (OCR) is one of the active subjects
	of research since the early days of computer science. Even if Arabic
	characters are used by more than a half a billion people; Arabic
	characters recognition has not received enough interests by the researchers.
	Little research progress has been achieved comparing to what has
	been done with Latin and Chinese. The cursive nature of the Arabic
	characters makes it more difficult to achieve a high accuracy in
	character recognition since even printed Arabic characters are in
	cursive form. This paper presents the main challenges (difficulties)
	researchers are facing and up to dated solutions (the common methods)
	are used for Arabic text recognition.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARAburas2008.pdf:PDF},
  keywords = {Survey, Arabic Handwriting},
  owner = {TOSHIBA},
  review = {Comments:
	
	Arabic character features and why they are difficult 
	
	
	Details:
	
	 According to Lippman[10]:“Features should contain information required
	to distinguish between classes, be insensitive to irrelevant variability
	in the input, and also be limited in number to permit efficient computation
	of discriminate functions and to limit the amount of training data
	required” (Just note)
	
	
	The main difficulty in Arabic character recognition is due to the
	cursive nature of Arabic writing which doesn’t allow direct application
	of many algorithms designed for other languages.
	
	
	Features of the arabic characters:
	
	1. Arabic is written from right to left
	
	2. Arabic is always written cursively and words are separated by spaces
	
	
	3. The shape of the character is different according to its position
	in the word
	
	4. Fifteen characters have dots with the character.
	
	5. If the following six characters (dal, alf, raa, zaay, thal, waow
	) appearing in a word, will cause the word to be divided PAW (part
	of arbic word).
	
	6. some character contains closed loops, other can be written using
	closed loops ( hah and jeem ..)
	
	
	Research done ==>
	
	preprocessing 
	
	 1. Thining 
	
	 2. Binarization 
	
	 3. Filtering and smoothing 
	
	Segmentation 
	
	 1. histogram is used to define location of spaces (problem in overlapping
	characters)
	
	 2. other latin based segmentation was also used. 
	
	Feature extraction
	
	 1. sturcutral (loops, branch points, peaks) ,
	
	 2. statistical ( histograms , pixel densities, moments ). 
	
	Classification 
	
	 ANN are mainly used in classifications but the system can be divided
	by 
	
	 1) Based on pre-segmented characters
	
	 2) Based on segmenting words into primitives
	
	 3) Based on segmenting words into characters
	
	 4) Based on recognition of words prior to segmentation
	
	 5) Based on recognition without segmentation[6]
	
	 6) Based on Image Compression. No preprocessing (no feature extraction
	and ANN classifications) New approach[3]
	
	
	Difficulties in arabic text.:
	
	 1. letter conectivity 
	
	 2. postion dependent letter shaping ( change of location of letter
	will generate a new shape.). 
	
	 3. different writing styles ( Requah , naskah ... ).},
  timestamp = {2009.10.08}
}

@INPROCEEDINGS{DSAgarwal1995,
  author = {Agarwal, A. and Granowetter, L. and Hussein, K. and Gupta, A. and
	Wang, P.S.P.},
  title = {Detection of courtesy amount block on bank checks},
  booktitle = {Proceedings of the Third International Conference on Document Analysis
	and Recognition, 1995 },
  year = {1995},
  volume = {2},
  pages = {748 -751 vol.2},
  month = {aug},
  abstract = {This paper discusses a technique for locating the courtesy amount
	block on bank checks. In the analysis and recognition process, connected
	components in the image are identified first. Then, strings are constructed
	on the basis of proximity and horizontal alignment of characters.
	Next, a set of rules and heuristics are applied to these strings
	to choose the correct one. The chosen string is only accepted if
	it passes a verification test, which includes an attempt to recognize
	the currency sign. A deterministic finite automaton system is then
	used for segmenting the handprinted courtesy amount. Finally, the
	separated components are passed on to a neural network based recognition
	system },
  doi = {10.1109/ICDAR.1995.602011},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSAgarwal1995.pdf:PDF},
  keywords = {bank checks;courtesy amount block;currency sign;deterministic finite
	automaton;handprinted courtesy amount;neural network based recognition;recognition;segmenting;bank
	data processing;, ;cheque processing;deterministic automata;document
	image processing;finite automata;handwriting recognition;image segmentation;neural
	nets;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@PHDTHESIS{ThesisAlOhali2002,
  author = {Al Ohali, Yousef (2002) Handwritten word recognition : application
	to Arabic cheque processing},
  title = {Handwritten word recognition : application to Arabic cheque processing},
  school = {Concordia University.},
  year = {2002},
  note = {Thesis Advisor:Suen, Ching Y},
  abstract = {This thesis presents a study to process Arabic handwritten cheques.
	It includes the development of a unique set of databases that constitute
	a solid base for research in this domain. The databases are unique
	in terms of their source, domain and tags validation process. First,
	they are originated from real-world bank cheques, which, to the best
	of our knowledge, has never been reached in a university setting.
	Second, it constitutes the only databases in the domain of Arabic
	handwritten cheques so far. To the best of our knowledge, there is
	no database that provides training and testing samples for Arabic
	cheques. Third, it involved a unique tagging validation process that
	takes advantage of the embedded redundancy in the format of the cheque
	to verify the tagging process. A grammar to validate Arabic legal
	amounts and translate them to numerical values is also included.
	This work includes an efficient method to derive one-dimensional
	feature sequence that preserves the dynamics of the original two-dimensional
	images. This is very important to accommodate small variations while
	modeling two-dimensional signals. The thesis provides a detailed
	description of an improved graph representation of sub-word images,
	a more efficient method to extract dynamic information from two-dimensional
	images and a clear positioning of the applicability of other curve-ordering
	criteria, e.g. vision rules. In addition, this thesis includes a
	significant improvement in the discrimination power of HMM which
	allows the differentiation between short sub-words and longer ones
	that share significant initial observation sequences. It also allows
	the HMM to properly classify incomplete observation sequences. The
	improvement is achieved by introducing a new parameter to the HMM
	called the termination probability. Included in this work are tests
	that prove the applicability and efficiency of the above contributions.
	At the time of this dissertation, our survey indicates that this
	work is the only research in the literature which handles images
	of handwritten sub-words extracted from Arabic cheques. The results
	of this study show a 94.36% sub-word recognition rate on the top
	10 choices. Error analysis indicates some errors caused by the pre-processing
	(48%), feature extraction (28%) and classification (24%) modules},
  booktitle = { },
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ThesisAlOhali2002.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@ARTICLE{ARALEMAMI1990,
  author = {SAMIR AL-EMAMI AND MIKE USHER},
  title = {On-Line Recognition of Handwritten Arabic Characters},
  journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,},
  year = {1990},
  volume = {7},
  pages = {704-710},
  number = {12},
  abstract = {Arabic characters are always in cursive script. Handwritten words
	were entered into an IBM PC via a graphics tablet and a segmentation
	process applied to the points; the length and the slope of each segment
	was then found, and the slope categorized to one of four directions.
	In the learning process, specifications of the strokes of each character
	are fed to the computer. In the recognition process, the parameters
	of each stroke are found and special rules applied to select the
	collection of strokes which best matches the features of one of the
	stored characters. The results are promising, and suggestions for
	improvements leading to 100% recognition are proposed.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARALEMAMI1990.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{DSAlMuhtaseb2009,
  author = {Husni A. Al-Muhtaseb and Sabri A Mahmoud and Rami S. Qahwahi},
  title = {A Novel Minimal Script for Arabic Text Recognition Databases and
	Benchmarks},
  journal = {INTERNATIONAL JOURNAL OF CIRCUITS, SYSTEMS AND SIGNAL PROCESSING},
  year = {2009},
  volume = {3},
  pages = {142-153},
  number = {3},
  abstract = {This paper presents a minimal Arabic text that covers
	
	the different basic shapes of Arabic alphabet (viz. standalone, initial,
	
	medial, and terminal). It is designed with minimal repetition of
	
	character shapes in the minimal text. The novelty of the suggested
	
	script could be seen from different perspectives. It enables the
	
	collection of handwritten text from different writers with minimized
	
	effort and time. It is enough for a writer to write three lines of
	
	meaningful Arabic text to cover all possible character shapes, a total
	
	of 125 shapes. The written text is designed to have even distribution
	
	of letter frequencies. This assures enough samples of all character
	
	shapes when text is collected from enough number of writers. The
	
	same is true for printed Arabic text. This is especially useful when
	
	using large number of features with classifiers that require large
	
	number of samples for each category. Hidden Markov Models and
	
	Neural networks are two examples of these classifiers. The use of
	the
	
	minimal text enables proper training, as all Arabic character shapes
	
	are present with adequate frequency, hence resulting in higher
	
	recognition rates. This is not the case with natural text where the
	
	frequency of some Arabic characters differ widely, where in some
	
	cases 100 folds or more. The proposed minimal text may be used to
	
	build a data base of handwritten Arabic text collected of many
	
	writers. This covers the need for a database in the research of Arabic
	
	handwritten text recognition and benchmarking.
	
	In addition, this paper presents statistical analysis of Arabic
	
	corpora for estimating the number of occurrences of the different
	
	shapes of Arabic characters in large corpora. The frequency of Arabic
	
	characters could be used in different applications. In this research
	
	work, it was utilized in enhancing the search for the minimal Arabic
	
	text.},
  booktitle = { },
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSAlMuhtaseb2009.pdf:PDF},
  keywords = {Arabic text recognition, Arabic OCR databases, Minimal Arabic script.},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@ARTICLE{AR3AlMuhtaseb2008,
  author = {Husni A. Al-Muhtaseb and Sabri A. Mahmouda and Rami S. Qahwaji},
  title = {Recognition of off-line printed Arabic text using Hidden Markov Models},
  journal = {Signal Processing},
  year = {2008},
  volume = {88},
  pages = {2902–2912},
  abstract = {This paper describes a technique for automatic recognition of off-line
	printed Arabic text using Hidden Markov Models. In this work different
	sizes of overlapping and nonoverlapping hierarchical windows are
	used to generate 16 features from each vertical sliding strip. Eight
	different Arabic fonts were used for testing (viz. Arial, Tahoma,
	Akhbar, Thuluth, Naskh, Simplified Arabic, Andalus, and Traditional
	Arabic). It was experimentally proven that different fonts have their
	highest recognition rates at different numbers of states (5 or 7)
	and codebook sizes (128 or 256). Arabic text is cursive, and each
	character may have up to four different shapes based on its location
	in a word. This research work considered each shape as a different
	class, resulting in a total of 126 classes (compared to 28 Arabic
	letters). The achieved average recognition rates were between 98.08%
	and 99.89% for the eight experimental fonts. The main contributions
	of this work are the novel hierarchical sliding window technique
	using only 16 features for each sliding window, considering each
	shape of Arabic characters as a separate class, bypassing the need
	for segmenting Arabic text, and its applicability to other languages.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\AR3AlMuhtaseb2008.pdf:PDF},
  keywords = {, Arabic Handwriting, HMM},
  owner = {TOSHIBA},
  review = {Comments:
	
	Different printed Arabic fonts
	
	16 Features extracted from Vertical sliding windows (Mainly number
	of black pixels).
	
	HMM for recognition and segmentation. 
	
	
	Details:
	
	 The database was extracted from the books of Saheh Al-Bukhari and
	Saheh Muslem. The training phase, 2500 lines were used for
	
	training and the remaining 266 lines for testing.
	
	 For each file the text was formatted to appear as a white font color
	in a black background. Moreover, each image in the ‘tif’ file has
	been side reversed through a mirroring tool to speed up the training
	and recognition testing processes. Featrues are extracted from overlapping
	moving vertical window of three-pixel width and a text line of height
	TLH. from each window, 16 different feature are extracted by dividing
	the window into parts of non overlapping 8 parts of 1/8 height, 4
	parts of 1/4 height and overlapping 1/2 height and one whole window.
	From each part, the number of black pixel is computed and used as
	feautres. 
	
	
	The HMM used is in HTK library, its structure allows nonlinear variations
	in the horizontal position. HTK models the feature vector with a
	mixture of Gaussians.
	
	It uses the Viterbi algorithm in the recognition phase, which searches
	for the most likely sequence of a character given the input feature
	vector. 
	
	
	Result on the arabic text was 99.94 %, the method was also used on
	english texts and result was 98.9%},
  timestamp = {2009.10.07}
}

@INPROCEEDINGS{PDB19Madeed2004,
  author = {Somaya Al-M{\'a}adeed and Dave Elliman and Colin Higgins},
  title = {A Data Base for Arabic Handwritten Text Recognition Research.},
  booktitle = {Proceedings of the Eighth International Workshop on Frontiers in
	Handwriting Recognition (IWFHR’02)},
  year = {2004},
  volume = {1},
  number = {1},
  abstract = {In this paper we present a new database for off-line Arabic handwriting
	recognition, together with associated preprocessing procedures. We
	have developed a new database for the collection, storage and retrieval
	of Arabic handwritten text (AHDB). This is an advance both in terms
	of the size of the database as well as the number of different writers
	involved. We further designed an innovative, simple yet powerful,
	in place tagging procedure for our database. It enables us to easily
	extract the bitmaps of words. We also constructed a preprocessing
	class, which contains some useful preprocessing operations. In this
	paper the most popular words in Arabic writing were identified for
	the first time, using an associated program.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://www.iajit.org/ABSTRACTS-1.htm\#06},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB19Madeed2004.pdf:PDF},
  journal = {Int. Arab J. Inf. Technol.},
  owner = {TOSHIBA},
  timestamp = {2009.10.05}
}

@ARTICLE{PDB6AlOhali2003,
  author = {Yousef Al-Ohali and Mohamed Cheriet and Ching Y. Suen},
  title = {Databases for recognition of handwritten Arabic cheques.},
  journal = {Pattern Recognition},
  year = {2003},
  volume = {36},
  pages = {111-121},
  number = {1},
  abstract = {This paper describes an e0ort towards the development of Arabic cheque
	databases for research in the recognition of hand-written Arabic
	cheques. Databases of real-life Arabic legal amounts, Arabic sub-words,
	courtesy amounts, Indian digits, and Arabic cheques are described.
	This paper highlights some characteristics of the Arabic language
	and presents the various steps that have been completed to build
	these databases including segmentation, binarization and data tagging.
	It also describes a solid validation procedure including grammars
	and algorithms used to verify the correctness of the tagging process.
	Detailed descriptions of the database organization and class distribution
	are included. These databases aim to facilitate experimental comparisons
	between various recognition methods, andwill be providedto all interestedresearchers
	upon request to CENPARMI.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://dx.doi.org/10.1016/S0031-3203(02)00064-X},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB6AlOhali2003.pdf:PDF},
  keywords = {Handwritten digits, Arabic Handwriting},
  owner = {TOSHIBA},
  timestamp = {2009.10.03}
}

@ARTICLE{PDB8AlOmari2004,
  author = {Faruq A. Al-Omari and Omar M. Al-Jarrah},
  title = {Handwritten Indian numerals recognition system using probabilistic
	neural networks.},
  journal = {Advanced Engineering Informatics},
  year = {2004},
  volume = {18},
  pages = {9-16},
  number = {1},
  abstract = {This paper presents a system for the recognition of the handwritten
	Indian numerals one to nine (1–9) using a probabilistic neural network
	(PNN) approach. The process involved extracting a feature vector
	to represent the handwritten digit based on the center of gravity
	and a set of vectors to the boundary points of the digit object.
	The feature vector is scale-, translation-, and rotation-invariant.
	The extracted feature vector is fed to a PNN, which in turn classifies
	it as one of the nine digits. A set of experiments were conducted
	to test the performance of the system under different angles between
	the vectors from the centroid to the boundary of the digit object.
	A 308 angle results in a 99.72% recognition rate with a short feature
	vector of 12 entries. This study is meant to be a seed toward building
	a recognition system for Arabic language characters.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://dx.doi.org/10.1016/j.aei.2004.02.001},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB8AlOmari2004.pdf:PDF},
  keywords = {, Handwritten digits, Pattern Recognition, Neural Networks, Probabilistic},
  owner = {TOSHIBA},
  timestamp = {2009.10.03}
}

@CONFERENCE{PBAlaei2009,
  author = {Alireza Alaei and P. Nagabhushan and Umapada Pal},
  title = {Fine Classification of Unconstrained Handwritten Persian/Arabic Numerals
	by Removing Confusion amongst Similar Classes},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {In this paper, we propose two types of feature sets based on modified
	chain-code direction frequencies in the contour pixels of input image
	and modified transition features (horizontally and vertically). A
	multi-level support vector machine (SVM) is proposed as classifier
	to recognize Persian isolated digits. In first level, we combine
	similar shaped numerals into a single group and as result; we obtain
	7 classes instead of 10 classes. We compute 196-dimension chain-code
	direction frequencies as features to discriminate 7 classes. In the
	second level, classes containing more than one numeral because of
	high resemblance in their shapes are considered. We use modified
	transition features (horizontally and vertically) for discriminating
	between two overlapping classes (0 and 1). To separate another overlapping
	group containing three numerals 2, 3 and 4 we first eliminate common
	parts of these digits (tail) and then compute chain code features.
	We employ SVM classifier for the classification and evaluate our
	scheme on 80,000 handwritten samples of Persian numerals [10]. Using
	60,000 samples for training, we tested our scheme on other 20,000
	samples and obtained 99.02% accuracy.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PBAlaei2009.pdf:PDF},
  keywords = {Persian Numeral Recognition, Chain Code, Handwritten, SVM.},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@INPROCEEDINGS{DSAlamri2009,
  author = {Huda Alamri and Chun Lei He and Ching Y. Suen},
  title = {A New Approach for Segmentation and Recognition of Arabic Handwritten
	Touching Numeral Pairs},
  booktitle = {Procedings CAIP (13th International Conference Computer Analysis
	of Images and Pattern s)},
  year = {2009},
  pages = {165-172},
  address = {Münster, Germany},
  month = {Sept. 2009},
  abstract = {In this paper, we propose a new approach on segmentation and recognition
	of off-line unconstrained Arabic handwritten numerals, which failed
	to be segmented with connected component analysis. In our approach,
	the touching numerals are automatically segmented when a set of parameters
	is chosen. Models with different sets of parameters for each numeral
	pair are designed for recognition. Each image in each model is recognized
	as an isolated numeral. After normalizing and binarizing the images,
	gradient features are extracted and recognized using SVMs. Finally,
	a post-processing is proposed by based on the optimal combinations
	of the recognition probabilities for each model. Experiments were
	conducted on the CENPARMI Arabic, Dari, and Urdu touching numeral
	pair databases [1,12].},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSAlamri2009.pdf:PDF},
  keywords = {Numeral pair segmentation, Arabic Digit Recognition, Gradient features.},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{ARAlamri2008,
  author = {Huda Alamri and Javad Sadri and Ching Y. Suen and Nicola Nobile},
  title = {A Novel Comprehensive Database for Arabic Off-Line Handwriting Recognition},
  booktitle = {Eleventh International Conference on Frontiers in Handwriting Recognition},
  year = {2008},
  pages = {664-669, Montreal},
  address = {Montreal, Canada},
  month = {August},
  abstract = {This paper presents the work toward developing a new comprehensive
	database for Arabic off-line handwriting recognition. The database
	includes: isolated Indian digits, numerical strings, Arabic isolated
	letters,i and a collection of 70 Arabic words. Also, the database
	includes a free format sample of an Arabic date. A data entry form
	was designed to collect written samples from Arabic native speakers.
	Our database is advanced in terms of the variety of sets, words and
	number of the participants involved. The databases have been divided
	into respective training, testing and validation sets which will
	be available in the future for the handwriting recognition community.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARAlamri2008.pdf:PDF},
  keywords = {Arabic Handwritten Recognition, Arabic OCR, Handwritten Segmentation,
	Farsi handwritten recognition},
  owner = {TOSHIBA},
  timestamp = {2010.02.15}
}

@INCOLLECTION{ARAli2008,
  author = {Mohamed A. Ali},
  title = {Arabic Handwritten Characters Classification Using
	
	Learning Vector Quantization Algorithm},
  booktitle = {ICISP},
  publisher = {Springer-Verlag Berlin Heidelberg},
  year = {2008},
  pages = {463 – 470,},
  abstract = {In this module, Learning Vector Quantization LVQ neural network is
	first time introduced as a classifier for Arabic handwritten character.
	Classification has been performed in two different strategies, in
	first strategy, we use one classifier for all 53 Arabic Character
	Basic Shapes CBSs in training and testing phases, in second strategy
	we use three classifiers for three subsets of 53 Arabic CBSs, the
	three subsets of Arabic CBSs are; ascending CBSs, descending CBSs
	and embedded CBSs. Three training algorithms; OLVQ1, LVQ2 and LVQ3
	were examined and OLVQ1 found as the best learning algorithm.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARAli2008.pdf:PDF},
  keywords = {Arabic handwritten recognition, Neural Network, Classification},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{AR2Assaleha2009,
  author = {Khaled Assaleha and Tamer Shanablehb and Husam Hajjaj},
  title = {Recognition of handwritten Arabic alphabet via hand motion tracking},
  journal = {Journal of the Franklin Institute},
  year = {2009},
  volume = {349},
  pages = {175-189},
  abstract = {This paperproposesanonlinevideo-basedapproachtohandwrittenArabicalphabetrecognition.
	Various temporalandspatialfeatureextraction techniquesareintroduced.Themotioninformation
	of thehandmovementisprojected ontotwostaticaccumulateddifferenceimagesaccordingtothe
	motion directionality.The temporalanalysisisfollowedbytwo-dimensionaldiscretecosine
	transform andZonalcoding or Radontransformationandlowpassfiltering.Theresultingfeature
	vectors aretime-independentthuscan beclassifiedbyasimpleclassificationtechniquesuchasK
	Nearest Neighbor(KNN).The solutionisfurtherenhancedbyintroducingthenotionofsuperclasses
	where similarclassesaregroupedtogetherforthe purposeofmultiresolutionalclassification.
	Experimental resultsindicateanimpressive99%recognitionrateonuser-dependantmode.To
	validate theproposedtechnique,wehavec onductedaseriesofexperimentsusingHiddenMarkov
	models (HMM),whichistheclassicalwayof classifyingdatawithtemporaldependencies.
	Experimental resultsrevealedthattheproposedfeatureextraction schemecombinedwithsimple
	KNN yieldssuperiorresultstothoseobtainedbytheclassicalHMM-basedscheme.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\AR2Assaleha2009.pdf:PDF},
  keywords = {Arabic Handwriting},
  owner = {TOSHIBA},
  timestamp = {2009.10.07}
}

@INPROCEEDINGS{PDBAthitsos2005,
  author = {Vassilis Athitsos and Jonathan Alon and Stan Sclaroff},
  title = {Efficient Nearest Neighbor Classification Using a Cascade of Approximate
	Similarity Measures.},
  booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern
	Recognition (CVPR 2005)},
  year = {2005},
  pages = {486-493},
  address = {San Diego, CA, USA},
  month = {20-26 June},
  publisher = {IEEE Computer Society},
  abstract = {This paper proposes a method for efficient nearest neighbor classification
	in non-Euclidean spaces with computationally expensive similarity/distance
	measures. Efficient approximations of such measures are obtained
	using the BoostMap algorithm, which produces embeddings into a real
	vector space. A modification to the BoostMap algorithm is proposed,
	which uses an optimization cost that is more appropriate when our
	goal is classification accuracy as opposed to nearest neighbor retrieval
	accuracy. Using the modified algorithm, multiple approximate nearest
	neighbor classifiers are obtained, that provide a wide range of trade-offs
	between accuracy and efficiency. The approximations are automatically
	combined to form a cascade classifier, which applies the slower and
	more accurate approximations only to the hardest cases. The proposed
	method is experimentally evaluated in the domain of handwritten digit
	recognition using shape context matching. Results on theMNIST database
	indicate that a speed-up of two to three orders of magnitude is gained
	over brute force search, with minimal losses in classification accuracy.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://dx.doi.org/10.1109/CVPR.2005.141},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBAthitsos2005.pdf:PDF},
  isbn = {0-7695-2372-2},
  keywords = {MNIST, Classifier Cascade},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@ARTICLE{AR4Awaidah2009,
  author = {Sameh M. Awaidah and Sabri A. Mahmoud},
  title = {A multiple feature/resolution scheme to Arabic (Indian) numerals
	recognition using hidden Markov models},
  journal = {Signal Processing},
  year = {2009},
  volume = {89},
  pages = {1176–1184},
  abstract = {This paper describes a technique for the recognition of optical off-line
	handwritten Arabic (Indian) numerals using hidden Markov models (HMM).
	Features that measure the image characteristics at local, intermediate,
	and large scales were applied. Gradient, structural, and concavity
	features at the sub-regions level are extracted and used as the features
	for the Arabic (Indian) numeral. Several experiments were conducted
	for estimating the suitable number of image divisions, and the best
	combination of features using the HMM classifier. A number of experiments
	were conducted to estimate the best number of states and codebook
	sizes in terms of the highest recognition rate possible. In this
	work, we did not follow the general trend of using the sliding window
	technique with HMM. Instead, a multi-resolution feature extraction
	approach was implemented on the whole digit. A database of 44 writers,
	with 48 samples per digit resulting in a database of 21120 samples
	was used. The achieved average recognition rate is 99%. The classification
	errors were analysed and attributed to bad data, different writing
	styles of some digits, errors between digit pairs, and genuine errors.
	The presented technique, which is writer independent, proved to be
	effective in the automatic recognition of Arabic (Indian) numerals.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\AR4Awaidah2009.pdf:PDF},
  keywords = {Handwritten digits, HMM, Features Extraction},
  owner = {TOSHIBA},
  review = {Comments:
	
	Use different segment size ( segment is part of digit). 
	
	Extract from each segment GSC features(gradient, structural, concativity))
	
	uses HMM in recognition
	
	99.1% result achieved. 
	
	
	Details:
	
	The features were chosen because they are somewhat orthogonal and
	are at different scales to each other. Collectively,these features
	are known as the gradient, structural, and concavity (GSC)feature
	set
	
	
	[ Interesting Dividing digit images into segmetns ... ] The first
	step in the GSC feature extraction algorithm is to divide the imageinto
	nXm grids with equal number of foreground pixels for each of n rows,and
	equal number of foreground pixels for each of m columns. A digit
	sample is segmented into n horizontal segments with approximately
	equal number of black(foreground)pixels in each segment. The system
	then segments the digit into m vertical slices with approximately
	equal number of black (foreground) pixels. the intersection of horizontal
	and verticalsegmentation lines define (n*m) non-overlapping segments
	that are used to extract the features in each segment. the segment
	sizes and x- and y-coordinates are different for each different sample
	based on the sample black(foreground)pixels’ distribution.
	
	
	Three set of features is computed for each segment ( Gradiaent features
	(sobel operator), Structual Featrues ( densisty features , stroke
	features, ...), Concativity shape features). Table 1 in page 1180
	shows the details of each feature. 
	
	
	Different HMM model for each digit and but on same number of states
	for all digits. USing HKT
	
	
	Test on The database consists of 21120 samples.
	
	
	Several expeirments based on segment size and features vector size
	(Recognition rate between 98% to 99%)},
  timestamp = {2009.10.07}
}

@ARTICLE{ARBahlmann2005,
  author = {Claus Bahlmann},
  title = {Directional features in online handwriting recognition},
  journal = {Pattern Recognition},
  year = {2005},
  volume = {39},
  pages = {115 – 125},
  abstract = {Abstract. The selection of valuable features is crucial in pattern
	recognition. In this paper we deal with the issue that part of features
	originate from directional instead of common linear data. Both for
	directional and linear data a theory for a statistical modeling exists.
	However, none of these theories gives an integrated solution to problems,
	where linear and directional variables are to be combined in a single,
	multivariate probability density function. We describe a general
	approach for a unified statistical modeling, given the constraint
	that variances of the circular variables are small. The method is
	practically evaluated in the context of our online handwriting recognition
	system frog on hand and the so-called tangent slope angle feature.
	Recognition results are compared with two alternative modeling approaches.
	The proposed solution gives significant improvements in recognition
	accuracy, computational speed and memory requirements},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBahlmann2005.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@CONFERENCE{ARBall2009,
  author = {Gregory R. Ball and Sargur N. Srihari},
  title = {Semi-supervised Learning for Handwriting Recognition},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBall2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@INCOLLECTION{ARBelaid2008,
  author = {Abdel Belaid and Christophe Choisy},
  title = {Human Reading Based Strategies for Off-Line Arabic Word Recognition},
  booktitle = {SACH},
  publisher = {Springer-Verlag Berlin Heidelberg},
  year = {2008},
  pages = {36–56},
  abstract = {This paper summarizes techniques proposed for off-line Arabic word
	recognition. This point of view concerns the human reading favoring
	an interactive mechanism between global memorization and local verification
	sim- plifying the recognition of complex scripts such as Arabic.
	According to this consideration, specific papers are analyzed with
	comments on strategies.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBelaid2008.pdf:PDF},
  owner = {TOSHIBA},
  review = {Comments :
	
	
	A survey 
	
	Problems
	
	Different methods to solve the arabic handwriting problem. 
	
	
	Details :
	
	They propose a survey based on the functioning of the human perception
	spectrum from coarse ( word level ) to fine ( letter level ) (i.e.
	local, analytical or precise). 
	
	
	Global-based vision classifiers. (word , sub word level...)
	
	
	segmentation-free approach, and even if segmentation is used the information
	are gathered at the word level. 
	
	Systems like Srihari, et al. proposes in [8] a handwritten Arabic
	word recognition system based on a feature vector similarity measure.
	The GSC (Gradient, Structural and Concavity) achieved from 50% to
	70%. 
	
	 Others, detecting a set of shape primitives in the analyzed word
	and arranging them best in the word space. The interpretation of
	each primitive depends on its context, positions, and the posterior
	probability maximization, allowing local misrecognition. Recognition
	rate from 73% to 99% based on quality of text. 
	
	MLP was used in Amin and Mansoor [29] using a set of arabic specific
	features ( number of peaks in sub-word) The recognition rate of 98%
	on different fonts accredits the interest of adapted language specific
	features.
	
	Farah, et al. [10] use a battery of three NNs for word recognition,
	specific features feed each one: statistical, structural, or a mixture.
	The tests on 2,400 word images from 100 different writers achieve
	94.93% recognition rate. 
	
	Pechwitz and Märgner [19] used semi-continuous Hidden Markov Models
	features are collected using a sliding window approach, leading to
	a language-independent features the system obtained 89% word-level
	recognition rate using the IFN/ENIT database. 
	
	
	Semi-global-based vision classifiers.
	
	
	
	Local-based vision classifiers.
	
	Hybrid-level classifiers.},
  timestamp = {2009.10.29}
}

@ARTICLE{ARBeldjehem2009,
  author = {Mokhtar Beldjehem},
  title = {A Granular Framework for Recognition of Arabic Handwriting},
  journal = {Journal of Advanced Computational Intelligence and Intelligent Informatics},
  year = {2009},
  volume = {13},
  pages = {512},
  number = {5},
  abstract = {We propose a novel cognitively motivated unifying framework for Arabic
	handwriting recognition that takes into account the nature of the
	human reading process of Arabic handwriting. This Modular Granular
	Architecture tackles the problem by observing Arabic handwriting
	from both perceptual and linguistic points of view and hence analyzes
	the underlying input signal from different granularity levels. It
	is based on three levels of abstraction: a low granularity level
	that uses perceptual features called global visual indices, a medium
	granularity level that is the conventional recognition stage and
	a high granularity level that consists on morphological analysis
	dedicated to segmentation/recognition. The original idea is the effective
	use of Arabic word’s morphology in the recognition not only in post-processing.
	This architecture carries well around the Arabic word’s morphology,
	as typically in Arabic, the Arabic word’s morphology is by excellence
	the logical structure (even semantic) of a given Arabic word, whereas
	the visual data constitute the physical geometric (topological) structure
	of a given word. We need to integrate both of them for an effective
	cooperative recognition of Arabic Handwriting. This framework subsumes
	the lexicon-driven approaches; in that it can recognize a word that
	does not exist within the lexicon.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBeldjehem2009.pdf:PDF},
  keywords = {granular Arab handwriting recognition, fuzzy and soft computing, morphological
	analysis, cooperative morphological-guided recognition, approximate
	fault tolerant recognizer},
  owner = {Maha},
  timestamp = {2010.02.23}
}

@CONFERENCE{ARBenjelil2009,
  author = {Mohamed Benjelil and Slim Kanoun and Rémy Mullot and Adel M. Alimi},
  title = {Arabic and Latin script identification in printed and handwritten
	types Based on Steerable Pyramid Features},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {Arabic and Latin script identification in printed and handwritten
	nature present several difficulties because the Arabic (printed or
	handwritten) and the handwritten Latin scripts are cursive scripts
	of nature. To avoid all possible confusions which can be generated,
	we propose in this paper an accurate and suitable designed system
	for script identification at word level which is based on steerable
	pyramid transform. The features extracted from pyramid sub bands
	serve to classify the scripts on only one script among the scripts
	to identify. The encouraging and promising results obtained are presented
	in this research paper.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBenjelil2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@ARTICLE{ARBertolami2006,
  author = {Roman Bertolami and Matthias Zimmermann and Horst Bunke},
  title = {Rejection strategies for offline handwritten text line recognition.},
  journal = {Pattern Recognition Letters},
  year = {2006},
  volume = {27},
  pages = {2005-2012},
  number = {16},
  abstract = {This paper investigates rejection strategies for unconstrained offline
	handwritten text line recognition. The rejection strategies depend
	on various confidence measures that are based on alternative word
	sequences. The alternative word sequences are derived from specific
	integration of a statistical language model in the hidden Markov
	model based recognition system. Extensive experiments on the IAM
	database validate the proposed schemes and show that the novel confidence
	measures clearly outperform two baseline systems which use normalised
	likelihoods and local n-best lists, respectively.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://dx.doi.org/10.1016/j.patrec.2006.06.002},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBertolami2006.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.08}
}

@ARTICLE{DSBezerra2008,
  author = {Byron L. D. Bezerra and George D. C. Cavalcanti and Cleber Zanchettin
	and Juliano C. B. Rabelo},
  title = {Detecting and treating invasion in the courtesy amount field on bank
	checks},
  year = {2008},
  abstract = {An approach is proposed for detecting and eliminating invasion in
	courtesy amount fields. This is a important step toward automatizing
	the bank check process. In a real database, 18% of handwritten courtesy
	amount fields exhibited invasions in the legal amount and signature
	fields. Experimental results have shown that the proposed approach
	is robust and efficient for improving the automatic recognition of
	real Brazilian bank checks},
  booktitle = { },
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSBezerra2008.pdf:PDF},
  keywords = {Contour Invasion Detection, Courtesy Amount Recognition, Automatic
	Check Processing.},
  owner = {Maha},
  review = {The system is used to remove the lines and any other parts of words,
	letter, delimiter from different parts of check in the courtsey amounts.
	It detect the invasion using some small rules and some image processing
	methods. It firstly locate the courtsey amount location then tries
	to find limit lines (top and bottom lines ) of the courtesy amount.
	It detect invation by looking for any connected segment that has
	parts either below or above those lines. The removal of the segment
	is done by searching for points that will break segment into two
	sub segment (one is invasion other is digit). The search is done
	depth first using walki in the labyrinth problem. The system is tested
	on real barazillian bank checks and it is found that improve the
	recogntion in the amount with invasions about 15% the result is tested
	with a comercial system and it is viewed to improve final recognition
	with 3% to reach 60.7%.},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{ARBhattacharya2005,
  author = {U. Bhattacharya and B. B. Chaudhuri},
  title = {Databases for Research on Recognition of Handwritten Characters of
	Indian Scripts},
  booktitle = {Proceedings of the 2005 Eight International Conference on Document
	Analysis and Recognition (ICDAR’05)},
  year = {2005},
  abstract = {Three image databases of handwritten isolated numerals of three different
	Indian scripts namely Devnagari, Bangla and Oriya are described in
	this paper. Grayscale images of 22556 Devnagari numerals written
	by 1049 persons, 12938 Bangla numerals written by 556 persons and
	5970 Oriya numerals written by 356 persons form the respective databases.
	These images were scanned from three different kinds of handwritten
	documents – postal mails, job application form and another set of
	forms specially designed by the collectors for the purpose. The only
	restriction imposed on the writers is to write each numeral within
	a rectangular box. These databases are free from the limitations
	that they are neither developed in laboratory environments nor they
	are non-uniformly distributed over different classes. Also, for comparison
	purposes, each database has been properly divided into respective
	training and test sets.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBhattacharya2005.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{PDB10Bhattacharya2003,
  author = {Ujjwal Bhattacharya and B. B. Chaudhuri},
  title = {A Majority Voting Scheme for Multiresolution Recognition of Handprinted
	Numerals.},
  booktitle = {7th International Conference on Document Analysis and Recognition
	(ICDAR 2003)},
  year = {2003},
  volume = {2},
  pages = {16-20},
  address = {Edinburgh, Scotland, UK},
  month = {August},
  publisher = {IEEE Computer Society},
  abstract = {This paper proposes a simple voting scheme for off-line recognition
	of handprinted numerals. One of the main features of the proposed
	scheme is that this is not script dependent. Another interesting
	feature is that it is sufficiently fast for real-life applications.
	In contrast to the usual practices, here we studied the efficiency
	of a majority voting approach when all the classifiers involved are
	multilayer perceptron (MLP) of different sizes and respective features
	are based on wavelet transforms at different resolution levels. The
	rationale for this approach is to explore how one can improve the
	recognition performance without adding much to the requirements for
	computational time and resources. For simplicity and efficiency,
	in the present work, we considered only three coarse-to-fine resolution
	levels of wavelet representation. We primarily simulated the proposed
	technique on a database of off-line handprinted Bangla (a major Indian
	script) numerals. We achieved 97.16% correct recognition rate on
	a test set of 5000 Bangla numerals. In this simulation we used two
	other disjoint sets (one for training and the other for validation
	purpose) of sizes 6000 and 1000 respectively. We have also tested
	our approach on MNIST database for handwritten English digits. The
	result is comparable with state-of-the-art technologies.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://csdl.computer.org/comp/proceedings/icdar/2003/1960/01/196010016abs.htm},
  file = {file:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB10Bhattacharya2003.pdf:PDF},
  isbn = {0-7695-1960-1},
  keywords = {MultiClassifier Systems, MNIST, Arabic Handwriting},
  owner = {TOSHIBA},
  timestamp = {2009.10.04}
}

@CONFERENCE{AROUBAKER2009,
  author = {Houcine BOUBAKER and Monji KHERALLAH and Adel M. ALIMI},
  title = {New Algorithm of Straight or Curved Baseline Detection for Short
	Arabic Handwritten writing},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {In this paper we present a new method of baseline detection of online
	or offline short handwriting. This work is part of a large project
	for the edification of a dual online / offline Arabic handwriting
	recognition system. Compared to the existing approaches in the literature,
	this new method brings three specific novelties: First, the consideration
	of the agreement between the alignment of the points and their trajectory
	tangent directions for the detection of aligned points regroupings.
	Then, the consideration of a topologic characteristics specific to
	the used writing language, to value the pertinence of the pretender
	points regroupings to be recognized as baseline. Finally, we showed
	the aptitude of the algorithm to detect curved baseline.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\AROUBAKER2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@ELECTRONIC{ARBouchain2006,
  author = {David Bouchain},
  year = {2006},
  title = {Character Recognition Using Convolutional Neural Networks},
  howpublished = {Seminar Statistical Learning Theory},
  organization = {University of Ulm, Germany},
  abstract = {Pattern recognition is one of the traditional uses of neural networks.
	When trained with gradient-based learning methods, these networks
	can learn the classification of input data by example. An introduction
	to classifiers and gradient-based learning is given. It is shown
	how several perceptrons can be combined and trained gradient-based.
	Furthermore, an overview of convolutional neural networks, as well
	as a real-world example, are discussed.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBouchain2006.pdf:PDF},
  keywords = {Character Recognition, Neural Networks},
  owner = {TOSHIBA},
  review = {Comments:
	
	
	explains about gradient-based neural learning. 
	
	Explains convolutional neural networks. 
	
	Use lenet5, lenet1 , lenet4 in the as classifier. ( no feature extraction
	as convolutional network works as feature extraction and clasifier).
	
	
	Test on MINST with 0.9%, 1.1 % and 1.7% error rate.},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARBoussellaa2007,
  author = {Wafa Boussellaa and Abderrazak Zahour and Adel Alimi},
  title = {A Methodology for the Separation of Foreground/Background in Arabic
	Historical Manuscripts using Hybrid Methods},
  booktitle = {SAC '07: Proceedings of the 2007 ACM symposium on Applied computing},
  year = {2007},
  pages = {605--609},
  address = {Seoul, Korea},
  publisher = {ACM},
  abstract = {This paper presents a new color document image segmentation system
	suitable for historical Arabic manuscripts. Our system is composed
	of a hybrid method which couple together background light intensity
	normalization algorithm and k-means clustering with maximum likelihood
	(ML) estimation, for foreground/ background separation. Firstly,
	the background normalization algorithm performs separation between
	foreground and background. This foreground is used in later steps.
	Secondly, our algorithm proceeds on luminance and distort the contrast.
	These distortions are corrected with a gamma correction and contrast
	adjustment. Finally, the new enhanced foreground image is segmented
	to foreground/background on the basis of ML estimation. The initial
	parameters for the ML method are estimated by k-means clustering
	algorithm. The segmented image is used to produce a final restored
	document image. The techniques are tested on a set of Arabic historical
	manuscripts documents from the National Tunisian Library. The performance
	of the algorithm is demonstrated on by real color manuscripts distorted
	with show-through effects, uneven background color and localized
	spot.},
  doi = {http://doi.acm.org/10.1145/1244002.1244141},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBoussellaa2007.pdf:PDF},
  isbn = {1-59593-480-4},
  keywords = {Segmentation, restoration, light intensity normalisation, k-means,
	maximum likelihood, foreground/background, Arabic historical color
	manuscript image.},
  location = {New York, NY, USA},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{PDBBritto2004,
  author = {Britto Jr., Alceu de S. and Sabourin, Robert and Bortolozzi, Flavio
	and Suen, Ching Y.},
  title = {Foreground and Background Information in an HMM-Based Method for
	Recognition of Isolated Characters and Numeral Strings},
  booktitle = {IWFHR '04: Proceedings of the Ninth International Workshop on Frontiers
	in Handwriting Recognition},
  year = {2004},
  pages = {371--376},
  abstract = {In this paper we combine complementary features based on foreground
	and background information in an HMM-based classifier to recognize
	handwritten isolated characters and numeral strings. A zoning scheme
	based on column and row models provides a way of dividing the character
	into zones without making the features size variant. This strategy
	allows us to avoid the character normalization, while it provides
	a way of having information from specific zones of the character.
	The experimental results on 10 digit classes, 52 character classes
	and 6 classes of numeral strings of different lengths have shown
	that the proposed features are highly discrimminant.},
  doi = {http://dx.doi.org/10.1109/IWFHR.2004.43},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBBritto2004.pdf:PDF},
  isbn = {0-7695-2187-8},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@INPROCEEDINGS{ARBroumandnia2007,
  author = {A. Broumandnia and J. Shanbehzadeh and M. Nourani},
  title = {Handwritten Farsi/Arabic Word Recognition},
  booktitle = {AICCSA '07. IEEE/ACS International Conference on Computer Systems
	and Applications, 2007.},
  year = {2007},
  publisher = {IEEE},
  abstract = {This paper presents a novel holistic Handwritten Farsi /Arabic Word
	Recognition scheme in situation where we face with word rotation
	and scale change. Image words features are extracted by exploiting
	rotation and scale invariance characteristics of M-Band packet wavelet
	transform performed on polar transform version of images of handwritten
	Farsi/Arabic words. The extracted features construct a feature vector
	for each word image. This vector is employed in recognition phase
	by finding the similar words based on the least Mahalanobis distance
	of feature vectors. This scheme is robust against rotation and scaling.
	Experimental results, obtained from testing different handwritten
	texts with various orientations and scales, show that proposed scheme
	outperforms Fourier-wavelet and Zernike moments algorithms. The robustness
	of new scheme has been tested with images corrupted by Gaussian noise
	and compared with similar schemes. Experimental results show that
	the accuracy of our algorithm reaches 95.8 percents.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBroumandnia2007.pdf:PDF},
  keywords = {Pattern Recognition, Farsi/Arabic Handwritings Recognition, Wavelet
	Transform},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{ARBroumandnia2008,
  author = {A. Broumandnia and J. Shanbehzadeh and M. Rezakhah Varnoosfaderani},
  title = {Persian/arabic handwritten word recognition using M-band packet wavelet
	transform},
  journal = {Image and Vision Computing},
  year = {2008},
  volume = {26},
  pages = {829–842},
  abstract = {The extraction of rotation and scale invariant features is an essential
	problem in document image analysis. This paper proposes an effective
	rotation and scale invariant holistic handwritten word recognition
	scheme. This approach utilizes M-band packet wavelet transform to
	extract feature vector of Farsi word image. The global and local
	features extracted are exploited in recognition of limited-size lexicon
	of handwritten words. The rotation and scale invariant feature of
	a word image involves applying a polar transform to eliminate rotation
	and scale effects, but this produces M-row shifted polar image, which
	is passed to a row shift invariant M-band wavelet packet transform
	to eliminate the row shift effects. The output wavelet coefficients
	are rotation and scale invariant. For each subband of these wavelet
	coefficients a set of local energy features are computed and we extract
	feature vectors from the subbands of wavelet coefficients. The proposed
	polar M-band wavelet features have been tested by employing Mahalanobis
	algorithm to classify a set of distinct natural handwriting Farsi
	words. We compared the proposed scheme with two well-known rotation
	invariant methods; Fourier-wavelet and Zernike moments. The experimental
	results show that the proposed algorithm improves the recognition
	rate about 12 percents.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBroumandnia2008.pdf:PDF},
  keywords = {Holistic word recognition, Farsi handwritings, M-band packet wavelet,
	Row shift invariant},
  owner = {Maha},
  timestamp = {2009.10.29}
}

@MASTERSTHESIS{ThBurrow2004,
  author = {Peter Burrow},
  title = {Arabic Handwriting Recognition},
  school = {School of InformaticsUniversity of Edinburgh},
  year = {2004},
  abstract = {This thesis explores a number of different techniques for use in the
	field of Arabic Handwriting Recognition. A review of previous work
	in the field is conducted, and then various techniques are explored
	in the context of classifying town names from the IFN/ENIT database.
	A baseline-finding algorithm using Principal Components Analysis
	is implemented, and the change in performance from reducing the influence
	of certain word features is also demonstrated. Several simple methods
	of town name classification are investigated, including a scheme
	using Tangent Features. These model the variations in the training
	examples in order to improve generalisation, and perform with 94%
	accuracy on a small 10-class lexicon. Moment invariants are considered
	as useful features for classification, but fail to surpass the performance
	of simpler methods. An approach where town names are split into parts
	and traced to recover temporal information is conceived, and found
	to have encouraging performance and several useful properties.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ThBurrow2004.pdf:PDF},
  keywords = {Arabic Handwriting},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@CONFERENCE{ARDC2009,
  author = {Shubhangi D C and P.S. Hiremath},
  title = {Multi-Class SVM Classifier for English Handwritten Digit Recognition
	using Manual Class Segmentation},
  booktitle = {International Conference on Advances in Computing, Communication
	and Control (ICAC3’09)},
  year = {2009},
  abstract = {A new method for recognition of isolated handwritten English digits
	is presented here. This method is based on Support Vector Machines
	(SVMs). Mean and standard deviation of each digit is considered as
	the features. Using these features, multiple SVM classifiers are
	trained to separate different classes of digits. Support vector machine
	are based on the concept of decision planes that defines the decision
	boundaries. The decision plane is one that separates between the
	set of digits having different class membership. The approach works
	in four steps 1) Preprocessing 2) Feature extraction 3) Classification
	4) detection. A database of 100 different representation of each
	digit is constructed for the training database. The digits are first
	manually segmented into 5 classes to minimize the time required to
	obtain the hyperplane. Then the input is again check against the
	two classes by 2-class SVM classifier. Experiments show that the
	proposed features can provide a very good recognition result using
	Support Vector Machines at a recognition rate 97%, compared with
	91.25% obtained by MLP neural network classifier using the same features
	and test set.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARDC2009.pdf:PDF},
  journal = {International Conference on Advances in Computing, Communication
	and Control (ICAC3’09)},
  keywords = {Multi-class SVM classifier, English handwritten digits, structural
	features},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{DSChandra2008,
  author = {Chandra, L. and Gupta, R. and Kumar, P. and Ganotra, D.},
  title = {Automatic courtesy amount recognition for Indian banks checks},
  booktitle = {TENCON 2008 - 2008 IEEE Region 10 Conference},
  year = {2008},
  pages = {1 -5},
  month = {nov.},
  abstract = {Preprocessing is an important step for automatic check processing
	in Indian scenario where there is huge variation in writing style,
	especially the way in which Courtesy Amount is terminated and the
	fractional (Paisa) amount is written. Courtesy Amount Recognition
	(CAR) and Legal Amount Recognition (LAR) form the core of automated
	check processing system. For CAR identification, a number of approaches
	have been suggested. However, most of these do not provide strong
	and comprehensive preprocessing techniques. We propose an algorithm
	that automatically detects the courtesy amount region, preprocesses
	this region, segments the courtesy amount into individual characters
	before feeding it to an ICR engine. For detecting the courtesy-amount
	region, a Most Probable Region (MPR) is detected, based on configurable
	rules and semantic analysis. The presented algorithm intelligently
	removes currency symbols (dasiaRs.psila), terminal characters (dasia/psila,
	dasia=psila, dasia/-psila), and delimiters (dasia,psila, dasia.psila)
	with a high degree of accuracy. Results show that the ICR rates have
	increased from 51% to 90% using our algorithm.},
  doi = {10.1109/TENCON.2008.4766626},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSChandra2008.pdf:PDF},
  keywords = {automatic bank check processing;courtesy amount recognition;handwriting
	recognition;legal amount recognition;most probable region detection;bank
	data processing;cheque processing;handwriting recognition;},
  owner = {Maha},
  review = {The system recognize the courtesy amount from the bank check and then
	feed them to a digit recognizier. The system starts by detecting
	the most probable region (MBR) for courtesy amount using configuralble
	ruls and scemantic analysis. some morpholigical operation applied
	to region to enhance the the extraction . The MBR region will then
	contain the currency and digit amount which is procesed to remove
	the currency and other seprators. A final step is added if there
	was a fractial amount added in the number then the split line is
	detected and removed and the fraction part is processed alone. The
	final part is noise removing and segmentation of digits based on
	histogram analysis. The whole system is based in image processing
	method and rules based on experiments of casses and location of each
	element. The final recognition of all the system is 89% of the whole
	amount with no comparision to similar systems. The database used
	was 4777 images of indian bank checks .},
  timestamp = {2010.2.24}
}

@ARTICLE{MCChen2009,
  author = {Jin Chen and Cheng Wang and Runsheng Wang},
  title = {Adaptive binary tree for fast SVM multiclass classification},
  journal = {Neurocomputing},
  year = {2009},
  volume = {72},
  pages = {3370–3375},
  abstract = {This paper presents an adaptive binary tree (ABT) to reduce the test
	computational complexity of multiclass support vector machine (SVM).
	It achieves a fast classification by: (1) reducing the number of
	binary SVMs for one classification by using separating planes of
	some binary SVMs to discriminate other binary problems; (2) selecting
	the binary SVMs with the fewest average number of support vectors
	(SVs). The average number of SVs is proposed to denote the computational
	complexity to exclude one class. Compared with five well-known methods,
	experiments on many benchmark data sets demonstrate our method can
	speed up the test phase while remain the high accuracy of SVMs.},
  doi = {http://dx.doi.org/10.1016/j.neucom.2009.03.013},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCChen2009.pdf:PDF},
  keywords = {Binary tree, Computational complexity, SVM, MultiClassifier Systems},
  owner = {TOSHIBA},
  review = {The paper describe a new method ( an improvement on the binary tree
	svm) to change binary SVM into Multi class classification. 
	
	
	The main point is decrese number of SVM needed to get classification
	at test phase. Even though it trains same number of classifiers as
	OVO it may requires less than the [ n(n-1) / 2 ] test the OVO needs.
	The computational complexity of the system is improved.},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{PDBChen2005,
  author = {Xiangrong Chen and Alan L. Yuille},
  title = {A Time-Efficient Cascade for Real-Time Object Detection: With applications
	for the visually impaired},
  booktitle = {Proceedings of the 2005 IEEE Computer Society Conference on Computer
	Vision and Pattern Recognition (CVPR’05)},
  year = {2005},
  abstract = {Real-time object detection is essential for many computer vision applications.
	Many rapid detection algorithms are based on using cascades of tests.
	But existing design criteria for cascades either ignore the time
	complexity of the tests or make over-simplified assumptions about
	them. This paper gives a criterion for designing a time-efficient
	cascade that explicitly takes into account the time complexity of
	tests (as evaluated by computer run time) including the time for
	pre-processing. We design a greedy algorithm to minimize this criterion
	(noting that the full problem is NP-complete). Finally, we illustrate
	our method on the task of text detection in city scenes. This gives
	a text detection algorithm that runs at 0.025 seconds per 320×240
	image, which is equivalent to 40 frames per second. This is a speed
	up factor of 2.5 compared to our previous text detector. It gives
	a realtime system which can be used for applications to help the
	blind and visually impaired.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBChen2005.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.21}
}

@INPROCEEDINGS{PDB7Chen2004,
  author = {Yangchi Chen and Crawford, M.M. and Ghosh, J.},
  title = {Integrating support vector machines in a hierarchical output space
	decomposition framework},
  booktitle = {Geoscience and Remote Sensing Symposium, 2004. IGARSS '04. Proceedings.
	2004 IEEE International},
  year = {2004},
  volume = {2},
  pages = {949- 952},
  month = {Sept},
  publisher = {IEEE International},
  abstract = {This paper presents a new approach called Hierarchical Support Vector
	Machines (HSVM), to address multi-class problems. The method solves
	a series of max-cut problems to hierarchically and recursively partition
	the set of classes into two-subsets, till pure leaf nodes that have
	only one class label, are obtained. The SVM is applied at each internal
	node to construct the discriminant function for a binary meta-class
	classifier. Because max-cut unsupervised decomposition uses distance
	measures to investigate the natural class groupings, HSVM has a fast
	and intuitive SVM training process that requires little tuning and
	yields both high accuracy levels and good generalization. The HSVM
	method was applied to Hyperion hyperspectral data collected over
	the Okavango Delta of Botswana. Classification accuracies and generalization
	capability are compared to those achieved by the Best Basis Binary
	Hierarchical Classifier, a Random Forest CART binary decision tree
	classifier and Binary Hierarchical Support Vector Machines.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB7Chen2004.pdf:PDF},
  keywords = {SVM},
  owner = {TOSHIBA},
  timestamp = {2009.10.03}
}

@INCOLLECTION{DSCheriet2008,
  author = {Mohamed Cheriet},
  title = {Visual Recognition of Arabic Handwriting: Challenges and New Directions},
  booktitle = {Arabic and Chinese Handwriting Recognition},
  publisher = {Springer Berlin / Heidelberg},
  year = {2008},
  volume = {4768},
  series = {Lecture Notes in Computer Science},
  pages = {1-21},
  abstract = {Automatic recognition of Arabic handwritten text presents a problem
	worth solving; it has increasingly more interest, especially in recent
	years. In this paper, we address the most frequently encountered
	problems when dealing with Arabic handwriting recognition, and we
	briefly present some lessons learned from several serious attempts.
	We show why morphological analysis of Arabic handwriting could improve
	the accuracy of Arabic handwriting recognition. In general, Arabic
	Natural Language Processing could provide some error handling techniques
	that could be used effectively to improve the overall accuracy during
	post-processing. We give a summary of techniques concerning Arabic
	handwriting recognition research. We conclude with a case study about
	the recognition of Tunisian city names, and place emphasis on visual-based
	strategies for Arabic Handwriting Recognition (AHR).},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSCheriet2008.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSCheriet2007,
  author = {Cheriet, M.},
  title = {Strategies for visual arabic handwriting recognition: Issues and
	case study},
  booktitle = {9th International Symposium on Signal Processing and Its Applications,
	2007. ISSPA 2007.},
  year = {2007},
  abstract = {Automatic recognition of Arabic handwritten text is a problem worth
	solving; it gained more and more interest in recent years, for various
	reasons. In this paper we address the most frequently encountered
	problems when dealing with Arabic handwriting recognition and we
	briefly present lessons learnt from several serious attempts that
	have been undertaken in this regard. We give a summary of techniques
	concerning Arabic handwriting recognition research. We will end by
	a case study, recognition of Tunisian city names, emphasizing on
	visual-based strategies for Arabic handwriting recognition (AHR).},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSCheriet2007.pdf:PDF},
  journal = {Signal Processing and Its Applications, 2007. ISSPA 2007. 9th International
	Symposium on},
  keywords = {Arabic handwriting recognition , Arabic handwritten text recognition
	, Tunisian city name recognition , automatic recognition , visual-based
	strategies},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INCOLLECTION{ARCheriet2007,
  author = {M. Cheriet and Y. Al-Ohali and N.E. Ayat and C.Y. Suen},
  title = {Arabic Cheque Processing System: Issues and Future Trends},
  booktitle = {Digital Document Processing},
  publisher = {Springer London},
  year = {2007},
  series = {Advances in Pattern Recognition},
  pages = {213-234},
  abstract = {From the administrative point of view, cheque processing involves
	all tasks a bank officer may perform to process an incoming cheque
	for a client. This includes: accessing account numbers, verifying
	names and signatures on the cheque, verifying the date of the cheque,
	matching the legal amount with the courtesy amount and verifying
	the credit of the cheque writer. However, from the technical point
	of view, cheque processing could involve capturing the cheque image,
	separating the foreground of the cheque from its background, extracting
	fields of interest and recognizing each of them. This work employs
	theories and methodologies from various fields ranging from Natural
	language processing, Optical Character Recognition to Banking. 
	
	The motivation of the work on Cheque processing is not less than the
	motivation of the entire research in artificial intelligence, which
	aims to program the computer to carry out tedious routine processes,
	freeing time and space for humans to perform tasks that require higher
	levels of intelligence. A major advantage of such study is that it
	can be easily adjusted to serve more than 20 different countries
	(all of them use Arabic as their first language). In addition, legal
	amounts are widely found in documents other than bank cheques (e.g.
	business sell/purchase forms). Therefore, this study will be applicable
	to a wide range of applications. Moreover, similar languages (e.g.
	Urdu, Farisi) which use the same alphabet can benefit from these
	studies.
	
	 The remaining sections provide a description of datasets available
	for researchers as well as a detailed description of one system that
	processes legal amounts and one system dedicated for processing of
	courtesy amount},
  doi = {http://dx.doi.org/10.1007/978-1-84628-726-8_10},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARCheriet2007.pdf:PDF},
  keywords = {Arabic Handwriting, Cheque processing},
  owner = {TOSHIBA},
  review = {Comments:
	
	Cheques processing. legal amount, and the numerals. 
	
	PAW based recognition. 
	
	HMM for word recognition.
	
	A arabic digit was also desciped using SVM and NN and some sturctural
	features.
	
	
	Contains details of a cheque system and analysis of other systems:
	
	Details of 
	
	1) Datasets. 
	
	 Due to strict banking rules to protect their customers, it is extremely
	difficult to gain access to real cheques.
	
	
	 Real data, however, has a number of disadvantages, one must note.
	First, researchers have no control on the number of samples from
	each class, as this would be determined by the actual distribution
	of the classes and by the sampling bias. Another point that some
	may regard is the exposure of natural carelessness by some people
	in the society when filling or handling cheques
	
	
	The system contributed with a new database that we built in collaboration
	with Al-Rajihi Bank in Riyad, Saudi Arabia [2]. These datasets include:
	legal amount dataset (containing 2499 legal amounts), courtesy amount
	dataset (containing 2499 courtesy amounts and written in Hindi digits),
	Arabic sub-word dataset (containing 29,498 sub-words within the domain
	of legal amount) and Indian digit dataset (containing 15,175 digits).
	
	
	2) Legal amount processing ( the text of the cheque) 
	
	I is divided in to 
	
	 a)Preprocessing 
	
	 First, the legal amount is statically segmented from the cheque form.
	Dynamic thresholding (based on average of and std of overall compaonents
	in legal amount)is then applied to binarize the extracted legal amount.
	
	 Basic filling and thinning operations are applied next to enhance
	the image of the legal amount.
	
	 Baseline is detected using and its thickness using horizontal projections.
	Slant correction is then applied by computing the density of the
	baseline in various angles. The minimum slant of the image will produce
	the maximum baseline density.
	
	 Removal of noise and undesirable object using one of the following
	factors: ===>[ Inter-component distances, Vertical position of components,
	Size of each component, Slant]
	
	
	 B) word sub word processing
	
	 To facilitate easier extraction of analytical features, skeletonization
	is applied to the input sub-word image
	
	 Pen-trajectory is then estimated by finding the most efficient traversal
	of all edges in the transformed tree Pen-trajectory is then estimated
	by finding the most efficient traversal of all points in the skeleton.
	
	
	 Sequential observations are detected from the input 2D image based
	on an estimation of the original writing sequence of strokes (pen
	trajectory). 
	
	 Finally, HMM is used to provide probabilistic similarity measures
	based on sequential observations
	
	 Clustring : The goal of this process is to partition each class into
	clusters that share similar feature vectors. Each class is then represented
	by a single HMM model. Clustering is performed on the extracted sequence
	of features [For example different wasy of writing the (arabic: hah)
	from different users or fonts... ]. (The total number of clusters
	used was 150, representing 67 different classes.)
	
	
	Classification:
	
	 A left-to-right HMM is used to model each cluster. Models are trained
	using the Baum–Welch algorithm. Each model is trained only to feature
	vectors that belong to it. Therefore, each model learns to produce
	high probability to similar inputs, but does not learn to produce
	low probabilities to different inputs. This is known as the maximum
	likelihood (ML) learning criteria
	
	
	
	Performance
	
	
	Recogntion rate of Testing 73.53% on top 1 81.50 % on top 3 
	
	( the paper gives full error analysis as error can be in ( noise in
	input, binarization steps, Imperfect skeletonization, feature extraction
	module, linear approximation, classification )
	
	
	Digit recognition (arabic )
	
	A decision system for the classification of handwritten non-touching
	Indian digits is described. It includes a feature extraction module
	and a classification module. The used characteristics are based on
	coding some morphological patterns through the use of histograms
	of belonging pixels, and few contour based statistical features.
	
	Two classification models are experimented. A neural network classifier
	and an SVM based system. SVM with 1.2 % error and NN with the best
	recognition rate on the testing set is 96.69%. The error and rejections
	rates are, respectively, 1.99% and 1.21%.. ( Test on The INDCENPARMI
	database)},
  timestamp = {2009.10.08}
}

@ARTICLE{ARCheriet2009,
  author = {Mohamed Cheriet and Horst Bunke and Jianying Hu and Fumitaka Kimura
	and Ching Y. Suen},
  title = {New Frontiers in Handwriting Recognition},
  journal = {Pattern Recognition},
  year = {2009},
  volume = {42},
  pages = {3129-3130},
  number = {12},
  abstract = {After more than 20 years of continuous and intensive effort devoted
	to solving the challenges of handwriting recognition, progress in
	recent years has been very promising. Those challenges are now considered
	to constitute a millennium problem (cf. addendum p. xx).
	
	
	On top of the excitement generated by the ground-breaking advances
	already made in this field can be added the significant contributions
	described in this special issue, which were presented at the ICFHR
	2008 Conference held in Montreal from August 18 to 21, 2008. Of the
	161 submissions received for this conference from 26 countries, 118
	papers were selected following the review process, 39 for oral presentation
	and 79 for poster presentation. The best 26 papers were then selected
	for extensive and thorough revision through peer review, and subsequent
	publication in this Special Issue on Handwriting Recognition of the
	Journal, Pattern Recognition.
	
	
	In addition, in this first edition of the ICFHR promoted as an international
	conference, a panel discussion was organized as a key feature to
	highlight advances in the field of handwriting recognition, which
	was chaired by M. Cheriet. Panelists M. El Yacoubi, H. Fujisawa,
	D. Lopresti, and G. Lorette talked about the achievements of those
	20 years of handwriting recognition and about its future. Following
	these outstanding presentations, conference attendees had the opportunity
	to air their questions and concerns during open and fruitful exchanges.
	Biographic details on the panelists, along with highlights of the
	discussions, are included in an addendum at the end of this editorial.
	
	
	
	
	(vi) Forensics (2),
	
	
	(vii) Historical documents (3), and
	
	
	(viii) Applications (3).},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  doi = {http://dx.doi.org/10.1016/j.patcog.2009.03.013},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARCheriet2009.pdf:PDF},
  owner = {TOSHIBA},
  review = {In order to provide comprehensive content for this special issue,
	we have classified the 26 papers into the following broad topics,
	an arrangement which corresponds to their mapping to conference sessions:
	
	
	(i) Segmentation (4),
	
	(ii) Offline recognition (2),
	
	
	(iii) Online recognition (2),
	
	
	(iv) Classification decision theory (6),
	
	
	(v) Multilingual recognition (4),
	
	(vi) Forensics (2),
	
	
	(vii) Historical documents (3), and
	
	
	(viii) Applications (3).},
  timestamp = {2009.10.08}
}

@ARTICLE{ARCheriet2009b,
  author = {Mohamed Cheriet and MounimElYacoubi and Hiromichi Fujisawa and Daniel
	Lopresti and Guy Lorette},
  title = {Handwriting recognition research:Twenty years of achievement...and
	beyond},
  journal = {Pattern Recognition},
  year = {2009b},
  volume = {42},
  pages = {3131-3135},
  number = {12},
  month = {December 2009},
  abstract = {CFHR 2008 Panel Discussion},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARCheriet2009b.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INCOLLECTION{ARCho2007,
  author = {Sung-Jung Cho and Jin Hyung Kim},
  title = {A Bayesian Network Approach for On-line Handwriting Recognition},
  booktitle = {Digital Document Processing Major Directions and Recent Advances},
  publisher = {Springer London},
  year = {2007},
  series = {Advances in Pattern Recognition},
  abstract = {On-line handwriting recognition is used to automatically transcribe
	characters handwritten with electronic devices like tablets and pens.
	Compared to off-line handwriting recognition, it has the advantage
	of utilizing time series information of hand movements captured by
	the electronic devices. Given the widespread use of mobile devices
	such as cell phones, PDAs and pen computers these days, on-line handwriting
	recognition has gained large attention again as a convenient and
	portable input method. Also, its application area has further extended
	to 3D writing space where a user can draw gestures and characters
	with inertial sensor-embedded input devices [1]. For highly accurate
	character recognition, it is necessary to model the structure of
	characters as realistically as possible. In this Chapter, a character
	is regarded to have points and basic strokes as its composition structure.
	The basic strokes are defined as straight or nearly straight traces
	that have distinct directions from connected traces in writing order.
	Figure 6.1(a) and (b) shows such examples in numerical and Chinese
	characters. They are apparently identifiable in the characters with
	only straight lines like 1, 4 and the Chinese ones. The curvilinear
	trace, like the lower part of the character 5, can be approximated
	with several ones. To describe and identify characters, strokes and
	their},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARCho2007.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARCowell2001,
  author = {Dr John Cowell and Dr Fiaz Hussain},
  title = {Thinning Arabic Characters for Feature Extraction},
  booktitle = {Proceedings of the Fifth International Conference on Information
	Visualisation},
  year = {2001},
  pages = {181},
  abstract = {A successful approach to the recognition of Latin characters is to
	extract fiatures from that character such as the number of strokes,
	stroke intersections and holes, and to use ad-hoc tests to diflerentiate
	between characters which have similar features. The first stage in
	this process ib to produce thinned 1 pixel thick representations
	of the characters to simplifi feature extraction. This approach works
	well with printed Latin characters which are of high quality. With
	poor quality characters, however, the thinning process itself is
	not, straighrfonvard and can introduce errors which clre manfested
	in the later stages of the recognition process. The recognition of
	poor quality Arabic characters is a particular problem since the
	chara(5tet-s are calligraphic with printed characters having widely
	varying stroke thicknesses to simulate the drawing of the character
	with a calligraphy pen or brush. This paper describes the problems
	encountered when thinning large poor quality Arabic characters prior
	to the extraction of their features and submission to a syntactic
	recognition system.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARCowell2001.pdf:PDF},
  keywords = {Arabic, characters, thinning, optical, OCR, Urdu},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@CONFERENCE{ARDaifallah2009,
  author = {Khaled Daifallah and Dr. Nizar Zarka and Hassan Jamous},
  title = {Recognition-Based Segmentation Algorithm for On-Line Arabic Handwriting},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {In this paper, we introduce an on-line Arabic handwritten recognition
	system based on new stroke segmentation algorithm. The proposed algorithm
	uses an over segmentation method that has the advantage of giving
	all correct segments at least. It is based on arbitrary segmentation
	followed by segmentation enhancement, consecutive joints connection
	and finally segmentation point locating. The proposed system gives
	an excellent recognition rate up to 97% and 92% for words and letter
	recognition.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARDaifallah2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@INPROCEEDINGS{DSDimauro2002,
  author = {Dimauro, G. and Impedovo, S. and Modugno, R. and Pirlo, G.},
  title = {A new database for research on bank-check processing},
  booktitle = {Frontiers in Handwriting Recognition, 2002. Proceedings. Eighth International
	Workshop on},
  year = {2002},
  pages = { 524 - 528},
  abstract = { This paper presents a new database for off-line handwriting recognition.
	The database, that is particularly devoted to research on bank-check
	recognition, up to now includes instances of isolated digits and
	characters, basic words of worded amounts, and signatures. Pattern
	images are stored using a standard image format, and hence they are
	easily usable by several commercial and scientific image processing
	packages.},
  doi = {10.1109/IWFHR.2002.1030964},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSDimauro2002.pdf:PDF},
  issn = { },
  keywords = {bank-check processing; bank-cheque processing; database; isolated
	characters; isolated digits; off-line handwriting recognition database;
	pattern images; signatures; standard image format; words; cheque
	processing; handwriting recognition; handwritten, ; visual databases;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSDing2008,
  author = {Wu Ding and Suen, C.Y. and Krzyzak, A.},
  title = {A new courtesy amount recognition module of a Check Reading System},
  booktitle = {Pattern Recognition, 2008. ICPR 2008. 19th International Conference
	on},
  year = {2008},
  pages = {1 -4},
  month = {dec.},
  abstract = {A new courtesy amount recognition module of CENPARMIpsilas check reading
	system (CRS) is proposed in this paper. The module consists of 3
	main segments: pre-processing, segmentation and recognition, and
	post-processing. A new feedback-based segmentation algorithm is adopted
	for the segmentation task. Besides one individual numeral recognizer
	for numerals from dasia0psila to dasia9psila, one convolutional neural
	network(CNN) recognizer for ldquo00rdquo and ldquo000rdquo numeral
	strings is also integrated into our module for the recognition task.
	The experimental results on the Quebec Bell Check database show that
	the recognition rate of the courtesy amount has improved from 41.2%
	to 74.3%.},
  doi = {10.1109/ICPR.2008.4761532},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSDing2008.pdf:PDF},
  issn = {1051-4651},
  keywords = {CRS;check reading system;convolutional neural network recognizer;courtesy
	amount recognition module;feedback-based segmentation algorithm;numeral
	recognizer;bank data processing;handwritten, ;image recognition;image
	segmentation;neural nets;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{PDB17Domeniconi2001,
  author = {Carlotta Domeniconi and Dimitrios Gunopulos},
  title = {Incremental Support Vector Machine Construction.},
  booktitle = {Proceedings of the 2001 IEEE International Conference on Data Mining},
  year = {2001},
  editor = {Nick Cercone and Tsau Young Lin and Xindong Wu},
  pages = {589-592},
  address = {San Jose, California, USA},
  month = {29 November - 2 December},
  publisher = {IEEE Computer Society},
  abstract = {SVMs suffer from the problem of large memory requirement and CPU time
	when trained in batch mode on large data sets. We overcome these
	limitations,and at the same time make SVMs suitable for learning
	with data streams,by constructing incremental learning algorithms.
	We first introduce and compare different incremental learning techniques,and
	show that they are capable of producing performance results similar
	to the batch algorithm, and in some cases superior condensation properties.
	We then consider the problem of training SVMs using stream data.
	Our objective is to maintain an updated representation of recent
	batches of data. We apply incremental schemes to the problem and
	show that their accuracy is comparable to the batch algorithm.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB17Domeniconi2001.pdf:PDF},
  isbn = {0-7695-1119-8},
  owner = {TOSHIBA},
  timestamp = {2009.10.05}
}

@CONFERENCE{ARDreuw2009,
  author = {Philippe Dreuw and David Rybach and Christian Gollan and Hermann
	Ney},
  title = {Writer Adaptive Training and Writing Variant Model Refinement for
	Offline Arabic Handwriting Recognition},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {We present a writer adaptive training and writer clus- tering approach
	for an HMM based Arabic handwriting recognition system to handle
	different handwriting styles and their variations. Additionally,
	a writing variant model refinement for specific writing variants
	is proposed. Current approaches try to compensate the impact of dif-
	ferent writing styles during preprocessing and normaliza- tion steps.
	Writer adaptive training with a CMLLR based feature adaptation is
	used to train writer dependent models. An unsupervised writer clustering
	with Bayesian information criterion based stopping condition for
	a CMLLR based fea- ture adaptation during a two-pass decoding process
	is used to cluster different handwriting styles of unknown test writ-
	ers. The proposed methods are evaluated on the IFN/ENIT Arabic handwriting
	database.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARDreuw2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@INPROCEEDINGS{DSSantos2002,
  author = {Eduardo Bastos Dos Santos, J. and Dubuisson, B. and Bortolozzi, F.},
  title = {Characterizing and distinguishing text in bank cheque images},
  booktitle = {Computer Graphics and Image Processing, 2002. Proceedings. XV Brazilian
	Symposium on},
  year = {2002},
  pages = { 203 - 209},
  abstract = { The most common goal of automatic bank cheque treatment systems is
	the recognition of handwritten information. However, in order to
	do this, it is necessary to use a reliable and efficient process
	able to identify and to extract the information, which can then be
	submitted to a further recognition phase. We present a process for
	identifying and distinguishing between handwritten information and
	machine printed text based on a set of local features. This process
	is based on the characterization of textual elements via properties
	derived from their content and their shape. The main advantage of
	this process compared with other similar approaches is that no a
	priori information of the treated document is used, thus making it
	more generic and effective.},
  doi = {10.1109/SIBGRA.2002.1167144},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSSantos2002.pdf:PDF},
  issn = {1530-1834 },
  keywords = {automatic bank cheque treatment systems;bank cheque image processing;document
	images;handwritten, ;image segmentation;machine printed text;text
	recognition;bank data processing;cheque processing;document image
	processing;handwritten, ;image segmentation;optical, ;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{PDBEid2007,
  author = {Eid, Mohamad A. and Mansour, Mohamed and El Saddik, Abdulmotaleb
	H. and Iglesias, Rosa},
  title = {A haptic multimedia handwriting learning system},
  booktitle = {Emme '07: Proceedings of the international workshop on Educational
	multimedia and multimedia education},
  year = {2007},
  pages = {103--108},
  address = {Augsburg, Bavaria, Germany},
  publisher = {ACM},
  abstract = {In this paper, we describe a multimedia system for learning handwriting
	and pronunciation of alphabet letters or characters in different
	languages. This system provides haptic, audio and visual information
	according to the desired letter or character chosen by a user. Letters
	or characters from the Arabic, English, French, Japanese, and Spanish
	languages have been considered, although the system utilizes an XML-based
	schema to easily introduce new characters from another language.
	
	
	Three different modes of learning can be chosen in terms of haptic
	information: full guidance, partial guidance and a no guidance mode
	(no haptic feedback). The full guidance guides the user to follow
	a pre-recorded letter trajectory; whereas in partial guidance, a
	user can freely follow a letter-drawing path, but if the user deviates
	significantly, the system automatically brings him/her back to the
	optimal displayed path. The no guidance mode allows users to perform
	letter handwriting with only visual information. This system guides
	users to write a character, in a similar way as a teacher holds a
	student’s hand. Moreover, the character trajectory is displayed as
	the user is performing it. The results of this system evaluation
	show its potential as a virtual tool for learning handwriting.},
  doi = {http://doi.acm.org/10.1145/1290144.1290161},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBEid2007.pdf:PDF},
  isbn = {978-1-59593-783-4},
  location = {New York, NY, USA},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@INPROCEEDINGS{DSMelegy2007,
  author = {El-Melegy, M.T. and Abdelbaset, A.A.},
  title = {Global features for offline recognition of handwritten Arabic literal
	amounts},
  booktitle = {Information and Communications Technology, 2007. ICICT 2007. ITI
	5th International Conference on},
  year = {2007},
  pages = {125 -129},
  month = {dec.},
  abstract = {The domain of automatic handwriting recognition has a variety of applications
	in real world problems such as cheque processing, office automation
	and data entry applications. In this paper an approach for describing
	the role of holistic structural features in the recognition of offline
	handwritten Arabic literal amounts has been presented. Our proposed
	system attempts to recognize words from their overall shape. The
	extracted features are presented to several classifiers. The system
	is trained and tested using an Arabic handwritten database.},
  doi = {10.1109/ITICT.2007.4475631},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSMelegy2007.pdf:PDF},
  keywords = {Arabic handwritten database;automatic handwriting recognition;cheque
	processing;data entry applications;handwritten Arabic literal amounts;holistic
	structural features;office automation;offline recognition;feature
	extraction;handwriting recognition;image classification;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@CONFERENCE{ARElbaati2009,
  author = {Abdelkarim Elbaati and Houcine Boubaker and Monji Kherallah and Adel
	M. Alimi},
  title = {Arabic Handwriting Recognition Using Restored Stroke Chronology},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {In this paper we present a system of the off-line handwriting recognition.
	Our recognition system is based on temporal order restoration of
	the off-line trajectory. For this task we use a genetic algorithm
	(GA) to optimize the sequences of handwritten strokes. To benefit
	from dynamic information we make a sampling operation by the consideration
	of trajectory curvatures. We proceed to calculate the curvilinear
	velocity signal and use the beta-elliptical modeling which is developed
	in on-line systems to calculate other characteristics. Our approach
	is validated by Hidden Markov Model (HMM) Tool Kit (HTK) recognition
	system using IfN/ENIT database.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARElbaati2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@ARTICLE{DSElnagar2003,
  author = {Ashraf Elnagar and Reda Alhajj},
  title = {Segmentation of connected handwritten numeral strings},
  journal = {Pattern Recognition},
  year = {2003},
  volume = {36},
  pages = {625 - 634},
  number = {3},
  abstract = {A new approach to separating single touching handwritten digit strings
	is presented. The image of the connected numerals is normalized,
	preprocessed and then thinned before feature points are detected.
	Potential segmentation points are determined based on decision line
	that is estimated from the deepest/highest valley/hill in the image.
	The partitioning path is determined precisely and then the numerals
	are separated before restoration is applied. Experimental results
	on the NIST Database 19, CEDAR CD-ROM and our own collection of images
	show that our algorithm can get a successful recognition rate of
	96%, which compares favorably with those reported in the literature.},
  doi = {DOI: 10.1016/S0031-3203(02)00097-3},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSElnagar2003.pdf:PDF},
  issn = {0031-3203},
  keywords = {Character segmentation},
  owner = {Maha},
  timestamp = {2010.2.24},
  url = {http://www.sciencedirect.com/science/article/B6V14-4771RWD-3/2/7cbc7f3ab0340029eb6aaaff3e9693b8}
}

@ARTICLE{DSFarah2006,
  author = {Nadir Farah and Labiba Souici and Mokhtar Sellami},
  title = {Classifiers combination and syntax analysis for Arabic literal amount
	recognition},
  journal = {Engineering Applications of Artificial Intelligence},
  year = {2006},
  volume = {19},
  pages = {29 - 39},
  number = {1},
  abstract = {Automatic handwriting recognition has a variety of applications in
	real world problems, such as mail sorting and check processing. Recently,
	it has been demonstrated that combining the decisions of several
	classifiers and integrating multiple information sources can lead
	to better recognition results. This article presents an approach
	for recognizing handwritten Arabic literal (legal) amounts. The proposed
	system uses a set of holistic structural features to describe the
	words. These features are presented to three classifiers: multilayer
	neural network, k nearest neighbor, and fuzzy k nearest neighbor.
	The classification results are then combined using several schemes;
	we retained the score summation one for this work. A syntactic post-classification
	process is then carried out to find the best match among the candidate
	words. The performance of this approach is superior to the system
	which ignores all contextual information and simply relies on the
	recognition scores of the recognizers.},
  doi = {DOI: 10.1016/j.engappai.2005.05.005},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSFarah2006.pdf:PDF},
  issn = {0952-1976},
  keywords = {Arabic word recognition},
  owner = {Maha},
  timestamp = {2010.2.24},
  url = {http://www.sciencedirect.com/science/article/B6V2M-4GHBP7C-1/2/bc87267a3a3a694e878d339e250b5e34}
}

@ARTICLE{DSFarah2005,
  author = {Nadir Farah and Labiba Souici and Mokhtar Sellami},
  title = {Arabic Word Recognition by Classifiers and Context},
  journal = {Journal of Computer Science and Technology},
  year = {2005},
  volume = {20},
  pages = {402- 410},
  number = {3},
  month = {May},
  abstract = {Given the number and variety of methods used for handwriting recognition,
	it has been shown that there is no single method that can be called
	the best. In recent years, the combination of different classifiers
	and the use of contextual information have become major areas of
	interest in improving recognition results. This paper addresses a
	case study on the combination of multiple classifiers and the integration
	of syntactic level information for the recognition of handwritten
	Arabic literal amounts. To the best of our knowledge, this is the
	first time either of these methods has been applied to Arabic word
	recognition. Using three individual classifiers with high level global
	features, we performed word recognition experiments. A parallel combination
	method was tested for all possible configuration cases of the three
	chosen classifiers. A syntactic analyzer makes a final decision on
	the candidate words generated by the best configuration scheme. The
	effectiveness of contextual knowledge integration in our application
	is confirmed by the obtained results.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSFarah2005.pdf:PDF},
  keywords = {handwriting recognition, multiclassifier systems, contextual knowledge,
	Arabic literal amounts},
  owner = {Maha},
  timestamp = {2010.02.25}
}

@ARTICLE{ARFarooq2009,
  author = {Faisal Farooq and Damien Jose and Venu Govindaraju},
  title = {Phrase-based correction model for improving handwriting recognition
	accuracies},
  journal = {Pattern Recognition},
  year = {2009},
  volume = {42},
  pages = {3271-3277},
  number = {12},
  abstract = {We propose a method for increasing word recognition accuracies by
	correcting the output of a handwriting recognition system. We treat
	the handwriting recognizer as a black box, such that there is no
	access to its internals. This enables us to keep our algorithm general
	and independent of any particular system. We use a novel method for
	correcting the output based on a “phrase-based” system in contrast
	to traditional source-channel models. We report the accuracies of
	two in-house handwritten word recognizers before and after the correction.
	We achieve highly encouraging results for a large synthetically generated
	dataset. We also report results for a commercially available OCR
	on real data.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  doi = {http://dx.doi.org/10.1016/j.patcog.2008.12.014},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARFarooq2009.pdf:PDF},
  keywords = {Post-processing, Noisy channel, Handwriting recognition, Error correction,
	Viterbi decoding},
  owner = {TOSHIBA},
  timestamp = {2009.10.08}
}

@INPROCEEDINGS{DSFeritas2000,
  author = {Freitas, Cinthia Obladen de Almendra and Yacoubi, Abdenaim El and
	Bortolozzi, Fl\'{a}vio and Sabourin, Robert},
  title = {Brazilian Bank Check Handwritten Legal Amount Recognition},
  booktitle = {SIBGRAPI '00: Proceedings of the 13th Brazilian Symposium on Computer
	Graphics and Image Processing},
  year = {2000},
  pages = {97--104},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  abstract = {This paper presents a system that is being developed for the recognition
	of the handwritten legal amount in Brazilian bank checks. Our strategy
	used to approach the handwritten legal amount recognition problem
	puts on evidence the keywords: "mil", "reals/real", "centavos/centavo"
	which are almost always present in each amount. The recognizer, based
	on hidden markov models, does a global word analysis, therefore,
	it does not carry out an explicit segmentation of words into characters
	or pseudo-characters. In this context, each word image is transformed
	into a sequence of observations using pre-processing and feature
	extraction stages. Our system, when tested on our database simulating
	Brazilian bank checks, shows the viability of our approach.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSFeritas2000.pdf:PDF},
  isbn = {0-7695-0878-2},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@ARTICLE{ARFrias-Martinez2006,
  author = {E. Frias-Martinez and A. Sanchez and J. Velez},
  title = {Support vector machines versus multi-layer perceptrons for efficient
	off-line signature recognition},
  journal = {Engineering Applications of Artificial Intelligence},
  year = {2006},
  volume = {19},
  pages = {693 - 704},
  number = {6},
  note = {Special Section on Innovative Production Machines and Systems (I*PROMS)},
  abstract = {The problem of automatic signature recognition has received little
	attention in comparison with the problem of signature verification
	despite its potential applications for accessing security-sensitive
	facilities and for processing certain legal and historical documents.
	This paper presents an efficient off-line human signature recognition
	system based on support vector machines (SVM) and compares its performance
	with a traditional classification technique, multi-layer perceptrons
	(MLP). In both cases we propose two approaches to the problem: (1)
	construct each feature vector using a set of global geometric and
	moment-based characteristics from each signature and (2) construct
	the feature vector using the bitmap of the corresponding signature.
	We also present a mechanism to capture the intrapersonal variability
	of each user using just one original signature. Our results empirically
	show that SVM, which achieves up to 71% correct recognition rate,
	outperforms MLP.},
  doi = {DOI: 10.1016/j.engappai.2005.12.006},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARFrias-Martinez2006.pdf:PDF},
  issn = {0952-1976},
  keywords = {Multi-layer perceptrons},
  owner = {TOSHIBA},
  timestamp = {2009.10.08},
  url = {http://www.sciencedirect.com/science/article/B6V2M-4JFGF83-2/2/aece9ce4b6ffffb81b9b6cf4f0861706}
}

@INPROCEEDINGS{PDBFu2006,
  author = {Zhouyu Fu and Terry Caelli and Nianjun Liu and Antonio Robles-Kelly},
  title = {Boosted Band Ratio Feature Selection for Hyperspectral Image Classification.},
  booktitle = {18th International Conference on Pattern Recognition (ICPR 2006)},
  year = {2006},
  pages = {1059-1062},
  address = {Hong Kong, China},
  month = { 20-24 August },
  publisher = {IEEE Computer Society},
  abstract = {Band ratios have many useful applications in hyperspectral image analysis.
	While optimal ratios have been chosen empirically in previous research,
	we propose a principled algorithm for the automatic selection of
	ratios directly from data. First, a robust method is used to estimate
	the Kullback-Leibler divergence (KLD) between different sample distributions
	and evaluate the optimality of individual ratio features. Then, the
	boosting framework is adopted to select multiple ratio features iteratively.
	Multiclass classification is handled by using a pairwise classification
	framework. The algorithm can also be applied to the selection of
	discriminant bands. Experimental results on both simple material
	identification and complex land cover classification demonstrate
	the potential of this ratio selection algorithm. 1. Introduction},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ICPR.2006.334},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBFu2006.pdf:PDF},
  isbn = {0-7695-2521-0},
  owner = {TOSHIBA},
  timestamp = {2009.10.21}
}

@ARTICLE{ARFujisawa2008,
  author = {Hiromichi Fujisawa},
  title = {Forty years of research in character and document recognition - an
	industrial perspective},
  journal = {Pattern Recognition},
  year = {2008},
  volume = {41},
  pages = {2435-2446},
  number = {8},
  abstract = {This paper presents an overview on the last 40-years of technical
	advances in the field of character and document recognition. Representative
	developments in each decade are described. Then, key technical developments
	in the specific area of Kanji recognition in Japan are highlighted.
	The main part of the paper discusses robustness design principles,
	which have proven to be effective to solve complex problems in postal
	address recognition. Included are the hypothesis-driven principle,
	deferred decision/multiple-hypotheses principle, information integration
	principle, alternative solution principle, and perturbation principle.
	Finally, future prospects, the ‘long-tail’ phenomena, and promising
	new applications are discussed.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  doi = {http://dx.doi.org/10.1016/j.patcog.2008.03.015},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARFujisawa2008.pdf:PDF},
  keywords = {OCR, Character Recognition, Japanese or Chinese Characters, Robustness
	design, Postal Adress Design},
  owner = {TOSHIBA},
  review = {(Reading order = 4) Mainly industrial view from 60 til 90's. 
	
	
	Application of character is form reading, bank check and postal address
	reading . Discuss comercial systems in OCR by ibm, hitachi from 60,70,80
	and 90's. 
	
	Mentions state of art in 
	
	M. Cheriet, N. Kharma, C.-L. Liu, C. Y. Suen; Character Recognition
	Systems: A Guide for Students and Practitioners, John Wiley & Sone,
	November 2007, ISBN: 978-0-471-41570-
	
	Focus on Kanji characters, the approaches of recognition can be structural
	or statistical. Mentions also the directional features are effictive.
	
	
	Mentions Character segmentation algorithms and the integration of
	linguistics information to add knowldge which helps recognition.
	
	
	
	Robustness desing to datl with uncertainity and variablity by 
	
	1)Principles Expected effects
	
	P1 Hypothesis-driven principle When type of a problem is uncertain,
	set up hypotheses and test them
	
	P2 Deferred decision/multiple hypotheses principle Do not decide;
	leave decision to next experts carrying over multiple hypotheses
	
	 P3a Process integration Solve a problem by multiple different-field
	experts as a team
	
	 P3b Information integration principle Combination-based integration
	Decide as a team of multiple same-field experts
	
	 P3c Corroboration-based integration Utilize other input information;
	seek more evidence
	
	P4 Alternative solutions principle Solve a problem by multiple alternative
	approaches
	
	P5 Perturbation principle Modify problem slightly and try again},
  timestamp = {2009.10.08}
}

@INPROCEEDINGS{PDBFumera2003,
  author = {Giorgio Fumera and Ignazio Pillai and Fabio Roli},
  title = {Classification with reject option in text categorisation systems.},
  booktitle = {12th International Conference on Image Analysis and Processing (ICIAP
	2003)},
  year = {2003},
  pages = {582-587},
  address = {Mantova, Italy},
  month = {17-19 September},
  publisher = {IEEE Computer Society},
  abstract = {The aim of this paper is to evaluate the potential usefulness of the
	reject option for text categorisation (TC) tasks. The reject option
	is a technique used in statistical pattern recognition for improving
	classification reliability. Our work is motivated by the fact that,
	although the reject option proved to be useful in several pattern
	recognition problems, it has not yet been considered for TC tasks.
	Since TC tasks differ from usual pattern recognition problems in
	the performance measures used and in the fact that documents can
	belong to more than one category, we developed a specific rejection
	technique for TC problems. The performance improvement achievable
	by using the reject option was experimentally evaluated on the Reuters
	dataset, which is a standard benchmark for TC systems.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://csdl.computer.org/comp/proceedings/iciap/2003/1948/00/19480582abs.htm},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBFumera2003.pdf:PDF},
  isbn = {0-7695-1948-2}
}

@ARTICLE{ARGadat2007,
  author = {S´ebastien Gadat and Laurent Younes},
  title = {A Stochastic Algorithm for Feature Selection in Pattern Recognition},
  journal = {Journal of Machine Learning Research},
  year = {2007},
  volume = {8},
  pages = {509-547},
  abstract = {We introduce a new model addressing feature selection from a large
	dictionary of variables that can be computed from a signal or an
	image. Features are extracted according to an efficiency criterion,
	on the basis of specified classification or recognition tasks. This
	is done by estimating a probability distribution P on the complete
	dictionary, which distributes its mass over the more efficient, or
	informative, components. We implement a stochastic gradient descent
	algorithm, using the probability as a state variable and optimizing
	a multi-task goodness of fit criterion for classifiers based on variable
	randomly chosen according to P. We then generate classifiers from
	the optimal distribution of weights learned on the training set.
	The method is first tested on several pattern recognition problems
	including face detection, handwritten digit recognition, spam classification
	and micro-array analysis. We then compare our approach with other
	step-wise algorithms like random forests or recursive feature elimination.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARGadat2007.pdf:PDF},
  keywords = {stochastic learning algorithms, Robbins-Monro application, pattern
	recognition, classification algorithm, feature selection},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{FE2Gader1996,
  author = {Paul D. Gader and Mohamed Ali Khabou},
  title = {Automatic Feature Generation for Handwritten Digit Recognition},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1996},
  volume = {18},
  pages = {1256-1261},
  number = {12},
  month = {December},
  note = {My notes are},
  abstract = {Abstract-An automatic feature generation method for handwritten digit
	recognition is described. Two different evaluation measures, orthogonality
	and information, are used to guide the search for features. The features
	are used in a backpropagation trained neural network. Classification
	rates compare favorably with results published in a survey of high-performance
	handwritten digit recognition systems. This classifier is combined
	with several other high performance classifiers. Recognition rates
	of around 98% are obtained using two classifiers on a test set with
	1,000 digits per class.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\FE2Gader1996.pdf:PDF},
  keywords = {Features Extraction, Handwritten digits},
  owner = {TOSHIBA},
  review = {Comments:
	
	 1) generate feature from the correlation data 
	
	2) feature evaluation using [ entropy of information orthognality
	of features] 
	
	3) ANN + Back propogation 
	
	4) result 96% 
	
	5) combine classifiers 98% 
	
	result are on 1000 images.},
  timestamp = {2009.10.03}
}

@ARTICLE{PDB15Golfarelli1997,
  author = {Matteo Golfarelli and Dario Maio and Davide Maltoni},
  title = {On the Error-Reject Trade-Off in Biometric Verification Systems.},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  year = {1997},
  volume = {19},
  pages = {786-796},
  number = {7},
  abstract = {Abstract—In this work, we address the problem of performance evaluation
	in biometric verification systems. By formulating the optimum Bayesian
	decision criterion for a verification system and by assuming the
	data distributions to be multinormals, we derive two statistical
	expressions for calculating theoretically the false acceptance and
	false rejection rates. Generally, the adoption of a Bayesian parametric
	model does not allow for obtaining explicit expressions for the calculation
	of the system errors. As far as biometric verification systems are
	concerned, some hypotheses can be reasonably adopted, thus allowing
	simple and affordable expressions to be derived. By using two verification
	system prototypes, based on hand shape and human face, respectively,
	we show our results are well founded.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://www.computer.org/tpami/tp1997/i0786abs.htm},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB15Golfarelli1997.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.05}
}

@INPROCEEDINGS{ARGolubitsky2008,
  author = {Oleg Golubitsky and Stephen M. Watt},
  title = {Online Stroke Modeling for Handwriting Recognition},
  booktitle = {CASCON '08 : Proceedings of the 2008 conference of the center for
	advanced studies on collaborative research},
  year = {2008},
  pages = {72 -80},
  address = {Ontario, Canada},
  abstract = {The process of recognizing individual handwritten characters is one
	of classifying curves. Typically, handwriting recognition systems—
	even “online” systems—require entire characters be completed before
	recognition is attempted. This paper presents another approach for
	real-time recognition: certain characteristics of a curve can be
	computed as the curve is being written, and these characteristics
	are used to classify the character in constant time when the pen
	is lifted. We adapt an earlier approach of representing curves in
	a functional basis and reduce real-time stroke modelling to the Hausdorff
	moment problem.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARGolubitsky2008.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{PDBGorgevik2004,
  author = {Dejan Gorgevik and Dusan Cakmakov},
  title = {An Efficient Three-Stage Classifier for Handwritten Digit Recognition.},
  booktitle = {ICPR (4)},
  year = {2004},
  pages = {507-510},
  abstract = {This paper proposes an efficient three-stage classifier for handwritten
	digit recognition based on NN (Neural Network) and SVM (Support Vector
	Machine) classifiers. The classification is performed by 2 NNs and
	one SVM. The first NN is designed to provide a low misclassification
	rate using a strong rejection criterion. It is applied on a small
	set of easy to extract features. Rejected patterns are forwarded
	to the second NN that uses additional, more complex features, and
	utilizes a wellbalanced rejection criterion. Finally, rejected patterns
	from the second NN are forwarded to an optimized SVM that considers
	only the “top k” classes as ranked by the NN. This way a very fast
	SVM classification is obtained without sacrificing the classifier
	accuracy. The obtained recognition rate is among the best on the
	MNIST database and the classification time is much better compared
	to the single SVM applied on the same feature set.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ICPR.2004.157},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBGorgevik2004.pdf:PDF},
  keywords = {Classifier Cascade, Handwritten digits},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@INPROCEEDINGS{DSGorski1999,
  author = {Gorski, N. and Anisimov, V. and Augustin, E. and Baret, O. and Price,
	D. and Simon, J.-C.},
  title = {A2iA Check Reader: a family of bank check recognition systems},
  booktitle = {Proceedings of the Fifth International Conference on Document Analysis
	and Recognition, 1999. ICDAR '99. },
  year = {1999},
  pages = {523 -526},
  month = {sep},
  abstract = {The paper presents new A2iA bank check recognition systems designed
	to process handwritten and/or printed checks issued in France, UK
	or USA. All the systems have identical architecture and design principles.
	However, each of them contains a country-specific part and is trained
	with country-specific data. Each system performs location, extraction,
	segmentation and recognition of courtesy and legal amounts in a document
	image, as well as deciding whether to accept or reject the check.
	The recognition rate is 80-90%. In the production mode, the check
	acceptance rate is 60-75%, with the misread rate corresponding to
	that of a human operator (close to 1%)},
  doi = {10.1109/ICDAR.1999.791840},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSGroski1999.pdf:PDF},
  keywords = {A2iA Check Reader;A2iA bank check recognition systems;France;UK;USA;architecture;check
	acceptance rate;check rejection;country-specific data training;country-specific
	part;courtesy amounts;decision making;design principles;document
	image;extraction;handwritten check processing;legal amounts;location;misread
	rate;printed check processing;recognition rate;segmentation;cheque
	processing;document image processing;handwritten, ;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSGu2006,
  author = {Jun-xia Gu and Xiao-qing Ding},
  title = {Fusion Recognition of Courtesy and Legal Amounts on Chinese Handwritten
	Bank Checks},
  booktitle = {8th International Conference on Signal Processing, 2006 },
  year = {2006},
  volume = {3},
  month = {16-20 },
  abstract = { In this paper, a novel fusion recognition algorithm of courtesy and
	legal amounts in handwritten Chinese bank checks is presented. Unlike
	the other fusion methods based on the independent recognition results
	of courtesy and legal amount, this proposed fusion algorithm begins
	with the segmentation of legal amount with the guide of recognition
	candidates of courtesy amount. And then we fuse the recognition candidates
	of courtesy and legal amounts. The system is validated with 1053
	real bank checks. When the substitution is 0.43%, the recognition
	rate at the amount level can reach 66.10%},
  doi = {10.1109/ICOSP.2006.345773},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSGu2006.pdf:PDF},
  keywords = {Chinese handwritten bank checks;fusion recognition algorithm;independent
	recognition;legal amount segmentation;legal amounts;handwritten,
	;image segmentation;law administration;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSGuillevic1994,
  author = {Didier Guillevic and Ching Y. Suen},
  title = {Cursive Script Recognition: A Sentence Level Recognition Scheme},
  booktitle = {In International Workshop on Frontiers of Handwriting Recognition},
  year = {1994},
  pages = {216--223},
  abstract = {We describe the cursive script recognition module of a cheque processing
	system currently under development at the Centre for Pattern Recognition
	and Machine Intelligence (CENPARMI). Common systems perform recognition
	either on a character by character basis, or on a word level. The
	present study investigates the recognition at a higher level of abstraction,
	at the sentence level. Our computational theory is based on a psychological
	model of the reading process of a fast reader. In this paper, we
	discuss the processing of the legal amount written on cheques. The
	preprocessing and feature extraction modules as well as the word
	classifier are presented. Preliminary results are not only promising,
	but they also support our computational theory},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSGuillevic1994.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.02.25}
}

@ARTICLE{MCGutta1997,
  author = {SRINIVAS GUTTA and HARRY WECHSLER},
  title = {FACE RECOGNITION USING HYBRID CLASSIFIERS},
  journal = {Pattern Recognition},
  year = {1997},
  volume = {30},
  pages = {539-553},
  number = {4},
  abstract = {We address the problem of surveillance and contents-based image retrieval
	(CBIR) for large image databases consisting of face images. The corresponding
	face recognition tasks considered herein include (i) surveying a
	gallery of images for the presence of specific probes, (ii) CBIR,
	and (iii) CBIR subject to correct ID ("match") displaying specific
	facial landmarks such as wearing glasses. We developed robust matching
	("classification") and retrieval schemes based on hybrid classifiers
	and showed their feasibility using the FERET database. The hybrid
	classifier architecture consists of an ensemble of connectionist
	networks--radial basis functions (RBF)--and inductive decision trees
	(DT). The specific characteristics of our hybrid architecture include
	(a) query by consensus as provided by ensembles of networks for coping
	with the inherent variability of the image formation and data acquisition
	process, (b) categorical classifications using decision trees, (c)
	flexible and adaptive thresholds as opposed to ad hoc and hard thresholds,
	and (d) interpretability of the way classification and retrieval
	are eventually achieved. Experimental results, proving the feasibility
	of our approach, yield (i) 96% accuracy, using cross validation,
	for surveillance on a database consisting of 904 images corresponding
	to 350 subjects (of whom 102 are duplicates), (ii) 97% accuracy for
	CBIR tasks, such as "find all subjects wearing glasses", on a database
	of 1084 images (including noisy versions) of 350 subjects (of whom
	102 are duplicates), and (iii) 93% accuracy, using cross validation,
	for CBIR subject to correct ID match tasks, such as "find Joe Smith
	with/without glasses", on a database of 200 images..},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCGutta1997.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@CONFERENCE{ARHaboubi2009,
  author = {Sofiene Haboubi and Samia Maddouri and Noureddine Ellouze and Haikal
	El-Abed},
  title = {Invariant Primitives for Handwritten Arabic Script: A Contrastive
	study of four feature sets},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {The choice of relevant features is very decisive in handwriting recognition
	rate. Our aim is to present some useful structural and statistical
	features and see their degree of variability. In this paper, we start
	with a description of the variability of the Arabic handwriting and
	the way how to reduce it. Four kinds of feature sets used by our
	handwriting systems are then presented evaluated and discussed. The
	comparison is carried on a database of images from IFN/ENIT databases.
	The Neural Network Multilayer perceptrons is our method of classification.
	A contrastive study of these primitives is done according to recognition
	their time and memory consuming and their variability degree},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARHaboubi2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@INPROCEEDINGS{ARHachour2006,
  author = {Hachour, O.},
  title = {The Combination of Fuzzy Logic and Expert System for Arabic Character
	Recognition},
  booktitle = { 2006 3rd International IEEE Conference on Intelligent Systems},
  year = {2006},
  pages = { 189-191},
  address = { London},
  month = { Sept.},
  abstract = {In this paper Fuzzy Logic (FL) and Expert System (ES) theories are
	studied with regard to their contribution to solving the problem
	of OCR (Optical Chara cter Recognition). These theories have improved
	the learning and adaptation capacities related to varying shapes
	where information is qualitative, inaccurate or incomplete. The use
	of these technologies FL and ES proves interesting, efficient, and
	necessary to recognize all Arabic character. This combination is
	very useful to improve the powerful of Hybrid Intelligent Systems
	HIS in the field of OCR. The primary goal of this combination (FL,
	ES) is to classify and to recognize all presented unknown shapes.
	These theories must achieve these tasks: to classify characters,
	and to make ones way of intelligent recognition by ES-FL system capturing
	the behaviour of a human expert knowledge. The training has used
	280 descended pictures of the database of ACR (Arabic Character Recognition).
	The Results gotten of ACR databases are promising.},
  doi = {10.1109/IS.2006.348415},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARHachour2006.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.08}
}

@CONFERENCE{FE11Hamamoto,
  author = {Yoshihiko Hamamoto and Shunji Uchimura and K. Masamizu and Shingo
	Tomita},
  title = {Recognition of handprinted Chinese characters using Gabor features.},
  booktitle = {Procedding of the 1996 International conference on pattern Recognition
	ICDAR},
  year = {1996},
  pages = {819-823},
  publisher = {IEEE},
  abstract = {We study a Gabor filter-based feature extraction method for handwritten
	numeral character recognition. The performance of the Gabor filter-based
	method is demonstrated on the ETL-1 database. Experimental results
	suggest that the Gabor jilter-based method should be considered in
	recognition of handwritten numeric characters..},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://computer.org/proceedings/icdar/7128/vol\_2/71280819abs.htm},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\FE11Hamamoto.pdf:PDF},
  owner = {TOSHIBA},
  review = {Comments:
	
	Advantages : simple 
	
	dis: with 518 features computation expensive and error rate is 2.8
	
	1) 2d gabor featrues feaatures from 128 to 515 depending on the m
	. 
	
	tested on ETL 1 1400 imagegs per class ( split into 700/700 
	
	classifiers ED NN 2 NN fisher and quad 
	
	
	12.44 , 4.04 , 4.09 , 5.9 , 5.3},
  timestamp = {2009.10.03}
}

@INPROCEEDINGS{PDB14Hamamura2003,
  author = {Tomoyuki Hamamura and Hiroyuki Mizutani and Bunpei Irie},
  title = {A Multiclass Classification Method Based on Multiple Pairwise Classifiers.},
  booktitle = {7th International Conference on Document Analysis and Recognition
	(ICDAR 2003)},
  year = {2003},
  volume = {2-Volume Set},
  pages = {809-813},
  address = {Edinburgh, Scotland, UK},
  month = {3-6 August},
  publisher = {IEEE Computer Society},
  abstract = {In this paper, a new method of composing a multiclass classifier using
	pairwise classifiers is proposed. A “Resemblance Model” is exploited
	to calculate a posteriori probability for combining pairwise classifiers.
	We proved the validity of this model by using approximation of a
	posteriori probability formula. Using this theory, we can obtain
	the optimal decision. An experimental result of handwritten numeral
	recognition is presented, supporting the effectiveness of our method.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://csdl.computer.org/comp/proceedings/icdar/2003/1960/02/196020809abs.htm},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB14Hamamura2003.pdf:PDF},
  isbn = {0-7695-1960-1},
  owner = {TOSHIBA},
  timestamp = {2009.10.05}
}

@CONFERENCE{ARHamdani2009,
  author = {Mahdi Hamdani and Haikal El Abed and Monji Kherallah and Adel M.
	Alimi},
  title = {Combining Multiple HMMs Using On-line and Off-line Features for Off-line
	Arabic Handwriting Recognition},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {This paper presents an off-line Arabic Handwriting recognition system
	based on the selection of different state of the art features and
	the combination of multiple Hidden Markov Models classifiers. Beside
	the classical use of the off-line features, we add the use of on-line
	features and the combination of the developed systems. The designed
	recognizer is implemented using the HMM-Toolkit. In a first step,
	we use different features to make the classification and we compare
	the performance of single classifiers. In a second step, we proceed
	to the combination of the on-line and the off-line based systems
	using different combination methods. The system is evaluated using
	the IFN/ENIT database. The recognition rate is in maximum 63.90%
	for the individual systems. The combination of the on-line and the
	off-line systems allows to improve the system accuracy to 81.93%
	which exceeds the best result of the ICDAR 2005 competition.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARHamdani2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@MISC{PDB11He2001,
  author = {Chun Lei He and Ping Zhang and Jianxiong Dong and Ching Y. Suen and
	Tien D. Bui},
  title = {The Role of Size Normalization on the Recognition Rate of Handwritten
	Numerals},
  year = {2001},
  abstract = {Size normalization is an important pre-processing technique in character
	recognition. Although various effective learning-based methods have
	been proposed, the role of the original data in a database is always
	ignored. In this paper, we have conducted experiments to investigate
	its effects with neural networks and support vector machines and
	have found that the performance of handwritten numeric recognition
	systems deteriorates dramatically due to low size resolution. For
	the MNIST dataset, this study shows that enlarging the size from
	20 * 20 to 26 * 26 by bilinear interpolation can improve the performance
	significantly. After constructing a smaller database of difficult
	original patterns from NIST, we find that normalizing the original
	data to a size larger than 20 * 20 in MNIST increases the recognition
	rate further.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB11He2001.pdf:PDF},
  keywords = {Arabic Handwriting, MNIST},
  owner = {TOSHIBA},
  timestamp = {2009.10.04}
}

@BOOKLET{PDB9SVM1998,
  title = {Trends and controversies, Support vector machines},
  author = {Marti A. Hearst},
  howpublished = {IEEE INTELLIGENT SYSTEMS},
  month = {July},
  year = {1998},
  abstract = {This is different article in IEEE Magazine
	
	
	
	My first exposure to Support Vector Machines came this spring when
	I heard Sue
	
	Dumais present impressive results on text categorization using this
	analysis technique.
	
	This issue’s collection of essays should help familiarize our readers
	with this interesting
	
	new racehorse in the Machine Learning stable. Bernhard Sch?lkopf,
	in an introductory
	
	overview, points out that a particular advantage of SVMs over other
	learning
	
	algorithms is that it can be analyzed theoretically using concepts
	from computational
	
	learning theory, and at the same time can achieve good performance
	when applied to
	
	real problems. Examples of these real-world applications are provided
	by Sue Dumais,
	
	who describes the aforementioned text-categorization problem, yielding
	the best results
	
	to date on the Reuters collection, and Edgar Osuna, who presents strong
	results
	
	on application to face detection. Our fourth author, John Platt, gives
	us a practical
	
	guide and a new technique for implementing the algorithm efficiently.
	
	–Marti Hearst},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB9SVM1998.pdf:PDF},
  keywords = {SVM},
  owner = {TOSHIBA},
  timestamp = {2009.10.03}
}

@ARTICLE{MCHotta2009,
  author = {Kazuhiro Hotta},
  title = {Adaptive weighting of local classifiers by particle filters for robust
	tracking},
  journal = {Pattern Recognition},
  year = {2009},
  volume = {42},
  pages = {619--628},
  abstract = {zzzzzzzzzzzzzzzzzz},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCHotta2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARHuang2006,
  author = {B.Q. Huang and M.-T. Kechadi},
  title = {An HMM-SNN Method for Online Handwriting Symbol Recognition},
  booktitle = {Image Analysis and Recognition, Third International ConferenceICIAR
	2006, Proceedings, Part II,},
  year = {2006},
  editor = {Aur{\'e}lio C. Campilho and Mohamed S. Kamel},
  volume = {4142},
  series = {Lecture Notes in Computer Science},
  pages = {897-905},
  address = {P{\'o}voa de Varzim, Portugal},
  month = {September 18-20},
  publisher = {Springer},
  abstract = {This paper presents a combined approach for online handwriting symbols
	recognition. The basic idea of this approach is to employ a set of
	left-right HMMs to generate a new feature vector as input, and then
	use SNN as a classifier to finally identify unknown symbols. The
	new feature vector consists of global features and several pairs
	of maximum probabilities with their associated different model labels
	for an observation pattern. A recogniser based on this method inherits
	the practical and dynamical modeling abilities from HMM, and robust
	discriminating ability from SNN for classification tasks. This hybrid
	technique also reduces the dimensions of feature vectors significantly,
	complexity, and solves size problem when using only SNN. The experimental
	results show that this approach outperforms several classifiers reported
	in recent research, and can achieve recognition rates of 97.41%,
	91.81% and 91.63% for digits and upper/lower case characters respectively
	on the UNIPEN database benchmarks.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARHuang2006.pdf:PDF},
  journal = {ICIAR 2006, LNCS 4142, pp. 897–905, 2006.},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{PDBHuang2004,
  author = {Huang, Xiangsheng and Li, Stan Z. and Wang, Yangsheng},
  title = {Learning with Cascade for Classification of Non-Convex Manifolds},
  booktitle = {CVPRW '04: Proceedings of the 2004 Conference on Computer Vision
	and Pattern Recognition Workshop (CVPRW'04) Volume 5},
  year = {2004},
  pages = {66},
  publisher = {IEEE Computer Society},
  abstract = {Images of a visual object, such as human face, reside in a complicated
	manifold in the high dimensional image space, when the object is
	subject to variations in pose, illumination, and other factors. Viola
	and Jones [1, 2, 3] have successfully tackled difficult nonlinear
	classification problem for face detection using AdaBoost learning.
	Moreover, their simple-to-complex cascade of classifiers structure
	makes the learning and classification even more effective. While
	training with cascade has been used effectively in many works [4,
	5, 6, 7, 2, 3, 8, 9, 10], an understanding of the role of the cascade
	strategy is still lacking.
	
	 In this paper, we analyze the problem of classifying nonconvex manifolds
	using AdaBoost learning with and without using cascade. We explain
	that the divide-and-conquer strategy in cascade learning has a great
	contribution on learning a complex classifier for non-convex manifolds.
	We prove that AdaBoost learning with cascade is effective when a
	complete or over-complete set of features (or weak classifiers) is
	available. Experiments with both synthesized and real data demonstrate
	that AdaBoost learning with cascade leads to improved convergence
	and accuracy.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBHuang2004.pdf:PDF},
  isbn = {0-7695-2158-4},
  keywords = {Classifier Cascade},
  location = {Washington, DC, USA},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@ARTICLE{MCHung2004,
  author = {K.-Y. Hunga and R.W.P. Luka and D.S.Yeunga and K.F.L. Chung and W.
	Sh ua},
  title = { A multiple classifier approach to detect Chinese character recognition
	errors},
  journal = {Pattern Recognition},
  year = {2004},
  volume = {38},
  pages = {723-738},
  abstract = {Detection of recognition errors is important in many areas, suchas
	improving recognition performance, saving manual effort for proof-reading
	and post-editing, and assigning appropriate weights for retrieval
	in constructing digital libraries.We propose a novel application
	of multiple classifiers for the detection of recognition errors.
	A need for multiple classifiers emerges when a single classifier
	cannot improve recognition-error detection performance compared with
	the current detection scheme using a simple threshold mechanism.
	Although the single classifier does not improve recognition error
	performance, it serves as a baseline for comparison and the related
	study of useful features for error detection suggests three distinct
	cases where improvement is needed. For eachcase, the multiple classifier
	approachassigns a classifier to detect the presence or absence of
	errors and additional features are considered for each case. Our
	results show that the recall rate (70–80%) of recognition errors,
	the precision rate (80–90%) of recognition error detection and the
	saving in manual effort (75%) were better than the corresponding
	performance using a single classifier or a simple threshold detection
	scheme},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCHung2004.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARHUSSAIN2000,
  author = {FIAZ HUSSAIN and JOHN COWELL},
  title = {Character Recognition of Arabic and Latin Scripts},
  booktitle = {IEEE International Conference on Information Visualization (IV'00)},
  year = {2000},
  abstract = {The goal to produce effective Optical Character
	
	Recognition (OCR) methods has lead to the
	
	development of a number of algorithms. The
	
	purpose of these is to take the hand-written or
	
	printed text and to translate it into a
	
	corresponding digital form. The multitude
	
	requirements and developments are well
	
	represented in the literature ( see for example
	
	Abuhaiba [ 1 J and Suen [2 J ).
	
	The primary objective of this paper is to provide
	
	an insight into a robust system which has been
	
	successfully developed and employed to
	
	recognise Latin and Arabic characters and
	
	whose workings has been described by the
	
	authors in a sister publication [ 3 J .The focus
	
	here is to discuss the main components used in
	
	the multi-stage system, paying particular
	
	attention to the nonnalisation process used for
	
	orientation and size for a given bitmapped
	
	character. The effectiveness of the approach is
	
	demonstrated through its workings for the Arabic
	
	and Latin case, both for characters and numbers.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARHUSSAIN2000.pdf:PDF},
  journal = {IEEE International Conference on Information Visualization (IV'00)},
  keywords = {Arabic, fonts, Latin, nonnalisation, OCR, pattern recognition, confusion
	matrix.},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{DSHussein1999,
  author = {Karim M. Hussein and Arun Agarwal and Amar Gupta and Patrick S. P.
	Wang},
  title = {A knowledge-based segmentation algorithm for enhanced recognition
	of handwritten courtesy amounts},
  journal = {Pattern Recognition},
  year = {1999},
  volume = {32},
  pages = {305 - 316},
  number = {2},
  abstract = {A knowledge-based segmentation algorithm to enhance recognition of
	courtesy amounts on bank checks is proposed in this paper. This algorithm
	uses multiple contextual cues to enhance segmentation and recognition.
	The system described extracts context from the handwritten numerals
	and uses a syntax parser based on a deterministic finite automaton
	to provide adequate feedback to enhance recognition. Further feedback
	is provided by a simple legal amount decoder that determines word
	count and recognizes several key words (e.g. thousand and hundred).
	This provides an additional semantic constraint on the dollar section.
	The segmentation analysis module presented is capable of handling
	a number of commonly used styles for courtesy amount representation.
	Both handwritten and machine written courtesy and legal amounts were
	utilized to test the efficacy of the preprocessor for the check recognition
	system described in this paper. The substitution error was reduced
	by 30-40% depending on the input check mix.},
  comment = {Last edited 2 march 2010},
  doi = {DOI: 10.1016/S0031-3203(98)00073-9},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSHussein1999.pdf:PDF},
  issn = {0031-3203},
  keywords = {Automata},
  owner = {Maha},
  review = {The paper discribe a system to correctly segment a string of number
	(courtsey amount) based on contextual knowldges. It create a grammer
	of different primitive for example digit, coma, dash, dollar sign.
	The rules will tell when the coma, dash, digit is in an acceptable
	location and when it is wrong. The system is divided into five major
	parts preprocessing , inital segmentaiton , segmentation analysis
	, classification , post processing. First in preprocessing locat
	the courtsey amount using another alogrithm ( [9] ). Then simple
	inital segmentation is used using upper / lower contour spliting
	. The segmentation analysis module is divided into four parts: a
	classifer, a syntax parser, a legal amount estimator and an evaluator.
	The classifier unit matches coarsely (classifire based on trees and
	using small features) classifies the input components into one of
	seven predefined primitives (these include digits and delimiters
	such as comma, period and dash see Fig. 5). The syntax parsing unit
	encodes the syntax of the typical styles for numeral representation
	shown in Fig. 2. The legal amount estimator determines the number
	of digits in the courtesy amount through a coarse segmentation and
	recognition of the numerical value of the check in words. Finally,
	the evaluator unit takes input from the syntax parser and legal amount
	estimator units and
	
	determines whether the segmentation was valid. If the segmentation
	is invalid, the evaluator directs the segmentation module and the
	classifier unit with the appropriate
	
	action to be taken.
	
	Test was done on 1000 bank checks and showed that the analysis mode
	reduce from 30 to 40% of the error. This was the only result given
	no final recognition rate. Only that the error was reduced. It also
	does not compare with other systems.},
  timestamp = {2010.2.24},
  url = {http://www.sciencedirect.com/science/article/B6V14-3W83VC2-1B/2/dd26998d2d5e3249216f915288abeb37}
}

@INPROCEEDINGS{PDBFavat1994,
  author = {J.T. FavatA G Srikantan, and S. N. Srihari},
  title = {Handprinted Character/DigitR ecognitionu singa Multiple Feature/ResolutioPnh
	itosophy},
  booktitle = {Proc. IWFHR},
  year = {1994},
  pages = {57-66},
  abstract = {this paper outli4es the philosophy, desrgn and implementation of the
	Gradient, Structural, bvily (GSC) recognition algorithm, which has
	been used successfully in several aGG"d rcading applications at CEDAR.
	The GSC algorithm takes a quasi multidio approach to feature generation.
	This philosophy coupled with the appropriaie *tf,aatim function results
	in a recognizer which has both high accuracy and good -'et-cc behavior.
	This allows it to be used in higher level digit string and word ggliti@
	algorithms which search for digit/character boundaries. Tests of
	the GSC Ner oDs tandardd igit, charactera nd non-characterd atabasesa
	rer eported.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBFavat1994.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{ARJawahar2009,
  author = {Jawahar, C. V. and Balasubramanian, A. and Meshesha, Million and
	Namboodiri, Anoop M.},
  title = {Retrieval of online handwriting by synthesis and matching},
  journal = {Pattern Recogn.},
  year = {2009},
  volume = {42},
  pages = {1445--1457},
  number = {7},
  abstract = {Search and retrieval is gaining importance in the ink domain due to
	the increase in the availability of online handwritten data. However,
	the problem is challenging due to variations in handwriting between
	various writers, digitizers and writing conditions. In this paper,
	we propose a retrieval mechanism for online handwriting, which can
	handle different writing styles, specifically for Indian languages.
	The proposed approach provides a keyboard-based search interface
	that enables to search handwritten data from any platform, in addition
	to pen-based and example-based queries. One of the major advantages
	of this framework is that information retrieval techniques such as
	ranking relevance, detecting stopwords and controlling word forms
	can be extended to work with search and retrieval in the ink domain.
	The framework also allows cross-lingual document retrieval across
	Indian languages.},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1016/j.patcog.2008.08.017},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARJawahar2009.pdf:PDF},
  issn = {0031-3203},
  keywords = {Search and retrieval, Handwriting synthesis, Online handwriting, Indian
	language scripts, Information retrieval},
  owner = {TOSHIBA},
  publisher = {Elsevier Science Inc.},
  timestamp = {2009.10.29}
}

@ARTICLE{ARJou2009,
  author = {Chichang Jou and Hung-Chang Lee},
  title = {Handwritten numeral recognition based on simplified structural classification
	and fuzzy memberships},
  journal = {Expert Systems with Applications},
  year = {2009},
  volume = {36},
  pages = {11858-11863},
  number = {9},
  month = {November 2009},
  abstract = {Previous handwritten numeral recognition algorithms applied structural
	classification to extract geometric primitives that characterize
	each image, and then utilized artificial intelligence methods, like
	neural network or fuzzy memberships, to classify the images. We propose
	a handwritten numeral recognition methodology based on simplified
	structural classification, by using a much smaller set of primitive
	types, and fuzzy memberships. More specifically, based on three kinds
	of feature points, we first extract five kinds of primitive segments
	for each image. A fuzzy membership function is then used to estimate
	the likelihood of these primitives being close to the two vertical
	boundaries of the image. Finally, a tree-like classifier based on
	the extracted feature points, primitives and fuzzy memberships is
	applied to classify the numerals. With our system, handwritten numerals
	in NIST Special Database 19 are recognized with correct rate between
	87.33% and 88.72%.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARJou2009.pdf:PDF},
  keywords = {Handwritten numeral recognition, Feature extraction, Structural classification,
	Fuzzy memberships},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{MCCao95,
  author = {JUN CAOt, M. AHMADI and M. SHRIDHAR},
  title = {RECOGNITION OF HANDWRITTEN NUMERALS WITH MULTIPLE FEATURE AND MULTISTAGE
	CLASSIFIER },
  journal = {Pattern Recoynition},
  year = {1995},
  volume = {28},
  pages = {153 160,},
  number = {2},
  abstract = {Multiple experts system is shown to be a promising strategy for handwritten
	recognition. This paper presents a multiple experts system using
	neural networks. In the proposed system, the authors have developed:
	(11 an incremental clustering neural network algorithm with merging
	and canceling process, (2) a modified directional histogram feature
	extraction method, and (3) a subclass method with learning rejection
	neuron strategy. Our experimental results on a large set of data
	show the efficiency and robustness of the proposed system.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCCao95pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{DSKapp2007,
  author = {Marcelo N. Kappa and Cinthia O. de A. Freitasb and Robert Sabourina},
  title = {Methodology for the design of NN-based month-word recognizers written
	on Brazilian bank checks},
  journal = {Image and Vision Computing},
  year = {2007},
  volume = {25},
  pages = {40-49},
  number = {1},
  month = {january 2007},
  abstract = {The study of handwritten words is tied to the development of recognition
	methods to be used in real-world applications involving handwritten
	words, such as bank checks, postal envelopes, and handwritten texts,
	among others. In this work, the focus is handwritten words in the
	context of Brazilian bank checks, specifically the months of the
	year, and no restrictions are placed on the types or styles of writing
	or the number of writers. A global feature set and two architectures
	of artificial neural networks (ANN) are evaluated for classification
	of the words. The objectives are to evaluate the performance of conventional
	and class-modular multiple-layer perceptron (MLP) architectures,
	to develop a rejection mechanism based on multiple thresholds, and
	to analyze the behavior of the feature set proposed in the two architectures.
	The experimental results demonstrate the superiority of the class-modular
	architecture over the conventional MLP architecture. A rejection
	mechanism with multiple thresholds demonstrates favorable performance
	in both architectures. The feature set analysis shows the importance
	of the structural primitives such as concavities and convexities,
	and perceptual primitives such as ascenders and descenders. The experimental
	results reveal a recognition rate of 81.75% without the rejection
	mechanism, and a reliability rate 91.52% with a rejection rate of
	25.33%.},
  booktitle = { },
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSKapp2007.pdf:PDF},
  keywords = {Neural networks; Rejection; Feature selection; Handwritten word recognition},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{ARKavianifar1999,
  author = {Mandana Kavianifar and Adnan Amin},
  title = {Preprocessing and Structural Feature Extraction for a Multi-Fonts
	Arabic / Persian OCR},
  booktitle = {Proceedings of the Fifth International Conference on Document Analysis
	and Recognition},
  year = {1999},
  abstract = {English and Chinese are languages, which have tremendously attracted
	interests of character recognition researchers. In contrast, research
	in the field of character recognition for Arabic / Persian scripts
	face major problems mainly related to the unique characteristics
	of these two like being cursive, multiple shapes of one character
	in different positions in a word and connectivity of characters on
	the baseline. The proposed work consists of three major phases. After
	digitizing the text, the original image is transformed into a gray
	scale image using a 300-dpi scanner. Different steps of preprocessing
	are then applied on the image file. In the next phase, sub-words
	of all words are recognized and global features for each word are
	extracted. Contour tracing plays a very important role in the phase
	of feature extraction.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARKavianifar1999.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@CONFERENCE{ARKESSENTINI2009,
  author = {Yousri KESSENTIN and Thierry PAQUET and AbdelMajid BEN HAMADOU},
  title = {A Multi-Lingual Recognition System for Arabic and Latin Handwriting},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {Generally, handwritten word recognition systems use script specific
	methodologies. In this paper, we present a unified approach for multi-lingual
	recognition of alphabetic scripts. The proposed system operates independently
	of the nature of the script using the multi-stream paradigm. The
	experiments have been carried out on a multi-script database composed
	of Arabic and Latin handwritten words from the IFN/ENIT and the IRONOFF
	public databases and show interesting recognition performances with
	only 1.5% of script confusion and an overall word recognition rate
	of 84.5% using a multi-script lexicon of 1142 words.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARKESSENTINI2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@ARTICLE{ARAssaleh2009,
  author = {Khaled Assaleha,c and Tamer Shanablehb and Husam Hajja},
  title = {Recognition of handwritten Arabic alphabet via hand motion tracking},
  journal = {Journal of the Franklin Institute},
  year = {2009},
  volume = {346},
  pages = {175–189},
  abstract = {This paperproposesanonlinevideo-based approachtohandwrittenArabicalphabetr
	cognition. Various temporalandspatialfeature extractiontechniquesareintroduced.
	the motion nformation of the handmovementisprojectedontotwostaticaccumulateddifferenceimagesac
	ordingto the motion directionality. the temporalanalysisisfollowed
	bytwo-dimensionaldiscret cosine transform andZonalcodingorRadon transformation
	andlowpassfiltering. the resulting feature vectors are time-independent
	thus canbeclassifiedbyasimpleclassificationtechn quesuchasK Nearest
	Neighbor(KNN). the solution is further enhanced by introducing the
	notionof uperclasses where similar classesaregrouped toge the rfor
	the purposeof multiresolutionalcl ssification. Experimental resultsindicateanimpressive
	99% recognitionrateonuser-dependantmode. o validate the proposed
	technique,we haveconductedaseriesof experimentsusingHidde Markov
	models (HMM),whichis the classicalwayofclassifyingdatawithtemporaldependenc
	es. Experimental resultsrevealedthat the proposedfeatureextractionschemecombinedwiths
	mple KNN yields superiorresults tothoseobtainedby the classicalHMM-basedscheme},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARAssaleh2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{ARKherallah2009,
  author = {M. Kherallah and F. Bouri and A.M. Alimi},
  title = {On-line Arabic handwriting recognition system based on visual encoding
	and genetic algorithm},
  journal = {Engineering Applications of Artificial Intelligence},
  year = {2009},
  volume = {22},
  pages = {153–170},
  abstract = {One of the most promising methods of interacting with small portable
	computing devices, such as personal digital assistants, is the use
	of handwriting. In order to make this communication method more natural,
	we propose to observe visually the writing process on an ordinary
	paper and to automatically recover the pen trajectory from numerical
	tablet sequences. On the basis of this work, we developed a handwriting
	recognition system based on visual coding and genetic algorithm ‘‘GA’’.
	The system is applied on Arabic script. In this paper, we will present
	the different steps of the handwriting recognition system. We focus
	our contribution on the encoding system and the fitness function
	conception used as basic steps of the GA. A new approach based on
	visual indices similarity is developed to calculate the evaluation
	function. We optimize the times cooling of our system to give the
	final output (proposed words). Several experimentations are developed
	using an Arabic data set words extracted from ‘‘LMCA’’ database elaborated
	in our laboratory by 24 participants. The results obtained are very
	promising and prove that our new method based on hybridization between
	visual codes and GA is a powerful method.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARKherallah2009.pdf:PDF},
  keywords = {Arabic script, Stroke overlapping, Beta-elliptical representation,
	Visual encoding,Word recognition},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{ARKhosravi2007,
  author = {Hossein Khosravi and Ehsanollah Kabir},
  title = {Introducing a very large dataset of handwritten Farsi digits and
	a study on their varieties},
  journal = {Pattern Recognition Letters},
  year = {2007},
  volume = {28},
  pages = {1133–1141},
  abstract = {A very large dataset of handwritten Farsi digits is introduced. Binary
	images of 102,352 digits were extracted from about 12,000 registration
	forms of two types, filled by B.Sc. and senior high school students.
	These forms were scanned at 200 dpi with a high speed scanner. A
	method for finding variety of handwritten digits in a typical dataset
	is proposed. Based on this method, training and test subsets are
	provided to facilitate sharing of results among researchers as well
	as performance comparison.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARKhosravi2007.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARKim2006,
  author = {Dae Hwan Kim and Hyun Il Choi and Jin Hyung Kim},
  title = {3D Space Handwriting Recognition with Ligature Model},
  booktitle = {UCS 2006, Ubiquitous Computing Systems, Third International Symposium},
  year = {2006},
  editor = {Hee Yong Youn and Minkoo Kim and Hiroyuki Morikawa},
  volume = {4239},
  series = {Lecture Notes in Computer Science LNCS},
  pages = {41-56},
  address = {Seoul, Korea},
  month = {October 11-13},
  publisher = {Springer},
  abstract = {3D space handwriting is a gesture-like character written in the air,
	and it is a promising input method for its portability. In this work,
	we propose a practical development of 3D space handwriting recognition
	system by combining 2D handwriting and the ligature of 3D handwriting
	based on that the most different part between these handwritings
	is the shape of ligature. We designed a ligature model not to depend
	on each character shape but to depend only on incoming and outgoing
	vectors. Therefore with a few ligature models, various ligature shapes
	are modeled. By using 2D space handwriting models and data, we can
	use existing models for various writing styles, and overcome the
	problem of the lack of data.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARKim2006.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{DSKim2001,
  author = {Kye Kyung Kim and Jin Ho Kim and Yun Koo Chung and Suen, C.Y.},
  title = {Legal amount recognition based on the segmentation hypotheses for
	bank check processing},
  booktitle = {Proceedings. Sixth International Conference on Document Analysis
	and Recognition, 2001.},
  year = {2001},
  pages = {964 -967},
  abstract = {A sophisticated methodology of legal amount recognition based on the
	word segmentation hypotheses is introduced for automatic bank check
	processing. Word segmentation hypotheses are derived according to
	the grapheme level segmentation results of the legal amount. Novel
	hybrid schemes of HMM-MLP classifiers are also introduced for producing
	the ordered legal word recognition results with reliable decision
	values. These values can be used for obtaining an optimal word segmentation
	path of over-segmentation hypotheses as well as an efficient rejection
	criterion of word recognition result. Simulation was performed with
	CENPARMI bank check database and shows quite encouraging results},
  doi = {10.1109/ICDAR.2001.953928},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSKim2001.pdf:PDF},
  journal = {Document Analysis and Recognition, 2001. Proceedings. Sixth International
	Conference on},
  keywords = {CENPARMI;HMM;bank check database;bank check processing;decision values;grapheme
	level segmentation;handwriting recognition;hidden Markov model;legal
	amount recognition;multilayer perceptron;word segmentation hypotheses;bank
	data processing;cheque processing;document image processing;handwritten,
	;hidden Markov models;image segmentation;multilayer perceptrons;optical,
	;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@ARTICLE{DSKoch2005,
  author = {G. Koch and L. Heutte and T. Paquet},
  title = {Automatic extraction of numerical sequences in handwritten incoming
	mail documents},
  journal = {Pattern Recognition Letters},
  year = {2005},
  volume = {26},
  pages = {1118 - 1127},
  number = {8},
  abstract = {In this paper, we propose a method for the automatic extraction of
	numerical fields in handwritten documents. The approach exploits
	the known syntactic structure of the numerical field to extract,
	combined with a set of contextual morphological features to find
	the best label for each connected component. Applying a Markov model
	based syntactic analyzer on the overall document allows to localize/extract
	fields of interest. Reported results on the extraction of zip codes,
	phone numbers and customer codes from handwritten incoming mail documents
	demonstrate the interest of the proposed approach.},
  doi = {DOI: 10.1016/j.patrec.2004.10.006},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSKoch2005.pdf:PDF},
  issn = {0167-8655},
  keywords = {Document analysis},
  owner = {Maha},
  review = {The system extract an label the location of number from the letters
	or documents. The first step is divided all the conected component
	of the whole document. The extraction is of numeric is then the lablel
	of the connected component is done by simple procedure and features
	like the size of connected component. ( we only have three classes
	, reject , digit or seperator. A sytatical anyliser is added using
	HMM to have correct sequence of lableling and correct label or reject
	the label previously given. the result was tested on non overlapping
	digits of 292 documents. a recognition rate of 41 % , 69 and 60%
	on different types of digit types (different syntactical analysisers).
	No comparison to similar systems.},
  timestamp = {2010.2.24},
  url = {http://www.sciencedirect.com/science/article/B6V15-4DTTHDM-5/2/2325ca93a5882505f71e923c7a1d52e9}
}

@INPROCEEDINGS{DSKornai1995,
  author = {Andras Kornai and K. M. Mohiuddin and Scott D. Connell},
  title = {An HMM-Based Legal Amount Field OCR System for Checks},
  booktitle = {IEEE International Conference on Systems, Man and Cybernetics, Vancouver
	BC},
  year = {1995},
  pages = {2800--2805},
  abstract = {The system described in this paper applies Hidden Markov technology
	to the task of recognizing the handwritten legal amount on personal
	checks. We argue that the most significant source of error in handwriting
	recognition is the segmentation process. In traditional handwriting
	OCR systems, recognition is performed at the character level, using
	the output of an independent segmentation step. Using a fixed stepsize
	series of vertical slices from the image, the HMM system described
	in this paper avoids taking segmentation decisions early in the recognition
	process. 0 Introduction The current generation of Optical Character
	Recognition (OCR) systems can be characterized as a pipeline composed
	of Preprocessing, Segmentation, Classification, and Identification
	stages. None of these stages are immune to error. Preprocessing may
	fail to remove existing noise, it may remove portions of the image
	or add noise by some other mechanism. Segmentation may fail to establish
	a boundary where there sh...},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSKornai1995.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.02.25}
}

@ARTICLE{ARKumara2008,
  author = {Karthik Kumara and Rahul Agrawal and Chiranjib Bhattacharyya},
  title = {A large margin approach for writer independent online handwriting
	classification},
  journal = {Pattern Recognition Letters},
  year = {2008},
  volume = {29},
  pages = {933–937},
  abstract = {This paper proposes a new approach for classifying multivariate time-series
	with applications to the problem of writer independent online handwritten
	character recognition. Each time-series is approximated by a sum
	of piecewise polynomials in a suitably defined Reproducing Kernel
	Hilbert Space (RKHS). Using the associated kernel function a large
	margin classification formulation is proposed which can discriminate
	between two such functions belonging to the RKHS. The associated
	problem turns out to be an instance of convex quadratic programming.
	The resultant classification scheme applies to many time-series discrimination
	tasks and shows encouraging results when applied to online handwriting
	recognition tasks.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARKumara2008.pdf:PDF},
  keywords = {Time-series classification; Kernels; Online handwriting recognition},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{MCKussul2005,
  author = {Ernst Kussul and Tatiana Baidyk},
  title = {Improved method of handwritten digit recognition tested on MNIST
	database},
  journal = {Image and Vision Computing},
  year = {2005},
  volume = {22},
  pages = {971–981},
  abstract = {We have developed a novel neural classifier LImited Receptive Area
	(LIRA) for the image recognition. The classifier LIRA contains three
	neuron layers: sensor, associative and output layers. The sensor
	layer is connected with the associative layer with no modifiable
	random connections and the associative layer is connected with the
	output layer with trainable connections. The training process converges
	sufficiently fast. This classifier does not use floating point and
	multiplication operations. The classifier was tested on two image
	databases. The first database is the MNIST database. It contains
	60,000 handwritten digit images for the classifier training and 10,000
	handwritten digit images for the classifier testing. The second database
	contains 441 images of the assembly microdevice. The problem under
	investigation is to recognize the position of the pin relatively
	to the hole. A random procedure was used for partition of the database
	to training and testing subsets. There are many results for the MNIST
	database in the literature. In the best cases, the error rates are
	0.7, 0.63 and 0.42%. The classifier LIRA gives error rate of 0.61%
	as a mean value of three trials. In task of the pin–hole position
	estimation the classifier LIRA also shows sufficiently good results.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCKussul2005.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{DSLam1995,
  author = {L. Lam and C. Y. Suen and Lam Suen Guillevic and N. W. Strathy Amdg
	and M. Cheriet and K. Liu and J. N. Said},
  title = {Automatic Processing of Information on Cheques},
  year = {1995},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSLam1995.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.02.25}
}

@ARTICLE{FE3Lauer2007,
  author = {Fabien Lauer and Ching Y. Suen and Gerard Bloch},
  title = {A trainable feature extractor for handwritten digit recognition},
  journal = {Pattern Recognition},
  year = {2007},
  volume = {40},
  pages = {1816-1824},
  abstract = {This article focuses on the problems of feature extraction and the
	recognition of handwritten digits. A trainable feature extractor
	based on the LeNet5 convolutional neural network architecture is
	introduced to solve the first problem in a black box scheme without
	prior knowledge on the data. The classification task is performed
	by support vector machines to enhance the generalization ability
	of LeNet5. In order to increase the recognition rate, new training
	samples are generated by affine transformations and elastic distortions.
	Experiments are performed on the well-known MNIST database to validate
	the method and the results show that the system can outperform both
	SVMs and LeNet5 while providing performances comparable to the best
	performance on this database. Moreover, an analysis of the errors
	is conducted to discuss possible means of enhancement and their limitations.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\FE3Lauer2007.pdf:PDF},
  keywords = {Features Extraction, Handwritten digits},
  owner = {TOSHIBA},
  review = {Details:
	
	Exapand dataset by using geomertical transforms (skew, shear...) and
	errode. 
	
	Feature extraction using raw data and lenet 5 : Use Lenet 5 to genarate
	the features with deleting the last two layers of lenet5 and useing
	the ouptut as feature vector. 
	
	classification using svm 
	
	
	result are 99.17 
	
	uses MNIST
	
	
	Qoutes============= From the paper =============
	
	 Thus, the paper showed how a convolutional network architecture may
	be used to build a feature extractor and how this allows the combination
	of two superb methods to enhance a recognition system. However, the
	overall complexity resulting from the combination of two algorithms
	may be a drawback in certain cases. For real world problems, the
	user must choose an algorithm according to the trade-off between
	speed and quality that fits the constraints of the application.
	
	
	
	An analysis of the errors allowed to conclude that the performance
	could not be increased above a certain limit, because of bad samples
	in the test set not recognizable without ambiguity by humans. Nonetheless
	some error patterns are very clear and are still misrecognized because
	of their structure and the lack of samples of the same prototype
	in the training set. Regarding these errors, the generation of more
	samples of rare prototypes could lead to further improvement.
	
	==============================================},
  timestamp = {2009.10.03}
}

@INPROCEEDINGS{DSLecce2000,
  author = {V. Di Lecce and G. Dimauro and A. Guerriero and S. Impedovo and G
	Pirlo and A. Salzo},
  title = {A New Hybrid Approach For Legal Amount Recognition},
  booktitle = {In Proceedings of International Workshop on Frontiers in Handwriting
	Recognition},
  year = {2000},
  pages = {199--208},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSLecce2000.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.02.25}
}

@ARTICLE{MCLecun1998a,
  author = {Yann LeCun and Leon Bottou and  Yoshua Bengio and Patrick Haffner},
  title = {GradientBased Learning Applied to Document Recognition},
  journal = {PROC OF THE IEEE},
  year = {1998},
  volume = {8},
  pages = {1-10},
  month = {NOVEMBER},
  abstract = {Multilayer Neural Networks trained with the backpropa- gation algorithm
	constitute the best example of a successful Gradient-Based Learning
	technique.Given an appropriate network architecture, Gradient-Based
	Learning algorithms can be used to synthesize a complex decision
	surface that can classify high-dimensional patterns such as handwritten
	char- acters, with minimal preprocessing.This paper reviews var-
	ious methods applied to handwritten character recognition and compares
	them on a standard handwritten digit recog- nition task.Convolutional
	Neural Networks, that are specif- ically designed to deal with the
	variability of shapes, are shown to outperform all other techniques.Real-life
	document recognition systems are composed of multiple modules including
	eld extraction, segmenta- tion, recognition, and language modeling.A
	new learning paradigm, called Graph Transformer Networks (GTN), al-
	lows such multi-module systems to be trained globally using Gradient-Based
	methods so as to minimize an overall per- formance measure.Two systems
	for on-line handwriting recognition are de- scribed.Experiments demonstrate
	the advantage of global training, and the exibility of Graph Transformer
	Networks.A Graph Transformer Network for reading bank check is also
	described.It uses Convolutional Neural Network char- acter recognizers
	combined with global training techniques to provides record accuracy
	on business and personal checks.It is deployed commercially and reads
	several million checks per day},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCLecun1998a.pdf:PDF},
  keywords = {Neural Networks, OCR, Document Recognition ,Machine Learning,
	GradientBased Learning, Convolutional Neural Networks ,Graph Transformer
	Networks, Finite State Transducers},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{MCLecun1998b,
  author = {Y. LeCun and L. Bottou and Y. Bengio and P. Haffner},
  title = {Gradient Based Learning Applied to Document Recognition},
  booktitle = {Proceedings of the IEEE},
  year = {1998},
  volume = {86},
  number = {11},
  pages = {2278-2324},
  month = {November},
  abstract = {A long and detailed paper on convolutional nets, graph transformer
	networks, and discriminative training methods for sequence labeling.
	We show how to build systems that integrate segmentation, feature
	extraction, classification, contextual post-processing, and language
	modeling into one single learning machine trained end-to-end. Applications
	to handwriting recognition and face detection are described.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCLecun1998b.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{MCLee1999,
  author = {Jin-Soo Lee and Oh-Jun Kwon and Sung-Yang Bang},
  title = {Highly accurate recognition of printed Korean characters through
	an improved two-stage classication method},
  journal = {Pattern Recognition},
  year = {1999},
  volume = {32},
  pages = {1935-1945},
  abstract = {This paper presents a recognition system which obtains a recognition
	rate higher than 99% for the printed Korean characters of multifont
	and multisize. We recognize a given input by "rst identifying the
	character type of the input and then recognizing its constituent
	graphemes. In order to improve the performance we incorporated three
	new ideas in our system: the expansion of the subimage areas used
	by the grapheme classi"ers, an algorithm to accurately segment the
	horizontal vowel's subimage areas, and a validation process to evaluate
	the result of the type classi"er. Through experiments we con"rmed
	that our system performs well in a multi-font and multi-size environment
	and that those three ideas actually contributed to improve the performance
	signi"cantly},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCLee1999.pdf:PDF},
  keywords = {Korean characters; OCR; Neural networks; Multi-font/multi-size; Grapheme
	recognition; Character type},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{PDBLi2003,
  author = {Li, Jinyan and Liu, Huiqing},
  title = {Ensembles of Cascading Trees},
  booktitle = {Proceedings of the 3rd IEEE International Conference on Data Mining
	(ICDM 2003), 19-22 December 2003},
  year = {2003},
  pages = {585},
  address = {Melbourne, Florida, USA},
  publisher = {IEEE Computer Society, Washington, DC, USA},
  abstract = {We introduce a new method, called CS4, to construct committees of
	decision trees for classification. The method considers different
	top-ranked features as the root nodes of member trees. This idea
	is particularly suitable for dealing with high-dimensional bio-medical
	data as top-ranked features in this type of data usually possess
	similar merits for classification. To make a decision, the committee
	combines the power of individual trees in a weighted manner. Unlike
	Bagging or Boosting which uses bootstrapped training data, our method
	builds all the member trees of a committee using exactly the same
	set of training data. We have tested these ideas on UCI data sets
	as well as recent bio-medical data sets of gene expression or proteomic
	profiles that are usually described by more than 10,000 features.
	All the experimental results show that our method is efficient and
	that the classification performance are superior to C4.5 family algorithms.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://csdl.computer.org/comp/proceedings/icdm/2003/1978/00/19780585abs.htm},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBLi2003.pdf:PDF},
  isbn = {0-7695-1978-4},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@ARTICLE{FE4Liu2003,
  author = {Cheng Lin Liu and Kazuki Nakashima and Hiroshi Sako and Hiromichi
	Fujisawa},
  title = {Handwritten digit recognition: benchmarking of state-of-the-art techniques},
  journal = {Pattern Recognition},
  year = {2003},
  volume = {36},
  pages = {2271-2285},
  abstract = {This paper presents the results of handwritten digit recognition on
	well-known image databases using state-of-the-art feature extraction
	and classification techniques. The tested databases are CENPARMI,
	CEDAR, and MNIST. On the test data set of each database, 80 recognition
	accuracies are given by combining eight classifiers with ten feature
	vectors. The features include chaincode feature, gradient feature,
	profile structure feature, and peripheral direction contributivity.
	The gradient feature is extracted from either binary image or gray-scale
	image. The classifiers include the k-nearest neighbor classifier,
	three neural classifiers, a learning vector quantization classifier,
	a discriminative learning quadratic discriminant function (DLQDF)
	classifier, and two support vector classifers (SVCs). All the classifers
	and feature vectors give high recognition accuracies. Relatively,
	the chaincode feature and the gradient feature show advantage over
	other features, and the profle structure feature shows eficiency
	as a complementary feature. The SVC with RBF kernel (SVC-rbf) gives
	the highest accuracy in most cases but is extremely expensive in
	storage and computation. Among the non-SV classifiers, the polynomial
	classifier and DLQDF give the highest accuracies. The results of
	non-SV classifiers are competitive to the best ones previously reported
	on the same databases.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\FE4Liu2003.pdf:PDF},
  keywords = {MNIST, Survey, Classifiers, Handwritten digits},
  owner = {TOSHIBA},
  review = {Comments:
	
	Compares 6 different features with 6 different classifiers 
	
	simillar to paper FE2 and FE3 
	
	
	The error rate is 0.54. 
	
	
	the features are 1) chain code 
	
	2) gradient 
	
	profile structure 
	
	concativity or convex hull 
	
	crossing point .},
  timestamp = {2009.10.03}
}

@ARTICLE{ARLiu2009,
  author = {Cheng-Lin Liu and ChingY.Suenb},
  title = {A new benchmark on the recognition of handwritten Bangla and Farsi
	numeral characters},
  journal = {Pattern Recognition},
  year = {2009},
  volume = {42},
  pages = {3287--3295},
  number = {12},
  abstract = {The recognition of Indian and Arabic handwriting is drawing increasing
	attention in recent years. To test the promise of existing handwritten
	numeral recognition methods and provide new benchmarks for future
	research, this paper presents some results of handwritten Bangla
	and Farsi numeral recognition on binary and gray-scale images. For
	recognition on gray-scale images, we propose a process with proper
	image pre-processing and feature extraction. In experiments on three
	databases, ISI Bangla numerals, CENPARMI Farsi numerals, and IFHCDB
	Farsi numerals, we have achieved very high accuracies using various
	recognition methods. The highest test accuracies on the three databases
	are 99.40%, 99.16%, and 99.73%, respectively. We justified the benefit
	of recognition on gray-scale images against binary images, compared
	some implementation choices of gradient direction feature extraction,
	some advanced normalization and classification methods.},
  doi = {http://dx.doi.org/10.1016/j.patcog.2008.10.007},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARLiu2009.pdf:PDF},
  issn = {0031-3203},
  keywords = {Bangla numeral recognition; Farsi numeral recognition; Pre-processing;
	Feature extraction; Classification},
  owner = {TOSHIBA},
  publisher = {Elsevier Science Inc.},
  timestamp = {2009.10.29}
}

@ARTICLE{ARLiu2008b,
  author = {Cheng-Lin Liu and Hiromichi Fujisawa},
  title = {Classification and Learning Methods for Character Recognition: Advances
	and Remaining Problems},
  journal = {Studies in Computational Intelligence (SCI)},
  year = {2008},
  volume = {90},
  pages = {139–161},
  abstract = {Pattern classification methods based on learning-from-examples have
	been widely applied to character recognition from the 1990s and have
	brought forth significant improvements of recognition accuracies.
	This kind of methods include statistical methods, artificial neural
	networks, support vector machines, multiple classifier combination,
	etc. In this chapter, we briefly review the learning-based classification
	methods that have been successfully applied to character recognition,
	with a special section devoted to the classification of large category
	set. We then discuss the characteristics of these methods, and discuss
	the remaining problems in character recognition that can be potentially
	solved by machine learning methods.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARLiu2008b.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{PDBLiu2004,
  author = {Cheng-Lin Liu and Kazuki Nakashima and Hiroshi Sako and Hiromichi
	Fujisawa},
  title = {Handwritten digit recognition: investigation of normalization and
	feature extraction techniques},
  journal = {Pattern Recognition},
  year = {2004},
  volume = {37},
  pages = {265 – 279},
  abstract = {The performance evaluation of various techniques is important to select
	the correct options in developing character recognition systems.
	In our previous works, we have proposedaspect ratio adaptive normalization
	(ARAN) andhave evaluated the performance of state-of-the-art feature
	extraction andclassi:cation techniques. For this time, we will propose
	some improvednormalization functions andd irection feature extraction
	strategies andwill compare their performance with existing techniques.
	We compare ten normalization functions (seven basedon dimensions
	andthree basedon moments) andeight feature vectors on three distinct
	data sources. The normalization functions and feature vectors are
	combined to produce eighty classi:cation accuracies to each dataset.
	The comparison of normalization functions shows that moment-based
	functions outperform the dimension-based ones and the aspect ratio
	mapping is in<uential. The comparison of feature vectors shows that
	the improvedfeature extraction strategies outperform their baseline
	counterparts. The gradient feature from gray-scale image mostly yields
	the best performance andthe improvedNCFE (normalization-cooperatedfeature
	extraction) features also perform well. The combinede>ects of normalization,
	feature extraction, andclassi:cation have yieldedvery high accuracies
	on well-known datasets.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBLiu2004.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{ARLiwicki2008,
  author = {Marcus Liwicki and HorstBunkeb},
  title = {Combining diverse on-line and off-line systems for handwritten text
	line recognition},
  journal = {Pattern Recognition},
  year = {2008},
  volume = {42},
  pages = {3254-3263},
  number = {12},
  month = {December 2009},
  abstract = {In this paper we present a multiple classifier system (MCS) for on-line
	handwriting recognition. The MCS combines several individual recognition
	systems based on hidden Markov models (HMMs) and bidirectional long
	short-term memory networks (BLSTM). Beside using two different recognition
	architectures (HMM and BLSTM), we use various feature sets based
	on on-line and off-line features to obtain diverse recognizers. Furthermore,
	we generate a number of different neural network recognizers by changing
	the initialization parameters. To combine the word sequences output
	by the recognizers, we incrementally align these sequences using
	the recognizer output voting error reduction framework (ROVER). For
	deriving the final decision, different voting strategies are applied.
	The best combination ensemble has a recognition rate of 84.13%, which
	is significantly higher than the 83.64% achieved if only one recognition
	architecture (HMM or BLSTM) is used for the combination, and even
	remarkably higher than the 81.26% achieved by the best individual
	classifier. To demonstrate the high performance of the classification
	system, the results are compared with two widely used commercial
	recognizers from Microsoft and Vision Objects.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARLiwicki2008.pdf:PDF},
  keywords = {On-line handwriting recognition; Off-line handwriting recognition;
	Multiple classifier combination; Hidden Markov models; Bidirectional
	long short-term memory networks},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@CONFERENCE{ARLopresti2006,
  author = {Daniel Lopresti and George Nagy and Sharad Seth and Xiaoli Zhang},
  title = {Multi-character Field Recognition for Arabic and Chinese Handwriting},
  booktitle = {SACH06},
  year = {2006},
  pages = {xx-yy},
  abstract = {Two methods, Symbolic Indirect Correlation (SIC) and Style Constrained
	Classification (SCC), are proposed for recognizing handwritten Arabic
	and Chinese words and phrases. SIC reassembles variablelength segments
	of an unknown query that match similar segments of labeled reference
	words. Recognition is based on the correspondence between the order
	of the feature vectors and of the lexical transcript in both the
	query and the references. SIC implicitly incorporates language context
	in the form of letter n-grams. SCC is based on the notion that the
	style (distortion or noise) of a character is a good predictor of
	the distortions arising in other characters, even of a different
	class, from the same source. It is adaptive in the sense that, with
	a long-enough field, its accuracy converges to that of a style-specific
	classifier trained on the writer of the unknown query. Neither SIC
	nor SCC requires the query words to appear among the references.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARLopresti2006.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{ARLorigo2006,
  author = {Liana M. Lorigo and Venu Govindaraju},
  title = {Offline Arabic Handwriting Recognition: A Survey},
  journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
  year = {2006},
  volume = {28},
  pages = {712-724},
  number = {5},
  month = {MAY},
  abstract = {The automatic recognition of text on scanned images has enabled many
	applications such as searching for words in large volumes of documents,
	automatic sorting of postal mail, and convenient editing of previously
	printed documents. The domain of handwriting in the Arabic script
	presents unique technical challenges and has been addressed more
	recently than other domains. Many different methods have been proposed
	and applied to various types of images. This paper provides a comprehensive
	review of these methods. It is the first survey to focus on Arabic
	handwriting recognition and the first Arabic character recognition
	survey to provide recognition rates and descriptions of test data
	for the approaches discussed. It includes background on the field,
	discussion of the methods, and future research directions.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARLorigo2006.pdf:PDF},
  keywords = {Computer vision, document analysis, handwriting analysis, optical},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{DSLou2008,
  author = {Zhen Lou and Jing-Yu Yang and Zhong Jin},
  title = {Recognition and checkout of legal amounts on chinese bank cheques},
  booktitle = {International Conference on Wavelet Analysis and Pattern Recognition,
	2008. ICWAPR '08. },
  year = {2008},
  volume = {1},
  pages = {395 -400},
  month = {August},
  abstract = { A system for the recognition and checkout of legal amounts on Chinese
	bank cheques was presented in the paper. Firstly, pianpang recognizer
	was proposed to combine any pianpang into a neighbor segment. Then,
	the width model of characters was proposed to determine the predicted
	positions of split lines. Finally, an efficient state transfer model
	was proposed to conduct Subset-based recognition. Experiments had
	been performed on more than 4,000 cheques from China Construction
	Bank. The proposed system was shown to be effective and efficient.
	Without (with) the reference values, the correct checkout rate of
	the best segmentation can reach as high as 85.83% (91.02%).},
  doi = {10.1109ICWAPR.2008.4635811},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSLou2008.pdf:PDF},
  keywords = {China Construction Bank;Chinese bank cheques;neighbor segment;pianpang
	recognizer;bank data processing;cheque processing;handwritten, ;image
	segmentation;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{PDBLuo2005,
  author = {Huitao Luo},
  title = {Optimization Design of Cascaded Classifiers.},
  booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern
	Recognition (CVPR 2005)},
  year = {2005},
  pages = {480-485},
  address = {San Diego, CA, USA},
  month = { 20-26 June},
  publisher = {IEEE Computer Society},
  abstract = {We present two optimization algorithms for the design of a cascade
	of classifiers, which is becoming a popular choice formany classification
	problems. Both algorithms represent each node classifier of a cascade
	using a high-level abstraction model and attempt to jointly optimize
	the setting of the thresholding parameters of all the node classifiers
	within the cascade. We applied both algorithms to optimize the famous
	Viola and Jones face detector and one of them in particular greatly
	improved the performance. We believe both algorithms can serve as
	a useful post-processing process for general cascaded classifier
	design},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://dx.doi.org/10.1109/CVPR.2005.266},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBLuo2005.pdf:PDF},
  isbn = {0-7695-2372-2},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@INPROCEEDINGS{DSMarisa2002,
  author = {Morita M and Oliveira L.S. and Sabourin R. and Bortolozzi F. and
	Suen C.Y.},
  title = {An HMM-MLP Hybrid System to Recognize Handwritten Dates},
  booktitle = {International Joint Conference on Neural Networks (IJCNN 2002)},
  year = {2002},
  pages = { 867-872},
  address = { Honolulu, USA},
  month = { May 12-17},
  publisher = { IEEE Computer Society Press},
  abstract = {This paper presents an HMM-MLP hybrid system to process complex date
	images written on Brazilian bank cheques. The system first segments
	implicitly a date image into sub-fields through the recognition process
	based on anHMMapproach. Afterwards, a recognition and verification
	strategy is proposed to recognize the three obligatory date sub-fields
	(day, month and year) using different classifiers. Markovian and
	neural approaches have been adopted to recognize and verify words
	and strings of digits respectively. We also introduce the concept
	of meta-classes of digits, which is used to reduce the lexicon size
	of the day and year and improve the precision of their segmentation
	and recognition. Experiments show interesting results on date recognition.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSMarisa2002.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@ARTICLE{ARAdankon2008,
  author = {Mathias M.Adankon and MohamedCheriet},
  title = {Model selection for the LS-SVM Application to handwriting recognition},
  journal = {Pattern Recognition},
  year = {2008},
  volume = {42},
  pages = {3264-3270},
  number = {12},
  month = {December},
  abstract = {The support vector machine (SVM) is a powerful classifier which has
	been used successfully in many pattern recognition problems. It has
	also been shown to perform well in the handwriting recognition field.
	The least squares SVM (LS-SVM), like the SVM, is based on the margin-maximization
	principle performing structural risk minimization. However, it is
	easier to train than the SVM, as it requires only the solution to
	a convex linear problem, and not a quadratic problem as in the SVM.
	In this paper, we propose to conduct model selection for the LS-SVM
	using an empirical error criterion. Experiments on handwritten character
	recognition show the usefulness of this classifier and demonstrate
	that model selection improves the generalization performance of the
	LS-SVM},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARAdankon2008.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{DSMadasu2005,
  author = { Madasu, V.K. and Lovell, B.C.},
  title = {Automatic Segmentation and Recognition of Bank Cheque Fields},
  booktitle = { Proceedings 2005 of Digital Image Computing: Techniques and Applications,
	2005. DICTA '05.},
  year = {2005},
  pages = { 33 - 33},
  month = {dec.},
  abstract = { This paper describes a novel method for automatically segmenting
	and recognizing the various information fields present on a bank
	cheque. The uniqueness of our approach lies in the fact that it doesn
	#146;t necessitate any prior information and requires minimum human
	intervention. The extraction of segmented fields is accomplished
	by means of a connectivity based approach. For the recognition part,
	we have proposed four innovative features, namely; entropy, energy,
	aspect ratio and average fuzzy membership values. Though no particular
	feature is pertinent in itself but a combination of these is used
	for differentiating between the fields. Finally, a fuzzy neural network
	is trained to identify the desired fields. The system performance
	is quite promising on a large dataset of real and synthetic cheque
	images.},
  doi = {10.1109/DICTA.2005.18},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSMadasu2005.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSMadasu2005b,
  author = { Madasu, V. and Lovell, B.C.},
  title = {Automatic Segmentation and Recognition of Bank Cheque Fields},
  booktitle = {Digital Image Computing: Technqiues and Applications, 2005. DICTA
	' 05. Proceedings},
  year = {2005},
  pages = { 223 - 228},
  month = {dec.},
  doi = {10.1109/DICTA.2005.1578131},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSMadasu2005b.pdf:PDF},
  issn = { },
  owner = {Maha},
  timestamp = {2010.2.24}
}

@CONFERENCE{ARMaddouri2008,
  author = {Samia Snoussi Maddouri and Fadoua Bouafif Samoud and Kaouthar Bouriel
	and Noureddine Ellouze and Haikal El Abed},
  title = {Baseline Extraction: Comparison of Six Methods on IFN/ENIT Database},
  booktitle = {The 11th International Conference on Frontiers in Handwriting Recognition},
  year = {2008},
  abstract = {Baseline extraction is hailed as an important step in handwriting
	primitive extraction process, seen the insights it proffers into
	the position and the length of the word. It further facilitates feature
	extraction. As regards Arabic words, tow baselines can be extracted:
	an upper baseline and a lower one. These tow lines divide the word
	into three parts, namely, Ascender and upper diacritic points above
	the upper baseline, Descender and lower diacritic points under the
	lower baseline. The main content of the word lies between the two
	baselines, it generally involves loops. In this paper we start with
	a presentation of six baseline extraction methods. These methods
	are developed and evaluated with reference to IFN/ENIT-database .
	Some of them are combined in order to improve baseline position.
	A comparison between these methods is made on the basis of the IFN/ENIT-database.
	Results on the set a of IFN/ENIT-database evince that the Skeletonbased
	method and the Min-Max & Primitives achieve very promising results
	reaching about 77% of good results and about 87% of acceptable results},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARMaddouri2008.pdf:PDF},
  keywords = {Baseline, projection, Entropy, Skeleton, Hough transform.},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{ARSabri2006,
  author = {Sabri A. Mahmoud and Ashraf S. Mahmoud},
  title = {Arabic Character Recognition using Modified Fourier Spectrum (MFS).},
  booktitle = {2006 International Conference on Geometric Modeling and Imaging (GMAI
	2006)},
  year = {2006},
  pages = {155-159},
  address = { London, UK},
  month = { 5-7 July 2006},
  publisher = {IEEE Computer Society},
  abstract = {Arabic character recognition algorithm using Modified Fourier Spectrum
	(MFS) is presented. The MFS descriptors are estimated by applying
	the Fast Fourier Transform (FFT) to the Arabic character primary
	part contour. Ten descriptors are estimated from the Fourier spectrum
	of the character primary part contour by subtracting the imaginary
	part from the real part (and not from the amplitude of the Fourier
	spectrum as is usually the case). These descriptors are then used
	in the training and testing of Arabic characters. The computation
	of the MFS descriptors requires less computation time than the computation
	of the Fourier descriptors. Experimental results have shown that
	the MFS features are suitable for Arabic character recognition. Average
	recognition rate of 95.9% was achieved for the model classes. The
	analysis of the errors indicates that this recognition rate can be
	improved by using the “hole” feature of a character and use cleaning
	corrupted data.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://doi.ieeecomputersociety.org/10.1109/GMAI.2006.8},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARSabri2006.pdf:PDF},
  isbn = {0-7695-2604-7},
  owner = {TOSHIBA},
  timestamp = {2009.10.08}
}

@CONFERENCE{ARMane2009,
  author = {Vanita Mane and Lena Ragha},
  title = {Handwritten Character Recognition using Elastic Matching and PCA},
  booktitle = {International Conference on Advances in Computing, Communication
	and Control},
  year = {2009},
  abstract = {Recognition of alphabetic characters is a basic need in incorporating
	intelligence to computers. Machine intelligence involves several
	aspects among which optical recognition is a tool, which can be integrated
	to text recognition. To make these aspects effective character recognition
	with better accuracy is important. However, handwritten character
	recognition is still a difficult task because of the high variability
	in the character shapes written by individuals. While large amount
	of work has been done towards recognition of handwritten English
	characters relatively less work is reported for the recognition of
	Indian language scripts. So, we proposed a new elastic image matching
	(EM) technique based on an eigen-deformation for recognition of offline
	isolated English uppercase handwritten characters and offline isolated
	handwritten characters of Devnagari ,the most popular script in India.
	Deformations in handwritten characters have category-dependent tendencies.
	The estimation and the utilization of such tendencies called eigen-deformations
	are investigated for the better performance of elastic matching based
	handwritten character recognition. The eigen-deformations are estimated
	by the principal component analysis of actual deformations automatically
	collected by the elastic matching. Typical deformations of each category
	can be extracted as the eigen-deformations. According to a similarity
	measure (e.g.: Euclidean, Mahalanobis similarity measures etc.),
	a prototype matching is done for recognition.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARMane2009.pdf:PDF},
  journal = {International Conference on Advances in Computing, Communication
	and Control},
  keywords = {OCR, HCR, Elastic Matching, Eigen Deformation, PCA},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@CONFERENCE{ARICDAR2007,
  author = {Volker Margner and Haikal El Abed},
  title = {ICDAR 2007 - Arabic Handwriting Recognition Competition},
  booktitle = {ICDAR},
  year = {2007},
  abstract = {This paper describes the Arabic handwriting recognition competition
	held at ICDAR 2007. This second competition (the first was at ICDAR
	2005) again uses the IFN/ENITdatabase with Arabic handwritten Tunisian
	town names. Today, more than 54 research groups from universities,
	research centers, and industry are working with this database worldwide.
	This year, 8 groups with 14 systems are participating in the competition.
	The systems were tested on known data and on two datasets which are
	unknown to the participants. The systems are compared on the most
	important characteristic, the recognition rate. Additionally, the
	relative speed of the different systems were compared. A short description
	of the participating groups, their systems, and the results achieved
	are finally presented.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARICDAR2007.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARMargner2006,
  author = {Volker Margner and Haikal El Abed},
  title = {Databases and Competitions: Strategies to Improve Arabic Recognition
	Systems},
  booktitle = {Proceedings of the Summit on Arabic and Chinese Handwriting Recognition},
  year = {2006},
  pages = {161-169},
  address = {University of Maryland, College Park, MD},
  abstract = {The great success and high recognition rates of both OCR systems and
	recognition systems for handwritten words are unconceivable without
	the availability of huge datasets of real world data. This chapter
	gives a short survey of datasets used for recognition with special
	focus on their application. The main part of this chapter deals with
	Arabic handwriting, datasets for recognition systems, and their availability.
	A description of different datasets and their usability is given,
	and the results of a competition are presented. Finally, a strategy
	for the development of Arabic handwriting recognition systems based
	on datasets and competitions is presented.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARMargner2006.pdf:PDF},
  journal = {SACH06},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{PDBKaczmar2005,
  author = {Urszula Markowska-Kaczmar and Pawel Kubacki},
  title = {Support Vector Machines in Handwritten Digits Classification.},
  booktitle = {Proceedings of the Fifth International Conference on Intelligent
	Systems Design and Applications (ISDA 2005) },
  year = {2005},
  pages = {406-411},
  address = {Wroclaw, Poland},
  month = {8-10 September 2005},
  publisher = {IEEE Computer Society},
  abstract = {In the paper our approach to classify handwritten digits by using
	Support Vector Machines is described. Because of the unsatisfying,
	long time of training of SVM we propose to apply k-nearest neighbours
	algorithm with Manhattan distance to obtain reduced size of training
	set having a hope that this hybrid method does not make the significantly
	worse results of recognition. The aim of presented further experiments
	was to verify this assumption.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ISDA.2005.87},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBKaczmar2005.pdf:PDF},
  isbn = {0-7695-2286-6},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@INPROCEEDINGS{PDB18Masuyama2002,
  author = {Takeshi Masuyama and Hiroshi Nakagawa},
  title = {Applying Cascaded Feature Selection to SVM Text Categorization.},
  booktitle = {13th International Workshop on Database and Expert Systems Applications
	(DEXA 2002)},
  year = {2002},
  pages = {241-245},
  address = {Aix-en-Provence, France},
  month = {2-6 September},
  publisher = {IEEE Computer Society},
  abstract = {This paper investigates the effect of a cascaded feature selection
	(CFS) in SVM text categorization. Unlike existing feature selections,
	our method (CFS) has two advantages. One can make use of the characteristic
	of each feature (word). Another is that unnecessary test documents
	for a category, which should be categorized into a negative set,
	can be removed in the first step. Compared with the method which
	does not apply CFS, our method achieved significant good performance
	especially about the categories which contain a small number of training
	documents.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  ee = {http://computer.org/proceedings/dexa/1668/16680241abs.htm},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB18Masuyama2002.pdf:PDF},
  isbn = {0-7695-1668-8},
  owner = {TOSHIBA},
  timestamp = {2009.10.05}
}

@INPROCEEDINGS{DSMello2007,
  author = {Mello, C. and Bezerra, B. and Zanchettin, C. and Macario, V.},
  title = {An Efficient Thresholding Algorithm for Brazilian Bank Checks},
  booktitle = {ICDAR '07: Proceedings of the Ninth International Conference on Document
	Analysis and Recognition},
  year = {2007},
  pages = {193--197},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  abstract = {It is present herein an algorithm for thresholding images of bank
	checks. These images have complex background elements. Some of these
	patterns make very hard to distinguish between the text and the texture
	pattern defined by the bank. For the binarizing process, an adaptive
	global thresholding algorithm is proposed based on ROC curves and
	it is compared to several well-known algorithms. The images generated
	by the new algorithm achieved a hit rate of 97% for recognition of
	the CMC7 code.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSMello2007.pdf:PDF},
  isbn = {0-7695-2822-8},
  owner = {Maha},
  timestamp = {2010.02.25}
}

@INPROCEEDINGS{ARMihov2005,
  author = {Stoyan Mihov and Klaus U. Schulz and Christoph Ringlstetter and Veselka
	Dojchinova and Vanja Nakova and Kristina Kalpakchieva and Ognjan
	Gerasimov and Annette Gotscharek and Claudia Gercke},
  title = {A Corpus for Comparative Evaluation of OCR Software and Postcorrection
	Techniques},
  booktitle = {Proceedings of the 2005 Eight International Conference on Document
	Analysis and Recognition (ICDAR’05)},
  year = {2005},
  abstract = {We describe a new corpus collected for comparative evaluation of OCR-software
	and postcorrection techniques. The corpus is freely available for
	academic groups and use. The major part of the corpus (2306 files)
	consists of Bulgarian documents. Many of these documents come with
	Cyrillic and Latin symbols. A smaller corpus with German documents
	has been added. All original documents represent real-life paper
	documents collected from enterprises and organizations. Most genres
	of written language and various document types are covered. The corpus
	contains the corresponding image files, rich meta-data, textual files
	obtained via OCR recognition, ground truth data for hundreds of example
	pages, and alignment software for experiments.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARMihov2005.pdf:PDF},
  keywords = {Optical, postcorrection of OCR results, public corpora, comparative
	evaluation, ground truth data, Cyrillic documents, mixed-alphabet
	documents, meta-data.},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{DSMarisa2002b,
  author = {Morita M, Sabourin R., Bortolozzi F. and Suen C.Y},
  title = {Segmentation and Recognition of Handwritten Dates },
  booktitle = { 8th International Workshop on Frontiers of Handwriting Recognition
	(IWFHR'8)},
  year = {2002},
  pages = { 105-110},
  address = { Niagara-on-the-Lake, CA},
  month = { August 6-8},
  abstract = {This paper presents an HMM-MLP hybrid system to recognize complex
	date images written on Brazilian bank cheques. The system first segments
	implicitly a date image into sub-fields through the recognition process
	based on an HMM-based approach. Afterwards, the three obligatory
	date sub-fields are processed by the system (day, month and year).
	A neural approach has been adopted to work with strings of digits
	and a Markovian strategy to recognize and verify words. We also introduce
	the concept of meta-classes of digits, which is used to reduce the
	lexicon size of the day and year and improve the precision of their
	segmentation and recognition. Experiments show interesting results
	on date recognition.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSMarisa2002b.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSMorita2001,
  author = {Morita, M. and El Yacoubi, A. and Sabourin, R. and Bortolozzi, F.
	and Suen, C.Y.},
  title = {Handwritten month word recognition on Brazilian bank cheques},
  booktitle = {Document Analysis and Recognition, 2001. Proceedings. Sixth International
	Conference on},
  year = {2001},
  pages = {972 -976},
  abstract = {This paper describes an off-line system under development to process
	unconstrained handwritten dates on Brazilian bank cheques in an omni-writer
	context. We show here some improvements on our previous work on isolated
	month word recognition using hidden Markov models (HMM). After preprocessing,
	a word image is explicitly segmented into characters or pseudo-characters
	and represented by two feature sequences of equal length, which are
	combined using HMM. The word models are generated from the concatenation
	of appropriate character models. In addition to the small date database,
	we also make use of the legal amount database to increase the frequency
	of characters in the training and the validation sets. Although this
	study deals with a limited lexicon, the many similarities among the
	word classes can affect the performance of the recognition. Experiments
	show an increase in the average recognition rate from 84% to 91%.
	Finally, we present our perspectives of future work},
  doi = {10.1109/ICDAR.2001.953930},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSMorita2001.pdf:PDF},
  keywords = {Brazilian bank cheques;HMM;Handwritten month word recognition;hidden
	Markov models;image is segmented;legal amount;omni-writer context;pseudo-characters;twofeature
	sequences;unconstrained handwritten dates;cheque processing;handwritten,
	;hidden Markov models;image segmentation;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{PDB13Morita2003,
  author = {Marisa E. Morita and Robert Sabourin and Fl{\'a}vio Bortolozzi and
	Ching Y. Suen},
  title = {A Recognition and Verification Strategy for Handwritten Word Recognition.},
  booktitle = {7th International Conference on Document Analysis and Recognition
	(ICDAR 2003)},
  year = {2003},
  volume = {2-Volume Set},
  pages = {482-},
  address = {Edinburgh, Scotland, UK},
  month = {3-6 August},
  publisher = {IEEE Computer Society},
  abstract = {In this paper a word recognition and verification scheme based on
	HMMs is presented. However, the main contribution of the current
	work lies in the validation of such a strategy. In order to perform
	this task, we carried out some experiments on word recognition using
	a legal amount database and then we compared the results reached
	with other study which makes use of the same database. The experiments
	demonstrate the efficiency of the strategy we developed for word
	recognition and verification.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://csdl.computer.org/comp/proceedings/icdar/2003/1960/01/196010482abs.htm},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB13Morita2003.pdf:PDF},
  isbn = {0-7695-1960-1},
  owner = {TOSHIBA},
  timestamp = {2009.10.05}
}

@INPROCEEDINGS{ARMoussa2008,
  author = {S. Ben Moussa and A. Zahour and A. Benabdelhafid and A.M. Alimi},
  title = {Fractal-Based System for Arabic/Latin, Printed/Handwritten Script
	Identification},
  booktitle = {19th International Conference on Pattern Recognition (ICPR 2008)},
  year = {2008},
  address = {Tampa, Florida, USA},
  month = {December 8-11},
  abstract = {In this paper, we present multilingual automatic identification of
	Arabic and Latin in both handwritten and printed script. The proposed
	scheme is based, Firstly, on morphological transform of line text
	images, secondly on fractal analysis features of both (i): original
	texture of 2-D images, (ii): vertical and horizontal profile projection.
	We used two techniques to obtain only 12 features based on fractal
	multidimension. The proposed system has been tested for 1000 prototypes
	with various typefaces, scriptors styles and sizes. The accuracy
	discrimination rate is about of 96.64 % by using KNN, and 98.72 %
	by using RBF. Experimental results show the importance of the proposed
	approach.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARMoussa2008.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2010.02.15}
}

@INPROCEEDINGS{PDBMozaffari2005,
  author = {Saeed Mozaffari and Karim Faez and Karim Faez and Majid Ziaratban},
  title = {Structural Decomposition and Statistical Description of Farsi/Arabic
	Handwritten Numeric Characters.},
  booktitle = {Eighth International Conference on Document Analysis and Recognition
	(ICDAR 2005)},
  year = {2005},
  pages = {237-241},
  address = {Seoul, Korea},
  month = {29 August - 1 September 2005},
  publisher = {IEEE Computer Society},
  abstract = {A Statistical method embedded with statistical features is proposed
	for Farsi/Arabic handwritten zip code recognition in this paper.
	The numeral is first smoothed and the skeleton is obtained. A set
	of feature points are then detected and the skeleton is decomposed
	into primitives. A primitive code includes the information of each
	primitive and a global code is derived from the primitive codes to
	describe the topological structure of the skeleton. By using the
	average and variance of X and Y changes in each primitive, the Direction
	and curvature of the skeleton can be statistically described. 
	
	Since the global codes have different lengths, we applied PCA algorithm
	to normalize their lengths. Thanks to statistically description of
	the skeleton, we can use the nearest neighbor classifier for recognition.
	
	
	According to experimental results, classification rate of 94.44% is
	obtained for numerals on the test sets gathered from various people
	with different educational background and different ages. Our database
	includes 480 samples per digit. We used 280 samples of each digit
	for training and the rest (200) for test.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ICDAR.2005.221},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBMozaffari2005.pdf:PDF},
  isbn = {0-7695-2420-6},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@INPROCEEDINGS{ARMozaffari2006,
  author = {Mozaffari, Saeed and Faez, Karim and Faradji, Farhad and Ziaratban,
	Majid and Golzan, S. Mohamad},
  title = { A Comprehensive Isolated Farsi/Arabic Character Database for Handwritten
	{OCR} Research},
  booktitle = {Tenth International Workshop on Frontiers in Handwriting Recognition
	},
  year = {2006},
  editor = {Guy Lorette },
  address = {La Baule (France) },
  month = {10},
  organization = { Universit{\'e} de Rennes 1 },
  publisher = {Suvisoft },
  note = {http://www.suvisoft.com I.: Computing Methodologies/I.5: {PATTERN}
	{RECOGNITION}, I.: Computing Methodologies/I.7: {DOCUMENT} {AND}
	{TEXT} {PROCESSING} Universit{\'e} de Rennes 1},
  abstract = {This paper presents a new comprehensive database for isolated offline
	handwritten Farsi/Arabic numbers and characters for use in optical
	character recognition research. The database is freely available
	for academic use. So far no such a freely database in Farsi language
	is available. Grayscale images of 52,380 characters and 17,740 numerals
	are included. Each image was scanned from Iranian school entrance
	exam forms during the years 2004-2006 at 300 dpi. The only restriction
	imposed on the writers is to write each character within a rectangular
	box. The number of samples in each class of the database is non-uniform
	corresponding to their real life distributions. Also, for comparison
	purposes, each dataset has been properly divided into respective
	training and test sets.},
  affiliation = {Pattern Recognition and Image Processing Laboratory - Amirkabir University
	of Technology, Tehran },
  audience = {not specified },
  day = {23},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARMozaffari2006.pdf:PDF},
  keywords = {{OCR}, Farsi/Arabic, Comparative database, offline, isolated numbers
	and characters},
  language = {English},
  url = {http://hal.inria.fr/inria-00112676/en/}
}

@INPROCEEDINGS{PDBMozaffari2004,
  author = {Saeed Mozaffari and Karim Faez and Hamidreza Rashidy Kanan},
  title = {Feature Comparison between Fractal Codes and Wavelet Transform in
	Handwritten Alphanumeric Recognition Using SVM Classifier.},
  booktitle = {ICPR (2)},
  year = {2004},
  pages = {331-334},
  abstract = {In this paper we proposed a new method for isolated handwritten Farsi/Arabic
	characters and numerals recognition using fractal codes and Haar
	wavelet transform. Fractal codes represent affine transformations
	which when iteratively applied to the range-domain pairs in an arbitrary
	initial image, the result is close to the given image. Each fractal
	code consists of six parameters such as corresponding domain coordinates
	for each range block, brightness offset and an affine transformation.
	in this system, The support vector machine (SVM) whih is based on
	statistical learning theory, with good generalization ability is
	used as the classifier. This method is robust to scale and frame
	size changes. 32 Farsi’s characters are categorized to 8 different
	classes in which the characters are very similar to each others.
	There are ten digits in Farsi/Arabic language and since two of them
	are not used in the postal codes in Iran, therefore 8 more classes
	are needed for digits. According to experimental results, classification
	rates of 92.71% and 92% were obtained for digits and characters respectively
	on the test sets gathered from various people with different educational
	background and different ages.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://csdl.computer.org/comp/proceedings/icpr/2004/2128/02/212820331abs.htm},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBMozaffari2004.pdf:PDF},
  keywords = {Handwritten digits},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@INPROCEEDINGS{ARMozaffari2007,
  author = {Saeed Mozaffari and Karim Faez and Volker Margner},
  title = {Application of Fractal Theory for On-Line and Off-Line Farsi Digit
	Recognition},
  booktitle = {MLDM '07: Proceedings of the 5th international conference on Machine
	Learning and Data Mining in Pattern Recognition},
  year = {2007},
  pages = {868--882},
  address = {Leipzig, Germany},
  publisher = {Springer-Verlag},
  abstract = {Fractal theory has been used for computer graphics, image compression
	and different fields of pattern recognition. In this paper, a fractal
	based method for recognition of both on-line and off-line Farsi/
	Arabic handwritten digits is proposed. Our main goal is to verify
	whether fractal theory is able to capture discriminatory information
	from digits for pattern recognition task. Digit classification problem
	(on-line and offline) deals with patterns which do not have complex
	structure. So, a general purpose fractal coder, introduced for image
	compression, is simplified to be utilized for this application. In
	order to do that, during the coding process, contrast and luminosity
	information of each point in the input pattern are ignored. Therefore,
	this approach can deal with on-line data and binary images of handwritten
	Farsi digits. In fact, our system represents the shape of the input
	pattern by searching for a set of geometrical relationship between
	parts of it. Some fractal-based features are directly extracted by
	the fractal coder. We show that the resulting features have invariant
	properties which can be used for object recognition},
  doi = {http://dx.doi.org/10.1007/978-3-540-73499-4_65},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARMozaffari2007.pdf:PDF},
  isbn = {978-3-540-73498-7},
  keywords = {Fractal theory, Iterated function system, on-line Arabic handwritten
	digit recognition, off-line Arabic handwritten digit recognition,
	Arabic handwritten digit recognition},
  location = {Berlin, Heidelberg},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARCompt2009,
  author = {Saeed Mozaffari and Hadi Soltanizadeh},
  title = {ICDAR 2009 Handwritten Farsi/Arabic Character Recognition Competition},
  booktitle = {2009 10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {In recent years, the recognition of Farsi and Arabic handwriting is
	drawing increasing attention. This paper describes the result of
	the ICDAR 2009 competition for handwritten Farsi/Arabic character
	recognition. To evaluate the submitted systems, we used large datasets
	containing both binary and grayscale images. Many different groups
	downloaded the training sets; however, finally 4 systems successfully
	participated in the competition. The systems were tested on two known
	databases and one unknown dataset. Due to the similarity between
	some digits and characters in Farsi and Arabic, each recognizer was
	tested for digit and character sets separately. For benchmarking,
	only the recognition rates, as the most important characteristic,
	are considered. Since participants used different software and even
	operating systems, the relative recognition speed is not compared
	in this competition.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARCompt2009.pdf:PDF},
  journal = {2009 10th International Conference on Document Analysis and Recognition},
  keywords = {OCR benchmarking, Performance evaluation, Farsi/Arabic languages,
	large database, isolated digits and characters.},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{DSMuntean2007,
  author = {Muntean, O. and Oltean, M.},
  title = {Processing Bank Checks with Genetic Programming and Histograms},
  booktitle = {Bio-inspired, Learning, and Intelligent Systems for Security, 2007.
	BLISS 2007. ECSIS Symposium on},
  year = {2007},
  pages = {102 -105},
  month = {aug.},
  abstract = {In spite of evolution of electronic techniques, a large number of
	applications continue to rely on the use of paper as the dominant
	medium. Bank checks are a widely known example. When filled by hand,
	the processing of the written information requires either a human
	or a special software which has intelligent abilities. This paper
	examines the issue of reading the amount of money written on the
	checks. Genetic Programming (GP) technique is used for dealing with
	this problem. A new type of input representation is proposed: histograms.
	Several numerical experiments with GP are performed by using large
	datasets taken from the MNIST benchmarking set. Preliminary results
	show a good behavior of the method.},
  doi = {10.1109/BLISS.2007.27},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSMuntean2007.pdf:PDF},
  keywords = {MNIST benchmarking set;electronic techniques;genetic programming;histograms;processing
	bank checks;cheque processing;genetic algorithms;},
  owner = {Maha},
  review = {It classify digits of MNST using Genetic programming. The features
	used is vertical and horizonatal histogram of digits. The system
	is build as a set of binary classifiers. the fintness funcion is
	th enuber of errors in classification. The result is not compared
	to other system and only error rate of each digit alone is present.
	The error rate range from 2% to 7%.},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{ARICDAR2005,
  author = {V. Märgner and M. Pechwitz and H. El Abed},
  title = {ICDAR 2005 Arabic Handwriting Recognition Competition},
  booktitle = {Proceedings of the 2005 Eight International Conference on Document
	Analysis and Recognition (ICDAR’05)},
  year = {2005},
  abstract = {This paper describes the Arabic handwriting recognition competition
	for ICDAR 2005. With the presentation of the IFN/ENIT-database in
	the year 2002 a database with handwritten Arabic town names was made
	available for free to non commercial research groups. Till now more
	than 30 groups are working with this data worldwide. By announcing
	a competition of Arabic handwriting recognition systems based on
	the IFN/ENIT-database, we hope to contribute to the development of
	Arabic handwriting recognition systems. The use of the same database
	by different research groups allows the comparison of different systems.
	We compare the systems on the most important characteristic: recognition
	rate, but also features like word length, writing style, and character
	connectivity will be discussed.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARICDAR2005.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@CONFERENCE{ARNatarajan2009,
  author = {Prem Natarajan and Krishna Subramanian and Anurag Bhardwaj and Rohit
	Prasad},
  title = {Stochastic Segment Modeling for Offline Handwriting Recognition},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {In this paper, we present a novel approach for incorporating structural
	information into the hidden Markov Modeling (HMM) framework for offline
	handwriting recognition. Traditionally, structural features have
	been used in recognition approaches that rely on accurate segmentation
	of words into smaller units (sub-words or characters). However, such
	segmentation based approaches do not perform well on real-world handwritten
	images, because breaks and merges in glyphs typically create new
	connected components that are not observed in the training data.
	To mitigate the problem of having to derive accurate segmentation
	from connected components, we present a novel framework where the
	HMM based recognition system trained on shorter-span features is
	used to generate the 2-D character images (the “Stochastic Segments”),
	and then another classifier that uses structural features extracted
	from the stochastic character segments generates a new set of scores.
	Finally, the scores from the HMM system and from structural matching
	are used in combination to generate a hypothesis that is better than
	the results from either the HMM or from structural matching alone.
	We demonstrate the efficacy of our approach by reporting experimental
	results on a large corpus of handwritten Arabic documents.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARNatarajan2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@INPROCEEDINGS{DSNeves2008,
  author = {Neves, R.F.P. and Mello, C.A.B. and Silva, M.S. and Bezerra, B.L.D.},
  title = {A new technique to threshold the courtesy amount of Brazilian bank
	checks},
  booktitle = {15th International Conference on Systems, Signals and Image Processing,
	IWSSIP 2008. },
  year = {2008},
  pages = {93 -96},
  month = {june},
  abstract = {Automatic bank check processing is a very difficult task. The images
	have complex background which is a problem for an automatic recognition
	system. Our purpose is focused in the thresholding phase of the courtesy
	amount. The new approach uses histogram specification and Tsallis
	entropy to find the best threshold value. The results show that the
	method proposed achieves the best results if compared to other classical
	algorithms.},
  doi = {10.1109/IWSSIP.2008.4604375},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSNeves2008.pdf:PDF},
  keywords = {Brazilian bank checks;Tsallis entropy;automatic bank check processing;automatic
	recognition system;histogram specification;bank data processing;cheque
	processing;security of data;},
  owner = {Maha},
  review = {paper presents a threshold algorithm that used to binarize a courtesy
	amount from the background in bank checks. It uses a a modification
	of tali entropy to specify the threshold but modify by dividing the
	image into groups obased on the amount of color in each group. The
	system then identify gourps with histogram distortion and tries to
	modifity his histogram to remove the problem. The method is compared
	to 6 different system using 4 different measures (percision, recall,
	specifity .. ). The system achieved better result from comparing
	a ground truth images with the test images.},
  timestamp = {2010.2.24}
}

@ARTICLE{DSGorski2001,
  author = {Nikolai Gorski, Valery Anisimov, Emmanuel Augustin, Olivier Baret,
	Sergey Maximov},
  title = {Industrial bank check processing: the A2iA CheckReaderTM},
  journal = {International Journal of Document Analysis and Processing},
  year = {2001},
  volume = {3},
  pages = {196–206},
  abstract = {This paper presents the current state of the A2iA CheckReaderTM –
	a commercial bank check recognition system. The system is designed
	to process the flow of payment documents associated with the check
	clearing process: checks themselves, deposit slips, money orders,
	cash tickets, etc. It processes document images and recognizes document
	amounts whatever their style and type – cursive, hand- or machine
	printed – expressed as numerals or as phrases. The system is adapted
	to read payment documents issued in different English- or Frenchspeaking
	countries. It is currently in use at more than 100 large sites in
	five countries and processes daily over 10 million documents. The
	average read rate at the document level varies from 65 to 85% with
	a misread rate corresponding to that of a human operator (1%).},
  booktitle = { },
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSGorski2001.pdf:PDF},
  keywords = {Bank check processing , Payment systems ,Handwriting recognition ,
	Automatic reading , Document analysis , Intelligent},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{PDBNunes2004,
  author = {Carlos M. Nunes and Alceu de Souza Britto Jr. and Celso A. A. Kaestner
	and Robert Sabourin},
  title = {Feature Subset Selection Using an Optimized Hill Climbing Algorithm
	for Handwritten Character Recognition.},
  booktitle = {Structural, Syntactic, and Statistical Pattern Recognition, Joint
	IAPR International Workshops, SSPR 2004 and SPR 2004, 2004 Proceedings},
  year = {2004},
  editor = {Ana L. N. Fred and Terry Caelli and Robert P. W. Duin and Aur{\'e}lio
	C. Campilho and Dick de Ridder},
  pages = {1018-1025},
  address = {Lisbon, Portugal
	
	},
  month = {August 18-20},
  publisher = {Springer},
  abstract = {This paper presents an optimized Hill-Climbing algorithm to select
	subset of features for handwritten character recognition. The search
	is conducted taking into account a random mutation strategy and the
	initial relevance of each feature in the recognition process. A first
	set of experiments have shown a reduction in the original number
	of features used in an MLP-based character recognizer from 132 to
	77 features (reduction of 42%) without a significant loss in terms
	of recognition rates, which are 99.1% for 30,089 digits and 93.0%
	for 11,941 uppercase characters, both handwritten samples from the
	NIST SD19 database. Additional experiments have been done by considering
	some loss in terms of recognition rate during the feature subset
	selection. A byproduct of these experiments is a cascade classifier
	based on feature subsets of different sizes, which is used to reduce
	the complexity of the classification task by 86.54% on the digit
	recognition experiment. The proposed feature selection method has
	shown to be an interesting strategy to implement a wrapper approach
	without the need of complex and expensive hardware architectures.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://springerlink.metapress.com/openurl.asp?genre$=$article\&issn$=$0302-9743\&volume$=$3138\&spage$=$1018},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBNunes2004.pdf:PDF},
  isbn = {3-540-22570-6},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@CONFERENCE{MCOliveira2007,
  author = {Luciano Oliveira and Paulo Peixoto and Urbano Nunes},
  title = {A Hierarchical Fuzzy Integration of Local and Global Feature-based
	Classifiers to Recognize Objects in Autonomous Vehicles},
  booktitle = {ICRA 2007 Workshop: Planning, Perception and Navigation for Intelligent
	Vehicles},
  year = {2007},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCOliveira2007.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{FE7Oliveira2002,
  author = {Luiz E. Soares Oliveira and Robert Sabourin and Fl{\'a}vio Bortolozzi
	and Ching Y. Suen},
  title = {Automatic Recognition of Handwritten Numerical Strings: A Recognition
	and Verification Strategy.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2002},
  volume = {24},
  pages = {1438-1454},
  number = {11},
  abstract = {A modular system to recognize handwritten numerical strings is proposed.
	It uses a segmentation-based recognition approach and a Recognition
	and Verification strategy. The approach combines the outputs from
	different levels such as segmentation, recognition, and postprocessing
	in a probabilistic model. A new verification scheme which contains
	two verifiers to deal with the problems of oversegmentation and undersegmentation
	is presented. A new feature set is also introduced to feed the oversegmentation
	verifier. A postprocessor based on a deterministic automaton is used
	and the global decision module makes an accept/reject decision. Finally,
	experimental results on two databases are presented: numerical amounts
	on Brazilian bank checks and NIST SD19. The latter aims at validating
	the concept of modular system and showing the robustness of the system
	using a well-known database},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  booktitle = {IEEE Trans. Pattern Anal. Mach. Intell.},
  doi = {http://computer.org/tpami/tp2002/i1438abs.htm},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\FE7Oliveira2002.pdf:PDF},
  keywords = {Handwritten digits, Features Extraction},
  owner = {TOSHIBA},
  review = {Read again 
	
	
	good features but focuses on the numerical string how to handle segmentation
	errors. 
	
	
	different classifiers and different set of features 
	
	hypothesis generation and selection s
	
	
	tries to hand over segmentation and under segmentation 
	
	two type of databases ( bank drafts and post office databases )
	
	Comments:
	
	-------------------------
	
	based on numerical string 
	
	neural network classifiers 
	
	set of classifiers each has its features and a propability model 
	
	 another classifiers to verify the segmentation and outputs 
	
	features are : 
	
	 concavity 
	
	contour 
	
	structural 
	
	skeleton 
	
	directional 
	
	count of white and black regions (for the verifiers )
	
	
	generate result for all string with % and choose best fit. 
	
	 result are 98% to 99%},
  timestamp = {2009.10.03}
}

@INPROCEEDINGS{DSOliveira2002,
  author = {L. S. Oliveira and R. Sabourin and F. Bortolozzi and C. Y. Suen},
  title = {Feature selection using multi-objective genetic algorithms for handwritten
	digit recognition},
  booktitle = {Proc. Int. Conf. on Pattern Recognition},
  year = {2002},
  volume = {1},
  pages = {568-571},
  address = {Quebec City},
  month = {Aug},
  abstract = {This paper discusses the use of genetic algorithm for feature selection
	for handwriting recognition. Its novelty lies in the use of a multi-objective
	genetic algorithms where sensitivity analysis and neural network
	are employed to allow the use of a representative database to evaluate
	fitness and the use of a validation database to identify the subsets
	of selected features that provide a good generalization. Comprehensive
	experiments on the NIST database confirm the effectiveness of the
	proposed strategy.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSOliveira2002.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.02.25}
}

@ARTICLE{MCOuyang2009,
  author = {Jie Ouyang and Nilesh Patel and Ishwar Sethi},
  title = { Induction of multiclass multifeature split decision trees from distributed
	data},
  journal = {Pattern Recognition},
  year = {2009},
  volume = {42},
  pages = {1786-1794},
  abstract = {The decisiontree-basedclassificationisapopularapproachforpatternrecognit
	onanddatamining. Most decisiontreeinductionmethodsassumetrainingdatabeingpresentatonecentr
	llocation.Given the growthindistributeddatabasesatgeographicallydispersedlocations,theme
	hodsfordecisiontree induction indistributedsettingsaregainingimportance.Thispaperdescribesonesuchm
	thodthat generates compacttreesusingmultifeaturesplitsinplaceofsinglefeaturesplitdecisi
	ntreesgenerated by mostexistingmethodsfordistributeddata.OurmethodisbasedonFisher'sline
	rdiscriminant function, andiscapableofdealingwithmultipleclassesinthedata.Forhomogeneously
	istributed data, thedecisiontreesproducedbyourmethodareidenticaltodecisiontreesgene
	atedusingFisher's linear discriminantfunctionwithcentrallystoreddata.Forheterogeneouslydist
	ibuteddata,acertain approximation isinvolvedwithasmallchangeinperformancewithrespecttothetreegenerat
	dwith centrally storeddata.Experimentalresultsforseveralwell-knowndatasetsareprese
	tedandcompared with decisiontreesgeneratedusingFisher'slineardiscriminantfunctionwithc
	ntrallystoreddata},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCOuyang2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{ARPal2008,
  author = {Umapada Pal and Kaushik Roy and Fumitaka Kimura},
  title = {Bangla Handwritten Pin Code String Recognition for Indian Postal},
  year = {2008},
  abstract = {Indian pin code is a six-digit string. Because of the writing style
	of different individuals some of the digits in a pin code string
	may touch with its neighboring digits. Accurate segmentation of such
	touching components into individual digits is a difficult task. To
	avoid such segmentation, here we consider a pin code string as word
	and the pin code recognition problem is treated as lexicon free word
	recognition. In the proposed method, at first, binarization of the
	input document is done. Next, water reservoir concept is applied
	to pre-segment a pin code string into possible primitive components
	(individual digits or its parts). Presegmented components of the
	pin code are then merged into possible digits to get the best pin
	code. In order to merge these primitive components into digits and
	to find optimum segmentation, dynamic programming (DP) is applied
	using total likelihood of digits as the objective function. To compute
	the likelihood of a digit, modified quadratic discriminant function
	(MQDF) is used. The features used in the MQDF are based on the directional
	information of the components. Our system on handwritten Bangla pin
	code shows 99.08% reliability when rejection and error rates are
	19.28% and 0.74%, respectively.},
  booktitle = { },
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARPal2008.pdf:PDF},
  keywords = {Handwritten digit recognition, Pin code recognition, Bangla script,
	Indian postal automation.},
  owner = {Maha},
  review = {The system discribe how to segment pin code numeric strings. An attempt
	to pre segment the pin code into individual digits but when there
	is a toching digits it was noticed that a big cavity region is generated.
	The system uses this cavity to detection toching regions and try
	to segment it with vertical colums using some visual rules. The next
	step is merging the improving the presegmentation to better optimiim
	segmentation. Two sets of feature set is computer 64 dimentinasl
	featurees is used in a dynamic programing module to merge and recognize
	the digits based on the best like hood of digit. After best segmentation
	achieved a set of 400 directional features is computed for correct
	recognition result. The final result achived is 99.01% of the string.
	The result was obtained on different languages (3 hindi , english
	and ...) the paper present other result but does not compare with
	them.},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSPal2009,
  author = {Umapada Pal and R. K. Roy and K. Roy and F. Kimura},
  title = {Indian Multi-Script Full Pin-code String Recognition for Postal Automation},
  booktitle = {2009 10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {Under three-language formula, the destination address block of postal
	document of an Indian state is generally written in three languages:
	English, Hindi and the State official language. Because of inter-mixing
	of these scripts in postal address writings, it is very difficult
	to identify the script by which a pin-code is written. Also, because
	of the writing style of different individuals some of the digits
	in a pin-code string may touch with its neighboring digits. Accurate
	segmentation of such touching components into individual digits is
	a difficult task. To avoid such difficulties, in this paper we proposed
	a tri-lingual (English, Hindi and Bangla) 6-digit full pin-code string
	recognition. We obtained 99.01% reliability from our proposed system
	when error and rejection rates are 0.83% and 15.27%, respectively},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSPal2009.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSPalacios2003,
  author = {Palacios, R. and Gupta, A.},
  title = {Training neural networks for reading handwritten amounts on checks},
  booktitle = {Neural Networks for Signal Processing, 2003. NNSP'03. 2003 IEEE 13th
	Workshop on},
  year = {2003},
  pages = { 607 - 616},
  month = {sept.},
  abstract = { While reading handwritten text accurately is a difficult task for
	computers, the conversion of handwritten papers into digital format
	is necessary for automatic processing. Since most bank checks are
	handwritten, the number of checks is very high, and manual processing
	involves significant expenses, many banks are interested in systems
	that can read check automatically. This work presents several approaches
	to improve the accuracy of neural networks used to read unconstrained
	numerals in the courtesy amount field of bank checks.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSPalacios2003.pdf:PDF},
  issn = {1089-3555 },
  keywords = {check processing; document imaging; neural networks; optical, ; unconstrained
	handwritten numerals; cheque processing; document image processing;
	handwritten, ; learning (artificial intelligence); multilayer perceptrons;
	optical, ;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@ARTICLE{DSPalacios2008,
  author = {Rafael Palaciosa and Amar Guptab},
  title = {A system for processing handwritten bank checks automatically},
  journal = {Image and Vision Computing},
  year = {2008},
  volume = {26},
  pages = {1297-1313},
  number = {10},
  month = {October 2008},
  abstract = {In the US and many other countries, bank previous termchecksnext term
	are preprinted with the account number and the previous termchecknext
	term number in special ink and format; as such, these two numeric
	fields can be easily read and processed using automated techniques.
	However, the amount fields on a filled-in previous termchecknext
	term is usually read by human eyes, and involves significant time
	and cost, especially when one considers that over 50 billion previous
	termchecksnext term are processed per annum in the US alone. The
	system described in this paper uses the scanned image of a bank previous
	termchecknext term to ‘read’ the previous termcheck.next term It
	includes three main modules that allow for fully automated bank previous
	termchecknext term processing.
	
	These three modules are described in the paper; they focus sequentially
	on: the detection of strings within the image; the segmentation and
	previous termrecognitionnext term of string in a feedback loop; and
	the post-processing issues that help to ensure higher accuracy of
	previous termrecognition.next term The major benefit of the integrated
	system is the ability to address the complex problem of reading handwritten
	bank previous termchecksnext term by implementing efficient algorithms
	for each processing step. All modules have been implemented and subsequently
	tested for reading the value of the previous termchecknext term using
	different image databases. Due to the particular requirements of
	this application, the system can be tuned to yield low levels of
	incorrect readings; this, in turn, leads to higher levels of rejection
	than the levels encountered in other handwritten previous termrecognitionnext
	term applications. A ‘rejected’ previous termchecknext term can be
	read subsequently by human eyes or other more advanced automated
	approaches. However, a previous termchecknext term ‘read’ incorrectly
	is more difficult to deal with, in terms of costs and time involved
	to rectify the mistake. As such, our architecture can be geared towards
	producing the most suitable balance between inaccurate readings and
	rejection level, in accordance with user preferences. The experimental
	results presented in the paper do not focus on the best possible
	results for a particular database of previous termchecks;next term
	instead, they show the benefits attained independently by each of
	the modules proposed.},
  booktitle = { },
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSPalacios2008.pdf:PDF},
  keywords = {Handwritten previous termchecksnext term; Reading unconstrained handwritten
	material; Neural network based reading; Automation of banking systems},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@CONFERENCE{PBPan2009,
  author = {W.M. Pan and T.D. Bui and C.Y. Suen},
  title = {Isolated Handwritten Farsi numerals Recognition Using Sparse And
	Over-Complete Representations},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {A new isolated handwritten Farsi numeral recognition algorithm is
	proposed in this paper, which exploits the sparse and over-complete
	structure from the handwritten Farsi numeral data. In this research,
	the sparse structure is represented as an over-complete dictionary,
	which is learned by the K-SVD algorithm. These atoms in this dictionary
	are adopted to initialize the first layer of the Convolutional Neural
	Network (CNN), the latter is then trained to do the classification
	task. Data distortion techniques are also applied to promote the
	generalization capability of the trained classifier. Experiments
	have shown that good results have been achieved in CENPARMI handwritten
	Farsi numeral database.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PBPan2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@ARTICLE{ARCavalin2009,
  author = {PauloR.Cavalina and RobertSabourina and ChingY.Suenb and AlceuS.BrittoJr},
  title = {Evaluation of incremental learning algorithms for HMM in the recognition
	of alphanumeric characters},
  journal = {Pattern Recognition},
  year = {2009},
  volume = {42},
  pages = {3241-3253},
  number = {12},
  month = {December 2009},
  abstract = {We present an evaluation of incremental learning algorithms for the
	estimation of hidden Markov model (HMM) parameters. The main goal
	is to investigate incremental learning algorithms that can provide
	as good performances as traditional batch learning techniques, but
	incorporating the advantages of incremental learning for designing
	complex pattern recognition systems. Experiments on handwritten characters
	have shown that a proposed variant of the ensemble training algorithm,
	employing ensembles of HMMs, can lead to very promising performances.
	Furthermore, the use of a validation dataset demonstrated that it
	is possible to reach better performances than the ones presented
	by batch learning.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARCavalin2009.pdf:PDF},
  keywords = {Incremental learning; Hidden Markov models; Ensembles of classifiers;
	Handwriting recognition; Isolated digits; Uppercase letters},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARPechwitz2002,
  author = {M. Pechwitz and S. Snoussi Maddouri and V. Märgner and N. Ellouze
	and H. Amiri},
  title = {IFN/ENIT-DATABASE OF HANDWRITTEN ARABIC WORDS},
  booktitle = {7th Colloque International Francophone sur l'Ecrit et le Document,
	CIFED 2002,},
  year = {2002},
  address = {Hammamet, Tunis},
  month = {Oct. 21-23},
  abstract = {In this paper we are presenting a new database with handwritten Arabic
	town/village names. For each name the ground truth information, e.g.
	the sequence of character shapes, some style information, and the
	baseline are coded. 411 writers filled forms with about 26400 names
	containing more than 210000 characters. The database is described
	in detail. It is designed for training and testing recognition systems
	for handwritten Arabic words. The IFN/ENIT-database is available
	for the purpose of research.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARPechwitz2002.pdf:PDF},
  journal = {i7th Colloque International Francophone sur l'Ecrit et le Document
	, CIFED 2002, Oct. 21-23, 2002, Hammamet, Tunis, (2002)},
  keywords = {Arabic, Handwriting, Database, Recognition system, Arabic OCR},
  owner = {TOSHIBA},
  timestamp = {2010.02.15}
}

@ARTICLE{ARPlamondon2006,
  author = {Re´jean Plamondon and Moussa Djioua},
  title = {A multi-level representation paradigm for handwriting stroke generation},
  journal = {Human Movement Science},
  year = {2006},
  volume = {25},
  pages = {586–607},
  abstract = {The study of rapid strokes is a direct or indirect prerequisite in
	many fundamental research projects, as well as in the design of many
	practical applications dealing with handwriting. This paper outlines
	a family of models, derived from the Kinematic Theory of Human Movements.
	It explains how the nested models in this family can be used coherently,
	in the context of a multi-level representation paradigm, to analyze
	both the trajectory and the velocity of strokes with a progressive
	amount of detail. In the context of a comprehensive survey of previously
	published work, this paper highlights many new features of stroke
	production, when the vectorial version of the theory is fully exploited.
	In this perspective, the Kinematic Theory is depicted as a potential
	tool to facilitate communications among researchers working in the
	multi-disciplinary field of Graphonomics.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARPlamondon2006.pdf:PDF},
  keywords = {Kinematic Theory; Handwriting strokes; Delta–lognormal; Sigma–lognormal;
	Neuromuscular systems; Rapid-aimed movements},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{PDBPong2006,
  author = {Hon-Keat Pong and Tat-Jen Cham},
  title = {Optimal Cascade Construction for Detection using 3D Models},
  booktitle = {Proceedings of the 18th International Conference on Pattern Recognition
	(ICPR'06)},
  year = {2006},
  publisher = {IEEE Computer Society},
  abstract = {We describe a method for optimal construction of a detection cascade
	comprising 3D models of increasing levelof- detail (LOD). An LOD
	3D model hierarchy of the target object is first generated. By analyzing
	detection performance of each individual model in the LOD hierarchy,
	an optimization framework that allows trade-off between speed and
	accuracy is formulated. The formulation allows models to be explicitly
	selected for inclusion in the final detection cascade while achieving
	optimal running time with respect to a target detection performance.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBPong2006.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@INPROCEEDINGS{PDB12Prevost2003,
  author = {Lionel Prevost and Christian Michel-Sendis and Alvaro Moises and
	Lo\"{\i}c Oudot and Maurice Milgram},
  title = {Combining model-based and discriminative classifiers : application
	to handwritten character recognition.},
  booktitle = {7th International Conference on Document Analysis and Recognition
	(ICDAR 2003)},
  year = {2003},
  volume = {2},
  pages = {31-},
  address = {Edinburgh, Scotland, UK},
  month = {3-6 August},
  publisher = {IEEE Computer Society},
  abstract = {Handwriting recognition is such a complex classification problem that
	it is quite usual now to make co-operate several classification methods
	at the preprocessing stage or at the classification stage. In this
	paper, we present an original two stages recognizer. The first stage
	is a model-based classifier that stores an exhaustive set of character
	models. The second stage is a discriminative classifier that separates
	the most ambiguous pairs of classes. This hybrid architecture is
	based on the idea that the correct class almost systematically belongs
	to the two more relevant classes found by the first classifier. Experiments
	on Unipen database show a 30% improvement on a 62 classes recognition
	problem.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://csdl.computer.org/comp/proceedings/icdar/2003/1960/01/196010031abs.htm},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB12Prevost2003.pdf:PDF},
  isbn = {0-7695-1960-1},
  keywords = {Handwritten digits},
  owner = {TOSHIBA},
  timestamp = {2009.10.04}
}

@MISC{PDB16Pudil1992,
  author = {P. Pudil and J. Novoicova and S. Blaha and J. Kittler},
  title = {Multistage Pattern Recognition with Rejct Option},
  month = {IEEE},
  year = {1992},
  abstract = {The idea on constructing a multi stage pattern clasification system
	with reject option is presented and conditions in terms of upper
	bounds of the cost of higher stage measurements for a multi stage
	classifier to give lower decision risk than a single classifier are
	derived.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB16Pudil1992.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.05}
}

@ARTICLE{ARRhee2009,
  author = {Taik Heon Rhee and Jin Hyung Kim},
  title = {Efficient search strategy instructural analysis for handwritten mathematical
	expression recognition},
  journal = {Pattern Recognition},
  year = {2009},
  volume = {42},
  pages = {3192-3201},
  number = {12},
  month = {December 2009},
  abstract = {Problems with local ambiguities in handwritten mathematical expressions
	(MEs) are often resolved at global level. Therefore, keeping local
	ambiguities is desirable for high accuracy, with a hope that they
	may be resolved by later global analyses. We propose a layered search
	framework for handwritten ME recognition. From given handwritten
	input strokes, ME structures are expanded by adding symbol hypotheses
	one by one, representing ambiguities of symbol identities and spatial
	relationships as numbers of branches in the expansion. We also propose
	a novel heuristic predicting how likely the set of remaining input
	strokes forms valid spatial relationships with the current partially
	interpreted structure. Further complexity reduction is achieved by
	delaying the symbol identity decision. The elegance of our approach
	is that the search result would be unchanged even if we prune out
	unpromising branches of the search. Therefore, we can examine a much
	larger number of local hypotheses with a limited amount of computing
	resource in making global level decisions. The experimental evaluation
	shows promising results of the efficiency of the proposed approach
	and the performance of our system, which results from the system's
	capacity to examine a large number of possibilities},
  doi = {10.1016/j.patcog.2008.10.036},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARRhee2009.pdf:PDF},
  keywords = {Handwritten mathematical, expression recognition, Structural analysis,
	Layered searchtree, Admissible heuristic,Delayed decisionofsymbolidentity},
  owner = {Maha},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{MCRodriguez2002,
  author = {Rodriguez, C. and Boto, F. and Soraluze, I. and P\'{e}rez, A.},
  title = {An Incremental and Hierarchical K-NN classifier for Hadwritten characters},
  booktitle = {ICPR '02: Proceedings of the 16 th International Conference on Pattern
	Recognition (ICPR'02) Volume 3},
  year = {2002},
  volume = {3},
  pages = {30098},
  abstract = {This paper analyses the application of hierarchical classifiers based
	on the k-NN rule to the automatic classification of handwritten characters.
	The discriminating capacity of a k-NN classifier increases as the
	size of the reference pattern set (RPS) increases. This supposes
	aproblem for k-NN classifiers in real applications: the high computational
	cost required when the RPS is large. In order to accelerate the process
	of calculating the distance to each pattern of the RPS, some authors
	propose the use of condensing techniques. These methods try to reduce
	the size of the RPS without losing classification power. Our alternative
	proposal is based on incremental learning and hierarchical classifiers
	with rejection techniques that reduce the computational cost of the
	classifier. We have used 133,944 characters (72,105 upper-case characters
	and 61,839 lower-case characters) of the NIST Special Data Bases
	3 and 7 as experimental data set. The binary image of the character
	is transformed to gray image. The best non-hierarchical classifier
	achieves a hit rate of 94.92% (upper-case) and 87,884% (lower-case).
	The hierarchical classifier achieves the same hit ratio, but with
	3 times lower computational cost than the cost of the best non-hierarchical
	classifier found in our experimentation and 14% less than Hart's
	Algorithm.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCRodriguez2002.pdf:PDF},
  isbn = {0-7695-1695-X},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{ARBertolami2008,
  author = {Roman Bertolami , Horst Bunke},
  title = {Hidden Markov model-based ensemble methods for offline handwritten
	text line
	
	recognition},
  journal = {Pattern Recognition},
  year = {2008},
  volume = {41},
  pages = {3452 -- 3460},
  abstract = {This paper investigates various ensemble methods for offline handwritten
	text line recognition. To obtain ensembles of recognisers, we implement
	bagging, random feature subspace, and language model variation methods.
	For the combination, the word sequences returned by the individual
	ensemble members are first aligned. Then a confidence-based voting
	strategy determines the final word sequence. A number of confidence
	measures based on normalised likelihoods and alternative candidates
	are evaluated. Experiments show that the proposed ensemble methods
	can improve the recognition accuracy over an optimised single reference
	recogniser},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBertolami2008.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INCOLLECTION{DSRomero2007,
  author = {Verónica Romero and Adrià Giménez and Alfons Juan},
  title = {Explicit Modelling of Invariances in Bernoulli Mixtures for Binary
	Images},
  booktitle = {Pattern Recognition and Image Analysis},
  publisher = {Springer Berlin / Heidelberg},
  year = {2007},
  volume = {4477},
  series = {Lecture Notes in Computer Science},
  pages = {539-546},
  abstract = {Bernoulli mixture models have been recently proposed as simple yet
	powerful probabilistic models for binary images in which each image
	pattern is modelled by a different Bernoulli prototype (component).
	A possible limitation of these models, however, is that usual geometric
	transformations of image patterns are not explicitly modelled and,
	therefore, each natural transformation of an image pattern has to
	be independently modelled using a different, rigid prototype. In
	this work, we propose a simple technique to make these rigid prototypes
	more flexible by explicit modelling of invariances to translation,
	scaling and rotation. Results are reported on a task of handwritten
	Indian digits recognition.},
  doi = {10.1007/978-3-540-72847-4_69},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSRomero2007.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@CONFERENCE{ARSaabni2009,
  author = {Raid Saabni and Jihad El-sana},
  title = {Efficient Generation of Comprehensive Database for Online Arabic
	Script Recognition},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {The difficulties in segmenting cursive words into individual characters
	have shifted the focus of handwriting recognition research from segmentation-based
	approaches to segmentation-free (holistic) methods. However, maintaining
	and training large number of prototypes (models) that represent the
	words in the dictionary make the training process extremely expensive
	and difficult in computing resources. In this paper we present an
	efficient system that automatically generates prototypes for each
	word in a given dictionary using multiple appearance of each letter
	shape. Multiple appearance allows for many permutation of shapes
	for each word and thus complicates searching for the right prototype.
	To simplify the training, reduce the maintained prototypes, and avoid
	over fitting, we used dimensionality reduction followed by clustering
	techniques to reduce the size of these sets without affecting their
	ability to represent the wide variations of the handwriting styles.
	A set of generated fonts are created by professional writers imitating
	all handwriting styles for each character in each position. These
	Fonts are used to generate all shapes for writing each word-part
	in a comprehensive dictionary. Principal component analysis and k-means
	clustering techniques are performed to select the minimal number
	of shapes representing the wide variations of handwriting styles
	for a word-part. Experimental results using an online recognition
	system proves the credibility of this process compared to manually
	generated databases.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARSaabni2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@CONFERENCE{ARSaabni2009b,
  author = {Raid Saabni and Jihad El-Sana},
  title = {Hierarchical On-line Arabic Handwriting Recognition},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {In this paper, we present a multi-level recognizer for online Arabic
	handwriting. In Arabic script (handwritten and printed), cursive
	writing – is not a style – it is an inherent part of the script.
	In addition, the connection between letters is done with almost no
	ligatures, which complicates segmenting a word into individual letters.
	In this work, we have adopted the holistic approach and avoided segmenting
	words into individual letters. To reduce the search space, we apply
	a series of filters in a hierarchicalmanner. The earlier filters
	perform light processing on a large number of candidates, and the
	later filters perform heavy processing on a small number of candidates.
	In the first filter, global features and delayed strokes patterns
	are used to reduce candidate word-part models. In the second filter,
	local features are used to guide a dynamic time warping (DTW) classification.
	The resulting k top ranked candidates are sent for shape-context
	based classifier, which determines the recognized word-part. In this
	work, we have modified the classic DTW to enable different costs
	for the different operations and control their behavior. We have
	performed several experimental tests and have received encouraging
	results.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARSaabni2009b.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@INPROCEEDINGS{DSSaabni2009,
  author = {Saabni, R. and El-Sana, J.},
  title = {Efficient Generation of Comprehensive Database for Online Arabic
	Script Recognition},
  booktitle = {10th International Conference on Document Analysis and Recognition,
	2009. ICDAR '09. },
  year = {2009},
  pages = {1231 -1235},
  month = {july},
  abstract = {The difficulties in segmenting cursive words into individual characters
	have shifted the focus of handwriting recognition research from segmentation-based
	approaches to segmentation-free (holistic) methods. However, maintaining
	and training large number of prototypes (models) that represent the
	words in the dictionary make the training process extremely expensive
	and difficult in computing resources. In this paper we present an
	efficient system that automatically generates prototypes for each
	word in a given dictionary using multiple appearance of each letter
	shape. Multiple appearance allows for many permutation of shapes
	for each word and thus complicates searching for the right prototype.
	To simplify the training, reduce the maintained prototypes, and avoid
	over fitting, we used dimensionality reduction followed by clustering
	techniques to reduce the size of these sets without affecting their
	ability to represent the wide variations of the handwriting styles.
	A set of generated fonts are created by professional writers imitating
	all handwriting styles for each character in each position. These
	fonts are used to generate all shapes for writing each word-part
	in a comprehensive dictionary. Principal component analysis and k-means
	clustering techniques are performed to select the minimal number
	of shapes representing the wide variations of handwriting styles
	for a word-part. Experimental results using an online recognition
	system proves the credibility of this process compared to manually
	generated databases.},
  doi = {10.1109/ICDAR.2009.258},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSSaabni2009.pdf:PDF},
  issn = {1520-5363},
  keywords = {Kohonen SOM;comprehensive database generation;cursive word segmentation;dictionary;dimensionality
	reduction;handwriting recognition;k-means clustering technique;machine
	learning;online Arabic script recognition;principal component analysis;segmentation-free
	method;text analysis;data mining;data reduction;handwriting recognition;image
	recognition;learning (artificial intelligence);natural languages;pattern
	clustering;principal component analysis;self-organising feature maps;text
	analysis;visual databases;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@ARTICLE{ARSadri2003,
  author = {Javad Sadri and Ching Y and Suen and Tien D. Bui},
  title = {Application of Support Vector Machines for Recognition of Handwritten
	Arabic/Persian Digits},
  journal = {2nd MVIP},
  year = {2003},
  volume = {1},
  pages = {300-307},
  abstract = {A new,method,for recognition,of isolated handwritten Arabic/Persian
	digits is presented. This method,is based,on Support Vector Machines
	(SVMs), and a new approach of feature extraction. Each digit is considered
	from four different views, and from each view 16 features are extracted
	and combined,to obtain 64 features. Using these features, multiple
	SVM classifiers are trained to separate different classes of digits.
	CENPARMI Indian (Arabic/Persian) handwritten digit database is used,for
	training and testing of SVM classifiers. Based on,this database,
	differences between Arabic and Persian digits in digit recognition
	are shown. This database provides 7390 samples for training and 3035
	samples,for testing from,the real life samples. Experiments show,that
	the proposed,features can provide,a very good,recognition result
	using Support Vector Machines at a recognition rate 94.14%, compared,with
	91.25% obtained by MLP neural network,classifier using the same features
	and test set},
  booktitle = { },
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARSadri2003.pdf:PDF},
  keywords = {Optical, (OCR), Feature Extraction, Machine Learning, Support},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@PHDTHESIS{ThesisSaid1997,
  author = {Said, Joseph Nassif},
  title = {Automatic processing of documents and bank cheques},
  school = {Concordia University.},
  year = {1997},
  note = {Thesis Advisor:Suen, Ching Y},
  abstract = {Automatic processing of documents with the purpose to scan different
	documents, recognize them, extract, and process different data items
	obtained from them could be achieved by top-down and bottom-up approaches.
	The former processes documents starting from the document class and
	ends with the pixel representation of the different items that should
	be extracted. The latter, however, processes documents in a reversed
	manner. In this thesis, a top-down formal approach for automatic
	processing of documents and bank cheques is proposed. This approach
	will view a document as a hierarchy of related items: (a) the background
	which contains simple or complex scenes that should be eliminated,
	and (b) the foreground which contains (i) base lines that must be
	removed and (ii) handwritten data, such as the date, the legal amount,
	and the courtesy amount, that should be extracted with minimum distortion.
	The novelty of this new approach is to eliminate the background,
	first, by introducing a new recursive dynamic thresholding technique
	that could be used globally or locally on a given cheque image. As
	a second step, base lines that intersect the handwritten data are
	recognized and removed with the challenge of minimizing the distortion
	on the extracted items. Two methods are proposed to tackle this difficulty.
	The first method detects the handwritten data that intersects with
	the base lines that should be eliminated and uses morphological and
	topological processing to identify and fill the gaps resulting from
	the elimination of the detected base lines. The second method proposed
	a new dynamic morphological processing technique which acts as a
	detector and a preserver of the handwritten data that intersect,
	with the base lines. The second method highly increased the efficiency
	of item extraction by more than 80% and enhanced the quality of the
	extracted items when combined with local processing techniques. In
	a step to study the reliability of the proposed top-down automatic
	item extraction system, a quantitative analysis technique is investigated
	and an experimental study is performed comparing the top-down formal
	approach with another newly developed bottom-up approach using the
	same training set of 500 real-life bank cheques and two testing sets
	of 200 bank cheques obtained from the CENPARMI database. The purpose
	of the quantitative performance analysis technique is to subject
	the extracted items of the top-down and the bottom-up approaches
	to the same item processing system that is able to recognize these
	corresponding items and provide quantitative results to indicate
	the reliability of both approaches. The experimental results showed
	that the reliability of the top-down approach on the training set.
	first testing set, and second testing set are 89.20%, 87.91%, and
	90.10% respectively while those on the bottom-up approach are 91.35%,
	91.30%, and 93.10%, respectively. Finally, in a step towards the
	construction of a highly reliable system, a feasibility study has
	been conducted by combining both approaches. The result is quite
	encouraging and a reliability of 97.09% has been achieved when these
	two systems are combined},
  booktitle = { },
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ThesisSaid1997.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSSAMOUD2008,
  author = {Fadoua BOUAFIF SAMOUD and Samia SNOUSSI MADDOURI and Haikal EL ABED
	and Noureddine ELLOUZE},
  title = {COMPARISON OF TWO HANDWRITTEN ARABIC ZONES EXTRACTION METHODS OF
	COMPLEX DOCUMENTS},
  booktitle = {The 11th International Conference on Frontiers in Handwriting Recognition},
  year = {2008},
  abstract = {This paper presents an automatic extraction of handwritten Arabic
	components of complex documents. Two methods are developed for this
	extraction. The first one is based on Mathematical Morphology (MM).
	The second one is based on Hough Transform (HT). The developed methods
	are evaluated on CENPARMI-Arabic Checks Database, in order to extract
	the handwritten components existing in the check: numerical amount,
	literal amount and date zone. We present a concept for automatic
	evaluation of the results, based on label tools for the different
	parts of used documents. We achieve a correct classification rate
	of 98% for numerical amount, 96% for literal amount, and 98% for
	date, extracted by Hough Transform method.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSSAMOUD2008.pdf:PDF},
  keywords = {Document processing, Extraction methods, Mathematical Morphology,
	Hough Transform, CENPARMIDatabase.},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSShah2008,
  author = {M. I. Shah and J. Sadri and C. Y. Suen and N. Nobile},
  title = {A New Multipurpose Comprehensive Database for Handwritten Dari Recognition},
  booktitle = {Eleventh International Conference on Frontiers in Handwriting Recognition},
  year = {2008},
  pages = {635-640},
  address = {Montreal, Canada},
  month = {August},
  abstract = {In this paper, we present the creation of the first comprehensive
	database for research and development on handwritten recognition
	of Dari language. This new handwritten database consists of many
	aspects of Dari scripts such as: handwritten isolated characters,
	isolated digits, numeral strings of various lengths, many words/terms,
	dates, and some special symbols. For each handwritten image in this
	database, very useful ground truth information is provided to facilitate
	successful recognition experiments on the images. The data has been
	archived into two different formats - Gray level and Binary. The
	contents of the database are frequently used in several kinds of
	documents such as scientific and business documents. The overall
	structure of the database has been designed in such a way to make
	it convenient for conducting recognition experiments on the handwritten
	Dari scripts.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSShah2008.pdf:PDF},
  keywords = {Optical, (OCR), Handwritten Recognition, Dari Handwritten Database,
	Dari Handwritten Recognition, Farsi and Arabic Hand-written Recognition.},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@ARTICLE{FE9Shi2002,
  author = {Meng Shi and Yoshiharu Fujisawa and Tetsushi Wakabayashi and Fumitaka
	Kimura},
  title = {Handwritten numeral recognition using gradient and curvature of gray
	scale image.},
  journal = {Pattern Recognition},
  year = {2002},
  volume = {35},
  pages = {2051-2059},
  number = {10},
  abstract = {In this paper, the authors study on the use ofgradient and curvature
	ofthe gray scale character image to improve the accuracy ofhandwritten
	numeral recognition. Three procedures, based on curvature coe2cient,
	bi-quadratic interpolation and gradient vector interpolation, are
	proposed for calculating the curvature ofthe equi-gray scale curves
	ofan input image. Then two procedures to compose a feature vector
	ofthe gradient and the curvature are described. The e2ciency ofthe
	feature vectors are tested by recognition experiments for the handwritten
	numeral database IPTP CDROM1 and NIST SD3 and SD7. The experimental
	results show the usefulness of the curvature feature and recognition
	rate of 99.49% and 98.25%, which are one ofthe highest rates ever
	reported for these databases (H. Kato et al., Technical Report ofIEICE,
	PRU95-3, 1995, p. 17; R.A. Wilkinson et al., Technical Report NISTIR
	4912, August 1992; J. Geist et al., Technical Report NISTIR 5452,
	June 1994), are achieved, respectively},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  booktitle = {Pattern Recognition},
  doi = {http://dx.doi.org/10.1016/S0031-3203(01)00203-5},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\FE9Shi2002.pdf:PDF},
  keywords = {Handwritten digits},
  owner = {TOSHIBA},
  review = {Read again .................},
  timestamp = {2009.10.03}
}

@ARTICLE{MCShieh2007,
  author = {Meng-Dar Shieh and Chih-Chieh Yang},
  title = {Multiclass SVM-RFE for product form feature selection},
  journal = {Expert Systems with Applications},
  year = {2007},
  volume = {35},
  pages = {531–541},
  abstract = {Various form features affect consumer preference regarding product
	design. It is, therefore, important that designers identify these
	critical form features to aid them in developing appealing products.
	However, the problems inherent in choosing product form features
	have not yet been intensively investigated. In this paper, an approach
	based on multiclass support vector machine recursive feature elimination
	(SVM-RFE) is proposed to streamline the selection of optimum product
	form features. First, a one-versus-one (OVO) multiclass fuzzy support
	vector machines (multiclass fuzzy SVM) model using a Gaussian kernel
	was constructed based on product samples from mobile phones. Second,
	an optimal training model parameter set was determined using two-step
	cross-validation. Finally, a multiclass SVM-RFE process was applied
	to select critical form features by either using overall ranking
	or class-specific ranking. The weight distribution of each iterative
	step can be used to analyze the relative importance of each of the
	form features. The results of our experiment show that the multiclass
	SVM-RFE process is not only very useful for identifying critical
	form features with minimum generalization errors but also can be
	used to select the smallest feature subset for building a prediction
	model with a given discrimination capability.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCShieh2007.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{DSShridhar2009,
  author = {Shridhar, M. and Houle, G.F. and Kimura, F.},
  title = {Document recognition strategies for bank cheques},
  booktitle = {IEEE International Conference on Electro/Information Technology,
	2009. },
  year = {2009},
  pages = {170 -173},
  month = {june},
  abstract = {This paper presents document recognition strategies for two important
	applications: 1) comprehensive check image reader and 2) recognition
	of text document containing multiple lines of text data. This paper
	describes the challenges in finding and recognizing the fields of
	interest on the broad document types. The remaining of the paper
	will focus on handwritten legal line recognition which remains the
	most challenging field.},
  comment = {Last edited 2 march 2010},
  doi = {10.1109/EIT.2009.5189604},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSShridhar2009.pdf:PDF},
  keywords = {bank cheques;comprehensive check image reader;handwritten legal line
	recognition;text data;text document recognition;banking;document
	image processing;image recognition;},
  owner = {Maha},
  review = {The paper present a complete check processing system. The system firstly
	segments the courtesy amounts and then uses the amount to validate
	the legal (written ) amount. The courtesy amount is Recognition of
	numeral strings is based on presegmentation followed by concurrent
	concatenation and recognition using dynamic
	
	programming optimization for the final recognition. The paper speaks
	about (cents and dollar sign part of the number ) and how to detect
	if they are in the check. It also mention that they represents a
	chalange for the system. The system does not give good comparison
	between other systems. and The final recognition rate acheived is
	85 %},
  timestamp = {2010.2.24}
}

@ARTICLE{MCSingh2005,
  author = {Sameer Singh and Maneesha Singh},
  title = {A dynamic classifier selection and combination approach to image
	region labelling},
  journal = {Signal Processing: Image Communication},
  year = {2005},
  volume = {20},
  pages = {219–231},
  abstract = {In this paper we propose a ‘bank of classifiers’ approach to image
	region labelling and evaluate dynamic classifier selection and classifier
	combination approaches against a baseline approach that works with
	a single best classifier chosen using a validation set. In this analysis,image
	segmentation,feature extraction, and classification are treated as
	three separate steps of analysis. The classifiers used are each trained
	with a different texture feature representation of training images.
	The paper proposes a new knowledge-based predictive approach based
	on estimating the Mahalanobis distance between test sample feature
	values and the corresponding probability distribution function from
	training data that selectively triggers classifiers. This approach
	is shown to perform better than probability-based classifier combination
	(all classifiers are triggered but their decisions are fused with
	combination rules),and single classifier, respectively,based on classification
	rates and confusion matrices. The experiments are performed on the
	natural scene analysis application.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCSingh2005.pdf:PDF},
  keywords = {Scene analysis; Texture analysis; Image segmentation; Classifier combination},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@CONFERENCE{ARSLIMANE2009,
  author = {Fouad SLIMANE and Rolf INGOLD and Slim KANOUN and Adel M. ALIMI and
	Jean HENNEBERT},
  title = {A New Arabic Printed Text Image Database and Evaluation Protocols},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {We report on the creation of a database composed of images of Arabic
	Printed words. The purpose of this database is the large-scale benchmarking
	of openvocabulary, multi-font, multi-size and multi-style text recognition
	systems in Arabic. The challenges that are addressed by the database
	are in the variability of the sizes, fonts and style used to generate
	the images. A focus is also given on low-resolution images where
	anti-aliasing is generating noise on the characters to recognize.
	The database is synthetically generated using a lexicon of 113’284
	words, 10 Arabic fonts, 10 font sizes and 4 font styles. The database
	contains 45’313’600 single word images totaling to more than 250
	million characters. Ground truth annotation is provided for each
	image. The database is called APTI for Arabic Printed Text Images.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARSLIMANE2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@INPROCEEDINGS{DSSolimanpour2006,
  author = {F. Solimanpour and J. Sadri and C. Y. Suen},
  title = {Standard Databases for Recognition of Handwritten Digits, Numerical
	Strings, Legal Amounts, Letters and Dates in Farsi Language,},
  booktitle = {In Proceedings of 10th International Workshop on Frontiers in Handwriting
	Recognition (IWFHR 10)},
  year = {2006},
  pages = { 743-751},
  address = { La Baule, France},
  month = {Oct},
  abstract = {This paper describes an important step towards the standardization
	of the research on Optical Character Recognition (OCR) in Farsi language.
	It describes formations of novel and standard handwritten databases
	including isolated digits, letters, numerical strings, Legal amounts
	(used for cheques), and dates. Despite conventional research and
	an Internet search, no publicly accessible Farsi database was found.
	Hence, it was decided that it would be a worthwhile academic effort
	to create several Farsi databases that could stand on their own merit
	functioning as useful tools for OCR researchers. Also, in order to
	show the potential uses of our new databases we also conducted some
	experiments on the recognition of handwritten isolated Farsi digits.},
  comment = {Last edited 2 march 2010},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSSolimanpour2006.pdf:PDF},
  keywords = {Farsi OCR, Farsi Handwritten Databases, Arabic Handwritten Databases,
	Indian Digits Database},
  owner = {Maha},
  review = {Describe the collection and processing of farsi database for handwriting
	digits, letter, legal amounts (3 words or 4 words cursive sentences
	), numeric strings (date) . The database should be avaliable from
	CENPAMRI (concordia university ). The paper also test the database
	with system from [12] ( soltanzah 2004) which is 4 projection , crossing
	and histogram features and classifier using SVM with RBF kernel.},
  timestamp = {2010.2.24}
}

@ARTICLE{ARFarah2006,
  author = {Nadir Farahand Labiba Souici and Mokhtar Sellami},
  title = {Classifiers combination and syntax analysis for Arabic literal amount
	recognition},
  journal = {Engineering Applications of Artificial Intelligence},
  year = {2006},
  volume = {19},
  pages = {29–39},
  abstract = {Automatic handwriting recognition has a variety of applications in
	real world problems, such as mail sorting and check processing. Recently,
	it has been demonstrated that combining the decisions of several
	classifiers and integrating multiple information sources can lead
	to better recognition results. This article presents an approach
	for recognizing handwritten Arabic literal (legal) amounts. The proposed
	system uses a set of holistic structural features to describe the
	words. These features are presented to three classifiers: multilayer
	neural network, k nearest neighbor, and fuzzy k nearest neighbor.
	The classification results are then combined using several schemes;
	we retained the score summation one for this work. A syntactic post-classification
	process is then carried out to find the best match among the candidate
	words. The performance of this approach is superior to the system
	which ignores all contextual information and simply relies on the
	recognition scores of the recognizers.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARFarah2006.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.08}
}

@ARTICLE{DSSouiciMeslati2004,
  author = {Labiba Souici-Meslati and Mokhtar Sellami},
  title = {A HYBRID APPROACH FOR ARABIC LITERAL AMOUNTS RECOGNITION},
  journal = {The Arabian Journal for Science and Engineering},
  year = {2004},
  volume = {29},
  pages = {177- 194},
  number = {2B},
  abstract = {The challenge of hybrid learning systems is to use the information
	provided by one source of information to compensate information missing
	from the other source. The neuro–symbolic combination represents
	a promising research way. The synergy between the symbolic (theoretical)
	and neural (empirical) approaches makes their combination more effective
	than each of them used alone. In this article, we describe an Arabic
	literal amount recognition system that uses a neuro-symbolic classifier.
	For this purpose, we first extract structural features from the words
	contained in the amounts vocabulary. Then, we build a symbolic knowledge
	base that reflects a classification of words according to their features.
	In a third step, we use a translation algorithm (from rules to neural
	network) to determine the neural network architecture and to initialize
	its connections with specific values rather than random values, as
	is the case in classical neural networks. This construction approach
	provides the network with theoretical knowledge and reduces the training
	stage, which remains necessary because of styles and writing conditions
	variability. After this empirical training stage using real examples,
	the network acquires a final topology, which allows it to recognize
	new handwritten amounts.},
  booktitle = { },
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSSouiciMeslati2004.pdf:PDF},
  keywords = {Artificial intelligence; pattern recognition; handwritten Arabic words
	recognition; hybrid systems, neuro-symbolic integration; literal
	amounts},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@ARTICLE{ARSrihari2008,
  author = {Sargur N. Srihari and Gregory R. Ball and Harish Srinivasan},
  title = {Versatile Search of Scanned Arabic Handwriting},
  journal = {LECTURE NOTES IN COMPUTER SCIENCE},
  year = {2008},
  volume = {NUMB 4768},
  pages = {57-69},
  abstract = {Searching handwritten documents is a relatively unexplored frontier
	for documents in any language. Traditional approaches use either
	image-based or text-based techniques. This paper describes a framework
	for versatile search where the query can be either text or image,
	and the retrieval method fuses text and image retrieval methods.
	A UNICODE and an image query are maintained throughout the search,
	with the results being combined by a neural network. Preliminary
	results show positive results that can be further improved by refining
	the component pieces of the framework (text transcription and image
	search).},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARSrihari2008.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{PDBSRIKANTAN1996,
  author = {GEETHA SRIKANTAN and STEPHEN W. LAM and SARGUR N. SRIHARI},
  title = {GRADIENT-BASED CONTOUR ENCODING FOR CHARACTER RECOGNITION},
  journal = {Pattern Recognition},
  year = {1996},
  volume = {29},
  pages = {1147- 1160},
  number = {7},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBSRIKANTAN1996.pdf:PDF},
  owner = {TOSHIBA},
  review = {We describe novel methods of feature extraction for recognition of
	single isolated character images. Our approach is flexible in that
	the same algorithms can be used, without modification, for feature
	extraction in a variety of OCR problems. These include handwritten,
	machine-print, grayscale, binary and low-resolution character recognition.
	We use the gradient representation as the basis for extraction of
	low-level, structural and stroke-type features. These algorithms
	require a few simple arithmetic operations per image pixel which
	makes them suitable for real-time applications. A description of
	the algorithms and experiments with several data sets are presented
	in this paper. Experimental results using artificial neural networks
	are presented. Our results demonstrate high performance of these
	features when tested on data sets distinct from the training data.},
  timestamp = {2009.10.29}
}

@ARTICLE{ARSternby2009,
  author = {Sternby, Jakob and Morwing, Jonas and Andersson, Jonas and Friberg,
	Christer},
  title = {On-line Arabic handwriting recognition with templates},
  journal = {Pattern Recognition},
  year = {2009},
  volume = {42},
  pages = {3278--3286},
  number = {12},
  abstract = {After a long period of focus on western and East Asian scripts there
	is now a general trend in the on-line handwriting recognition community
	to explore recognition of other scripts such as Arabic and various
	Indic scripts. One difficulty with the Arabic script is the number
	and position of diacritic marks associated to Arabic characters.
	This paper explores the application of a template matching scheme
	to the recognition of Arabic script with a novel algorithm for dynamically
	treating the diacritical marks. Template based systems are robust
	to conditions with scarce training data and in experiments the proposed
	system outperformed a reference system based on the promising state-of-the-art
	network technique of BLSTM. Experiments have been conducted in an
	environment similar to that of many handheld devices with promising
	results both in terms of memory consumption and response time.},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1016/j.patcog.2008.12.017},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARSternby2009.pdf:PDF},
  issn = {0031-3203},
  keywords = {On-line HWR, Arabic , Template Modeling, Branch-and-bound, Diacritic
	Graph, Trie},
  owner = {TOSHIBA},
  publisher = {Elsevier Science Inc.},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARSturgill2008,
  author = {Margaret Sturgill and Steven J. Simske},
  title = {An Optical Character Recognition Approach to Qualifying Thresholding
	Algorithms},
  booktitle = {DocEng '08: Proceeding of the eighth ACM symposium on Document engineering},
  year = {2008},
  pages = {263--266},
  address = {Sao Paulo, Brazil},
  publisher = {ACM},
  abstract = {Pre-processing for raster image based document segmentation begins
	with image thresholding, which is a binarization proces separating
	foreground from background. In this paper, we compare an existing
	(Otsu), modified existing (Kittler-Illingworth) and simple peak-based
	thresholding approach on a set of 982 documents for which existing
	ground truth (full text) is available. We use the output of an open
	source OCR engine which incorporates an adaptive/dynamic thresholder
	that can be bypassed by one of the three global thresholds we tested.
	This allowed comparison of these three approaches in the aggregate.
	We then used an independently generated dictionary as a means of
	characterizing thresholder efficacy. Such an approach, if successful,
	will provide the means for selecting an optimal thresholder in the
	absence of a large set of ground truthed documents. Our preliminary
	findings here indicate that this approach may provide a reliable
	means for thresholder comparison and eventually preclude the need
	for time-intensive human ground truthing.},
  doi = {http://doi.acm.org/10.1145/1410140.1410197},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARSturgill2008.pdf:PDF},
  isbn = {978-1-60558-081-4},
  keywords = {Threshold, OCR, Testing, Accuracy, Kittler-Illingworth, Otsu, Meta-Algorithms.},
  location = {New York, NY, USA},
  owner = {Maha},
  timestamp = {2009.10.29}
}

@ARTICLE{ARSu2009,
  author = {Tong-Hua Su and Tian-Wen Zhang and De-Jun Guan and Hu-Jie Huang},
  title = {Off-line recognitionof realisticChinesehandwritingusing segmentation-free
	strategy},
  journal = {Pattern Recognition},
  year = {2009},
  volume = {42},
  pages = {167 -- 182},
  abstract = {Great challenges are faced in the off-line recognition of realistic
	Chinese handwriting. This paper presents a segmentation-free strategy
	based on Hidden Markov Model (HMM) to handle this problem, where
	character segmentation stage is avoided prior to recognition. Handwritten
	textlines are first converted to observation sequence by sliding
	windows. Then embedded Baum–Welch algorithm is adopted to train character
	HMMs. Finally, best character string maximizing the a posteriori
	is located through Viterbi algorithm. Experiments are conducted on
	the HIT-MW database written by more than 780 writers. The results
	show the feasibility of such systems and reveal apparent complementary
	capacities between the segmentation-free systems and the segmentation-based
	ones.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARSu2009.pdf:PDF},
  keywords = {Optical, Chinese handwriting recognition, Sliding window, Hidden Markov
	Model, Segmentation-free strategy, Classifier combination},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{MCSu2007,
  author = {Yu Su and Shiguang Shan and Xilin Chen and Wen Gao},
  title = {Hierarchical Ensemble of Global and Local Classifiers for Face Recognition},
  booktitle = {IEEE 11th International Conference on Computer Vision, 2007. ICCV
	2007},
  year = {2007},
  pages = {1-8},
  address = {Rio de Janeiro,},
  month = {14-21 Oct.},
  publisher = {IEEE},
  abstract = {In the literature of psychophysics and neurophysiology, many studies
	have shown that both global and local features are crucial for face
	representation and recognition. This paper proposes a novel face
	recognition method which combines both global and local discriminative
	features. In this method, global features are extracted from whole
	face images by Fourier transform and local features are extracted
	from some spatially partitioned image patches by Gabor wavelet transform.
	After this, multiple classifiers are obtained by applying Fisher
	Discriminant Analysis on global Fourier features and local patches
	of Gabor features. All these classifiers are combined to form a hierarchical
	ensemble by sum rule. We evaluated the proposed method using Face
	Recognition Grand Challenge (FRGC) experimental protocols and database
	known as the largest data sets available. Experimental results on
	FRGC version 2.0 data set have shown that the proposed method achieves
	a verification rate of 86%, while the best reported was 76%.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCSu2007.pdf:PDF},
  keywords = {Features Extraction, Classifiers Ensemble},
  owner = {TOSHIBA},
  review = {Comments:
	
	
	Hierarchical sum of different classifier. 
	
	Fisher discriminant analysis as classification. 
	
	Local and global features ( each to different classifier)
	
	Gabor and DFT as features. 
	
	
	
	Details:
	
	
	Uses two types of features 
	
	 Global features ==> fourier transform on all the face image ( DFT
	)
	
	 local features ==> Gabor wavelet transforms transofrm on all image,
	and on N parts of the image 
	
	Fisher discriminant analsysis classifier is used seperatally on Each
	features set (DFT, Gabor for whole image + gabor on N parts )
	
	After feature extraction, we obtain N+1 feature sets, that is, one
	GFFS G and N LGFSes Li (i=1,…,N). Then, N+1 classifiers can be trained
	by applying FDA to each feature set.
	
	A simple sum rule of all classifier is used to generate the final
	decision. The hierarchical ensemble consists of two layers. In the
	first layer, N Local Component Classifiers (LCCs) CLi trained on
	Li (i=1,…,N) are combined to form a Local Ensemble Classifier (LEC)
	CL 
	
	Experimental results show that the ensemble classifier greatly outperforms
	its component classifiers which have large error diversity. Have
	achieved verification rates of from 86% to 98%. Also the Results
	of improvement of 10% over current systems. 
	
	
	Note: (compare between gabor and fourier transforms)
	
	
	 Gabor wavelet consists of a planar sinusoid multiplied by a two dimensional
	Gaussian. The sinusoid wave is activated by frequency information
	in the image. The Gaussian insures that the convolution is dominated
	by the region of the image close to the center of the wavelet. That
	is, when a signal is convolved with the Gabor wavelet, the frequency
	information near the center of the Gaussian is captured and frequency
	information far away from the center of the Gaussian has a negligible
	effect. Therefore, compared with Fourier transform which extracts
	the frequency information in the whole face region, Gabor wavelets
	only focus on some local areas of the face and extract information
	with multi-frequency and multi-orientation in these local areas.
	
	.},
  timestamp = {2009.10.29}
}

@ARTICLE{ARBasu2009,
  author = {Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri?,
	Dipak Kumar Basu},
  title = {A hierarchical approach to recognition of handwritten Bangla characters},
  journal = {Pattern Recognition},
  year = {2009},
  volume = {42},
  pages = {1467 -- 1484},
  abstract = {A novel hierarchical approach is presented here for optical character
	recognition (OCR) of handwritten Bangla words. Instead of dealing
	with isolated characters as found in selected works [T.K. Bhowmik,
	U. Bhattacharya, S.K. Parui, Recognition of Bangla handwritten characters
	using an MLP classifier based on stroke features, in: Proceedings
	of the ICONIP, Kolkata, India, 2004, pp. 814–819; K. Roy, U. Pal,
	F. Kimura, Bangla handwritten character recognition, in: Proceedings
	of the Second Indian International Conference on Artificial Intelligence
	(IICAI), 2005, pp. 431–443; S. Basu, N. Das, R. Sarkar, M. Kundu,
	M. Nasipuri, D.K. Basu, Handwritten Bangla alphabet recognition using
	an MLP based classifier, in: Proceedings of the Second National Conference
	on Computer Processing of Bangla, Dhaka, 2005, pp. 285–291; A.F.R.
	Rahman, R. Rahman, M.C. Fairhurst, Recognition of handwritten Bengali
	characters: a novel multistage approach, Pattern Recognition 35,
	2002, pp. 997–1006; U. Bhattacharya, S.K. Parui, M. Sridhar, F. Kimura,
	Twostage recognition of handwritten Bangla alphanumeric characters
	using neural classifiers, in: Proceedings of the Second Indian International
	Conference on Artificial Intelligence (IICAI), 2005, pp. 1357–1376;
	U. Bhattacharya, M. Sridhar, S.K. Parui, On recognition of handwritten
	Bangla characters, in: Proceedings of the ICVGIP-06, Lecture Notes
	in Computer Science, vol. 4338, 2006, pp. 817–828], the present approach
	segments a word image on Matra hierarchy, then recognizes the individual
	word segments and finally identifies the constituent characters of
	the word image through intelligent combination of recognition decisions
	of the associated word segments. Due to possible appearances of consecutive
	characters of Bangla words on overlapping character positions, segmentation
	of Bangla word images is not easy. For successful OCR of handwritten
	Bangla text, not only recognition but also segmentation of word images
	are important. In this respect the present hierarchical approach
	deals with both segmentation and recognition of handwritten Bangla
	word images for a complete solution to handwritten word recognition
	problem, an essential area of OCR of handwritten Bangla text. In
	dealing with certain category of word segments, created on Matra
	hierarchy, a sophisticated recognition technique, viz., two-pass
	approach [S. Basu, C. Chaudhury, M. Kundu, M. Nasipuri, D.K. Basu,
	A two pass approach to pattern classification, in: N.R. Pal et al.
	(Ed.), Lecture Notes in Computer Science, vol. 3316, ICONIP, Kolkata,
	2004, pp. 781–786] is employed here. The degree of sophistication
	of the classification technique is also rationally tuned depending
	on various categories of word segments to be recognized. For example,
	the two-pass approach is employed here for recognizing middle zone
	character segments, whereas recognition of middle zone modified shapes
	of Bangla script is done through simple template matching. Considering
	learning and generalization abilities of multi layer perceptrons
	(MLPs), MLP based pattern classifiers are used here for most of the
	classification related tasks. A powerful feature set is also designed
	under this work for recognition of complex character patterns using
	three types of topological features, viz., longest-run features,
	modified shadow features and octant-centroid features. In a nutshell,
	the work deals with a practical problem of OCR of Bangla text involving
	recognition as well as segmentation of constituent characters of
	handwritten Bangla words.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARBasu2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARSubramanian2009,
  author = {Subramanian, Krishna and Prasad, Rohit and Natarajan, Prem},
  title = {Robust named entity detection using an Arabic offline handwriting
	recognition system},
  booktitle = {AND '09: Proceedings of The Third Workshop on Analytics for Noisy
	Unstructured Text Data},
  year = {2009},
  pages = {63--68},
  address = {Barcelona, Spain},
  publisher = {ACM},
  abstract = {Text from Arabic optical handwriting recognition (OHR) systems can
	provide key indexing information. In particular, the text is rich
	in named entities (NEs) and detection of such entities is critical
	for search applications. Traditional approaches for detecting NEs
	in optical character recognition (OCR) output look for these NEs
	in the single-best recognition results. Due to the inevitable presence
	of recognition errors in the single-best output, such approaches
	usually result in low recall. Given that a lattice is more likely
	to contain the correct answer, we explore NE detection from word
	lattices produced by our Arabic handwriting recognition system. Since
	the improvement in recall is accompanied by a large number of false
	positives, we use confidence scores based on posterior scores to
	control precision. We show a 7% improvement in true detects for the
	same false acceptance rate on using lattices instead of 1-best hypothesis
	for NE lookup.},
  doi = {http://doi.acm.org/10.1145/1568296.1568308},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARSubramanian2009.pdf:PDF},
  isbn = {978-1-60558-496-6},
  location = {New York, NY, USA},
  owner = {TOSHIBA},
  timestamp = {2009.10.20}
}

@INPROCEEDINGS{DSTang2004c,
  author = {Hanshen Tang and Augustin, E. and Suen, C.Y. and Baret, O. and Cheriet,
	M.},
  title = {Spiral recognition methodology and its application for recognition
	of Chinese bank checks},
  booktitle = { Ninth International Workshop on Frontiers in Handwriting Recognition,
	2004. IWFHR-9 2004.},
  year = {2004},
  pages = { 263 - 268},
  month = {oct.},
  abstract = {This paper presents the spiral recognition methodology with its application
	in unconstrained handwritten Chinese legal amount recognition in
	a practical environment of a CheckReader trade;. This paper first
	describes the failed application of neural network - hidden Markov
	model hybrid recognizer on Chinese bank check legal amount recognition,
	and explains the reasons for the failure: the neural network - hidden
	Markov model hybrid recognizer cannot handle the complexity in the
	training for Chinese legal amounts. Then a spiral recognition methodology
	is presented. This methodology enables the system to increase its
	recognition power (both the recognition rate and the number of recognized
	characters) during the training iterations. Some experiments were
	done to show that the spiral recognition methodology has a high performance
	in the recognition of unconstrained handwritten Chinese legal amounts.
	The recognition rate at the character level is 93.5%, and the recognition
	rate at the legal amount level is 60%. Combined with the recognition
	of courtesy amount, the overall error rate is less than 1%.},
  doi = {10.1109/IWFHR.2004.96},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSTang2004c.pdf:PDF},
  issn = {1550-5235 },
  keywords = {Chinese bank checks recognition; handwritten Chinese legal amount
	recognition; hidden Markov model; neural network; spiral recognition
	methodology; bank data processing; handwritten, ; hidden Markov models;
	natural languages; neural nets;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSTang2004,
  author = {Hanshen Tang and Emmanuel Augustin and Ching Y. Suen and Olivier
	Baret and Mohamed Cheriet},
  title = {Recognition of Unconstrained Legal Amounts Handwritten on Chinese
	Bank Checks},
  booktitle = {ICPR 2004. Proceedings of the 17th International Conference on Pattern
	Recognition},
  year = {2004},
  volume = {2},
  pages = {610-613},
  address = {Los Alamitos, CA, USA},
  publisher = {IEEE Computer Society},
  abstract = {This paper presents a novel research investigation on legal amount
	recognition of unconstrained cursive handwritten Chinese character
	in the environment of A2iA CheckReader - a commercial bank check
	recognition system. The following problems and their solutions are
	described: character set of Chinese legal amounts, preprocessing
	(slant detection and correction), segmentation, feature extraction,
	grammar, automatic annotation of Chinese characters before and during
	training, and neural network hidden Markov model training and recognition.
	The system is trained with 47.8 thousand real bank checks, and validated
	with 12 thousand real bank checks. The recognition rate at the character
	level is 93.5%, and the recognition rate at the legal amount level
	is 60%. This is the first successful commercial product in this domain.},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ICPR.2004.1334322},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSTang2004.pdf:PDF},
  issn = {1051-4651},
  keywords = {A2iA CheckReader; Chinese bank checks; commercial bank check recognition
	system; feature extraction; hidden Markov model training; neural
	network training; slant detection; unconstrained cursive handwritten
	Chinese character; unconstrained legal handwritten amount recognition;
	cheque processing; feature extraction; handwritten, ; learning (artificial
	intelligence); natural languages;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@ARTICLE{FE10Teow2002,
  author = {Loo-Nin Teow and Kia Fock Loe},
  title = {Robust vision based features and classification schemes for off line
	handwritten digit recognition},
  journal = {Pattern Recognition},
  year = {2002},
  volume = {35},
  pages = {2355-2364},
  abstract = {We use well-established results in biological vision to construct
	a model for handwritten digit recognition. We show
	
	empirically that the features extractedby our model are linearly separable
	over a large training set (MNIST). Using only a
	
	linear discriminant system on these features, our model is relatively
	simple yet outperforms other models on the same data set.
	
	In particular, the best result is obtainedby applying triowise linear
	support vector machines with soft voting on vision-based
	
	features extractedfrom deslantedimages.},
  comment = {Other then in review no...},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\FE10Teow2002.pdf:PDF},
  keywords = {, Handwritten digits, MNIST},
  owner = {Maha},
  review = {My review consider the following 
	
	1) Use 16 different mask then convolve with image to use as features
	
	
	2) aggregate features using magnitude weighted average 
	
	3) classifiers ==> perception , linear svm and knn error for each
	is as following (2.1, 0.98, 0.72, 1.09)
	
	
	advantages : 1) feaster than lenet 5 2) automated features 
	
	 disadvantages : larger feature vector.},
  timestamp = {2009.10.02}
}

@ARTICLE{PDBTjan2006,
  author = {Bosco S. Tjan and Anirvan S. Nandy},
  title = {Classification images with uncertainty},
  journal = {Journal of Vision},
  year = {2006},
  volume = {6},
  pages = {387–413},
  abstract = {Classification image and other similar noise-driven linear methods
	have found increasingly wider applications in revealing psychophysical
	receptive field structures or perceptual templates. These techniques
	are relatively easy to deploy, and the results are simple to interpret.
	However, being a linear technique, the utility of the classification-image
	method is believed to be limited. Uncertainty about the target stimuli
	on the part of an observer will result in a classification image
	that is the superposition of all possible templates for all the possible
	signals. In the context of a well-established uncertainty model,
	which pools the outputs of a large set of linear frontends with a
	max operator, we show analytically, in simulations, and with human
	experiments that the effect of intrinsic uncertainty can be limited
	or even eliminated by presenting a signal at a relatively high contrast
	in a classification-image experiment. We further argue that the subimages
	from different stimulus-response categories should not be combined,
	as is conventionally done. We show that when the signal contrast
	is high, the subimages from the error trials contain a clear high-contrast
	image that is negatively correlated with the perceptual template
	associated with the presented signal, relatively unaffected by uncertainty.
	The subimages also contain a ‘‘haze’’ that is of a much lower contrast
	and is positively correlated with the superposition of all the templates
	associated with the erroneous response. In the case of spatial uncertainty,
	we show that the spatial extent of the uncertainty can be estimated
	from the classification subimages. We link intrinsic uncertainty
	to invariance and suggest that this signal-clamped classificationimage
	method will find general applications in uncovering the underlying
	representations of high-level neural and psychophysical mechanisms.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBTjan2006.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{MCToygar2004,
  author = {Onsen Toygar and Adnan Acan},
  title = {Multiple classifier implementation of a divide-and-conquer approach
	using appearance-based statistical methods for face recognition},
  journal = {Pattern Recognition Letters},
  year = {2004},
  volume = {25},
  pages = {1421–1430},
  abstract = {This paper presents a multiple classifier system for the face recognition
	problem-based on a novel divide-andconquer approach using appearance-based
	statistical methods, namely principal component analysis (PCA), linear
	discriminant analysis (LDA) and independent component analysis (ICA).
	A facial image is divided into a number of horizontal segments and
	the associated local features are extracted using a particular statistical
	method. Using a simple distance measure and an appropriate classifier
	combination method, facial images are successfully classified. The
	standard FERET database and the FERET evaluation methodology are
	used in all experimental evaluations. Computational and storage space
	efficiencies and experimental recognition performance of the proposed
	approach indicate that significant achievements are obtained compared
	to individual classifiers.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCToygar2004.pdf:PDF},
  keywords = {Appearance-based statistical methods; Multiple classifier systems;
	Classifier combination; Local feature-based face recognition},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{FE8Trier1996,
  author = {{\O}ivind Due Trier and Anil K. Jain and Torfinn Taxt},
  title = {Feature extraction methods for character recognition-A survey.},
  journal = {Pattern Recognition},
  year = {1996},
  volume = {29},
  pages = {641-662},
  number = {4},
  abstract = {This paper presents an overview of feature extraction methods for
	off-line recognition of segmented (isolated) characters. Selection
	of a feature extraction method is probably the single most important
	factor in achieving high recognition performance in character recognition
	systems. Different feature extraction methods are designed for different
	representations 6f the characters, such as solid binary characters,
	character contours, skeletons (thinned characters) or gray-level
	subimages of each individual character. The feature extraction methods
	are discussed in terms of invariance properties, reconstructability
	and expected distortions and variability of the characters. The problem
	of choosing the appropriate feature extraction method for a given
	application is also discussed. When a few promising feature extraction
	methods have been identified, they need to be evaluated experimentally
	to find the best method for the given application},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  booktitle = {Pattern Recognition},
  doi = {http://dx.doi.org/10.1016/0031-3203(95)00118-2},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\FE8Trier1996.pdf:PDF},
  keywords = {Survey, Character Recognition, Features Extraction},
  owner = {TOSHIBA},
  review = {Comments
	
	different types of extraction 
	
	old methods zoning , momment, ..
	
	In this paper, we
	
	reviewed feature extraction methods including:
	
	(1) template matching;
	
	(2) deformable templates;
	
	(3) unitary image transforms;
	
	(4) graph description;
	
	(5) projection histograms;
	
	(6) contour profiles;
	
	(7) zoning;
	
	(8) geometric moment invariants;
	
	(9) Zernike moments;
	
	(10) spline curve approximation;
	
	(11) Fourier descriptors.
	
	Each of these methods may be applied to one or more
	
	of the following representation forms:
	
	(1) gray-level character image;
	
	(2) binary character image;
	
	(3) character contour;
	
	(4) character skeleton or character graph.
	
	For each feature extraction method and each character},
  timestamp = {2009.10.03}
}

@INPROCEEDINGS{FE6Tsymbal2002,
  author = {Alexey Tsymbal and Seppo Puuronen and Mykola Pechenizkiy and Matthias
	Baumgarten and David W. Patterson},
  title = {Eigen vector-Based Feature Extraction for Classification.},
  booktitle = {Proceedings of the Fifteenth International Florida Artificial Intelligence
	Research Society Conference, May 14-16, 2002, Pensacola Beach, Florida,
	USA},
  year = {2002},
  editor = {Susan M. Haller and Gene Simmons},
  pages = {354-358},
  address = {Pensacola Beach, Florida, USA},
  month = {May 14-16},
  publisher = {AAAI Press},
  abstract = {This paper shows the importance of the use of class information in
	feature extraction for classification and inappropriateness of conventional
	PCA to feature extraction for classification. We consider two eigenvector-based
	approaches that take into account the class information. The first
	approach is parametric and optimizes the ratio of between-class variance
	to within-class variance of the transformed data. The second approach
	is a nonparametric modification of the first one based on local calculation
	of the between-class covariance matrix. We compare the two approaches
	with each other, with conventional PCA, and with plain nearest neighbor
	classification without feature extraction.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\FE6Tsymbal2002.pdf:PDF},
  isbn = {1-57735-141-X},
  keywords = {Features Extraction},
  owner = {Maha},
  review = {Comments:
	
	Mainly features Selection 
	
	
	features extraction using pca modified 
	
	Two main types ==> 1) parametric PCA 2) non Parametric PCA
	
	result compares on variasion of dataset (see table 2) 
	
	
	Not digits .},
  timestamp = {2009.10.03}
}

@ARTICLE{DSVellasques2008,
  author = {E. Vellasques and L.S. Oliveira and A.S. Britto Jr. and A.L. Koerich
	and R. Sabourin},
  title = {Filtering segmentation cuts for digit string recognition},
  journal = {Pattern Recognition},
  year = {2008},
  volume = {41},
  pages = {3044 - 3053},
  number = {10},
  abstract = {In this paper we propose a method to evaluate segmentation cuts for
	handwritten touching digits. The idea of this method is to work as
	a filter in segmentation-based recognition system. This kind of system
	usually rely on over-segmentation methods, where several segmentation
	hypotheses are created for each touching group of digits and then
	assessed by a general-purpose classifier. The novelty of the proposed
	methodology lies in the fact that unnecessary segmentation cuts can
	be identified without any attempt of classification by a general-purpose
	classifier, reducing the number of paths in a segmentation graph,
	what can consequently lead to a reduction in computational cost.
	An cost-based approach using ROC (receiver operating characteristics)
	was deployed to optimize the filter. Experimental results show that
	the filter can eliminate up to 83% of the unnecessary segmentation
	hypothesis and increase the overall performance of the system.},
  doi = {DOI: 10.1016/j.patcog.2008.03.019},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSVellasques2008.pdf:PDF},
  issn = {0031-3203},
  keywords = {Segmentation},
  owner = {Maha},
  timestamp = {2010.2.24},
  url = {http://www.sciencedirect.com/science/article/B6V14-4S50K33-2/2/11a1c56d8d56706b9a141e0d4a7285ea}
}

@ARTICLE{ARVellasques2008,
  author = {E. Vellasquesa and L.S. Oliveiraa and A.S. Britto Jr.a and A.L. Koericha
	and R. Sabourinb},
  title = {Filtering segmentation cuts for digit string recognition},
  journal = {Pattern Recognition},
  year = {2008},
  volume = {41},
  pages = {3044 -- 3053},
  abstract = {In this paper we propose a method to evaluate segmentation cuts for
	handwritten touching digits. The idea of this method is to work as
	a filter in segmentation-based recognition system. This kind of system
	usually rely on over-segmentation methods, where several segmentation
	hypotheses are created for each touching group of digits and then
	assessed by a general-purpose classifier. The novelty of the proposed
	methodology lies in the fact that unnecessary segmentation cuts can
	be identified without any attempt of classification by a general-purpose
	classifier, reducing the number of paths in a segmentation graph,
	what can consequently lead to a reduction in computational cost.
	An cost-based approach using ROC (receiver operating characteristics)
	was deployed to optimize the filter. Experimental results show that
	the filter can eliminate up to 83% of the unnecessary segmentation
	hypothesis and increase the overall performance of the system.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARVellasques2008.pdf:PDF},
  keywords = {Handwriting recognition, Segmentation, Filtering},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{ARWang2008,
  author = {Zhongjian Wang and Kenji Araki and Koji Tochinai},
  title = {Word Segmentation Method Based on Inductive Learning and Segmentation
	Rule},
  booktitle = {2008 International Symposium on Computational Intelligence and Design},
  year = {2008},
  abstract = {A word segmentation method based on Inductive Learning for non-segmented
	language uses only surface information of a character string; it
	has an advantage that is entirely not dependent on any specific language.
	The method extracts recursively a character string that occur frequently
	in text as word candidates, extracts segmentation rule with context
	information to deal with segmentation ambiguity. The method classifies
	those extracted word candidates to different ranking according to
	extraction situation, segments a text into words with extracted word
	candidates. Though proofread process erroneous segmentation was corrected,
	ranking of word candidates and segmentation rules was renewed. Evaluation
	experiments showed availability of the method for Japanese and Chinese
	word segmentation.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARWang2008.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{MCKharma2001,
  author = {Nawwaf N. Kharmaand Rabab K. Ward},
  title = { A novel invariant mapping applied to hand-written arabic character
	recognition},
  journal = {Pattern Recognition},
  year = {2001},
  volume = {34},
  pages = {2115-2120},
  abstract = {This paper describes an application of a novel mapping, one that is
	intended for use in on-line hand-written character recognition. This
	mapping produces the same output pattern regardless of the orientation,
	position, and size of the input pattern. The mapping has the advantage
	of being simple. This makes it computationally e$cient and fast,
	which in turn makes it appropriate for on-line implementations. To
	demonstrate the usefulness of this mapping, a recognition system
	utilizing it has been developed for hand-written Arabic characters.
	The performance of this system is shown to be comparable to that
	of the existing on-line Arabic character recognition systems.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCKharma2001.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@CONFERENCE{ARWshah2009,
  author = {Safwan Wshah and Zhixin Shi and Venu Govindaraju},
  title = {Segmentation of Arabic Handwriting based on both Contour and Skeleton
	Segmentation},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {We propose a new algorithm for segmentation of off-line handwritten
	Arabic words. The algorithm segments the connected letters to smaller
	segments each of which contains no more than three letters. Each
	letter may be segmented to at most five pieces. In addition to improving
	the recognition of Arabic words, another potential application of
	the proposed segmentation method is to build lexicon of small size,
	consisting of no more than three letter combinations. Generally,
	it is very hard to generate lexicon for recognition of unconstraint
	handwritten Arabic documents due to the large number of words of
	Arabic language. The algorithm has been tested on over 6300 words
	from 45 different documents written by 18 writers. The system is
	able to segment more than 93% of the words into segments, each containing
	at most one letter, 6% of the words into segments that contains two
	letters and 3% of the words into segments that contains three letters.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARWshah2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@INPROCEEDINGS{ARXiu2006,
  author = {Pingping Xiu and Liangrui Peng and Xiaoqing Ding and Hua Wang},
  title = {Offline Handwritten Arabic Character Segmentation with Probabilistic
	Model.},
  booktitle = {Document Analysis Systems VII, 7th International Workshop},
  year = {2006},
  editor = {Horst Bunke and A. Lawrence Spitz},
  pages = {402-412},
  address = {Nelson, New Zealand},
  month = {February 13-15},
  publisher = {Springer},
  abstract = {Abstract. The research on offline handwritten Arabic character recognition
	has received more and more attention in recent years, because of
	the increasing needs of Arabic document digitization. The variation
	in Arabic handwriting brings great difficulty in character segmentation
	and recognition, eg., the subparts (diacritics) of the Arabic character
	may shift away from the main part. In this paper, a new probabilistic
	segmentation model is proposed. First, a contourbased over-segmentation
	method is conducted, cutting the word image into graphemes. The graphemes
	are sorted into 3 queues, which are character main parts, sub-parts
	(diacritics) above or below main parts respectively. The confidence
	for each character is calculated by the probabilistic model, taking
	into account both of the recognizer output and the geometric confidence
	besides with logical constraint. Then, the global optimization is
	conducted to find optimal cutting path, taking weighted average of
	character confidences as objective function. Experiments on handwritten
	Arabic documents with various writing styles show the proposed method
	is effective},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1007/11669487\_36},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARXiu2006.pdf:PDF},
  isbn = {3-540-32140-3},
  owner = {TOSHIBA},
  timestamp = {2009.10.08}
}

@ARTICLE{DSXu1992,
  author = {Xu, L. and Krzyzak, A. and Suen, C.Y.},
  title = {Methods of combining multiple classifiers and their applications
	to handwriting recognition},
  journal = {IEEE Transactions on Systems, Man and Cybernetics},
  year = {1992},
  volume = {22},
  pages = {418 -435},
  number = {3},
  month = {may/jun},
  __markedentry = {[Maha]},
  abstract = {Possible solutions to the problem of combining classifiers can be
	divided into three categories according to the levels of information
	available from the various classifiers. Four approaches based on
	different methodologies are proposed for solving this problem. One
	is suitable for combining individual classifiers such as Bayesian,
	k -nearest-neighbor, and various distance classifiers. The other
	three could be used for combining any kind of individual classifiers.
	On applying these methods to combine several classifiers for recognizing
	totally unconstrained handwritten numerals, the experimental results
	show that the performance of individual classifiers can be improved
	significantly. For example, on the US zipcode database, 98.9% recognition
	with 0.90% substitution and 0.2% rejection can be obtained, as well
	as high reliability with 95% recognition, 0% substitution, and 5%
	rejection},
  doi = {10.1109/21.155943},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSXu1992.pdf:PDF},
  issn = {0018-9472},
  keywords = {Bayesian classifiers;US zipcode database;distance classifiers;handwriting
	recognition;k-nearest-neighbour classifiers;multiple classifier combination;unconstrained
	handwritten numerals;character recognition;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSXu2002,
  author = {Qizhi Xu and Jin Ho Kim and Lam, L. and Suen, C.Y.},
  title = {Recognition of handwritten month words on bank cheques},
  booktitle = {Frontiers in Handwriting Recognition, 2002. Proceedings. Eighth International
	Workshop on},
  year = {2002},
  pages = { 111 - 116},
  __markedentry = {[Maha]},
  abstract = { This paper describes an off-line system which recognizes unconstrained
	handwritten month words extracted from Canadian bank cheques. A segmentation
	based grapheme level HMM (hidden Markov model) classifier and two
	multilayer perceptron classifiers with different architectures and
	different features have been developed in CENPARMI for the recognition
	of month words. In this paper, a combination method with an effective
	conditional topology is presented, and the most widely used combination
	rules including Vote, Sum and Product, are experimented. A new modified
	Product rule is also proposed, which has produced the best recognition
	rate of 85.36% when tested on a real-life standard Canadian bank
	cheque database.},
  doi = {10.1109/IWFHR.2002.1030895},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSXu2002.pdf:PDF},
  issn = { },
  keywords = { CENPARMI; Canadian bank cheques; Hidden Markov Model; Product rule;
	grapheme level HMM classifier; handwritten month word recognition;
	image segmentation; multilayer perceptron; off-line system; pattern
	classification; bank data processing; handwritten character recognition;
	hidden Markov models; image segmentation; multilayer perceptrons;
	pattern classification;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSXu2003,
  author = {Qizhi Xu and Louisa Lam and Suen, C.Y.},
  title = {Automatic segmentation and recognition system for handwritten dates
	on Canadian bank cheques},
  booktitle = {Document Analysis and Recognition, 2003. Proceedings. Seventh International
	Conference on},
  year = {2003},
  pages = { 704 - 708},
  month = {aug.},
  __markedentry = {[Maha]},
  abstract = {This paper describes a system being developed to recognize date information
	handwritten on Canadian bank cheques. A segmentation based strategy
	is adopted in this system. In order to achieve high performances
	in terms of efficiency and reliability, a knowledge-based module
	is proposed for the date segmentation and a cursive month word recognition
	module is implemented based on a combination of classifiers. The
	interaction between the segmentation and recognition stages is properly
	established by using multihypotheses generation and evaluation modules.
	As a result, promising performance is obtained on a test set from
	a real life standard cheque database},
  comment = {Edited in 2 march 2010},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSXu2003.pdf:PDF},
  issn = { },
  owner = {Maha},
  review = {The paper presents a system that segments the date part of the cheques.
	The date is divided into (day, month, year). The month can be a word
	and year can be the full format or the shorten format ( after the
	"19" or the "20" ). The detect the year from both end of the numeric
	string using candidate then confirm strategy. The month and day is
	passed to a knowldge based module which firstly remove any seperators,
	hypens, coma. They use spatial and shape features to differentation
	between candadit separators. After that a pattern grammer is used
	to help to add knowldge based information. The grammer is based on
	rule on how a date should be written for example NSA or ASN is accepted
	but ASA is not. (where n numeric, a alpha, s sperator) . A list of
	hypotheses is generated with all accepted possible segmenation. A
	confidence is computed and the hypothesis with max confidence is
	the selected segmentation. The digit are then passed to digit recognizier
	and the month words are given to cursive word recognizier which is
	a new compination of a HMM, mlpa and mlpb system using vairous decision
	making methods. The result of the combined recognizer is improvement
	of 15% and final recognition of 87 %. The whole system recognize
	date with accuracy of 67 % for english dates and 57% for french dates.
	There is no comparison with other systems.},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSXu2001,
  author = {Qizhi Xu and Lam, L. and Suen, C.Y.},
  title = {A knowledge-based segmentation system for handwritten dates on bank
	cheques},
  booktitle = {Document Analysis and Recognition, 2001. Proceedings. Sixth International
	Conference on},
  year = {2001},
  pages = {384 -388},
  __markedentry = {[Maha]},
  abstract = {Segmenting handwritten date fields on bank cheque images into three
	subimages corresponding to the day, month and year is the first and
	critical step of our date recognition system. The paper describes
	a knowledge-based segmentation system, which introduces different
	kinds of knowledge at different segmentation stages to improve the
	performance. The knowledge includes information on the writing style,
	syntactic and semantic constraints, etc. Results have shown that
	the system is very effective compared with a previous structural
	feature based method},
  doi = {10.1109ICDAR.2001.953818},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSXu2001.pdf:PDF},
  keywords = {bank cheque images;date recognition system;handwritten date field
	segmentation;knowledge-based segmentation system;segmentation stages;semantic
	constraints;subimages;syntactic constraints;writing style;bank data
	processing;cheque processing;handwritten character recognition;image
	segmentation;knowledge based systems;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@ARTICLE{FE5Yang2002,
  author = {Jian Yang and Jing yu Yang},
  title = {Generalized K–L transform based combined feature extraction},
  journal = {Pattern Recognition},
  year = {2002},
  volume = {35},
  pages = {295-297},
  abstract = {In this paper, we combine two kinds of features together by virtue
	of complex vectors and then use the developed generalized K–L transform
	(or expansion) for feature extraction. The experiments on NUST603
	handwritten Chinese character database and CENPARMI handwritten digit
	database indicate that the proposed method can improve the recognition
	rate significantly.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\FE5Yang2002.pdf:PDF},
  keywords = {Features Extraction},
  owner = {TOSHIBA},
  review = {Comments:
	
	1) The main contribution is iin combinig 2 feature vector. 
	
	2) uses kl transform 
	
	3) feature tested (cross peripheral ) [gabor and legndre]
	
	4) baysen classifier 
	
	5) result 
	
	0.013 Error %
	
	0.036},
  timestamp = {2009.10.03}
}

@CONFERENCE{ARYin2009,
  author = {Xu-Cheng Yin and Hong-Wei Hao and Yun-Feng Tang and Jun Sun and Satoshi
	Naoi},
  title = {Rejection Strategies with Multiple Classifiers for Handwritten Character
	Recognition},
  booktitle = {10th International Conference on Document Analysis and Recognition},
  year = {2009},
  abstract = {With rejection strategies in a handwriting recognition system, we
	are able to improve the reliability and accuracy of the recognized
	characters. In this paper, we propose several rejection strategies
	with multiple classifiers for handwritten character recognition.
	First, the rejection strategy for the single classifier is introduced,
	which is composed of three stages: initial scaling, confidence measure
	calculation, and rejection performing. Then, we analyze rejection
	strategies for multiple classifiers. We divided our rejection strategies
	into two categories: (1) for voting combination; and (2) for linear
	combination with multiple classifiers. In the voting combination
	style, three rejection strategies, OR, AND, and VOTING, are proposed.
	And for the linear combination one, rejection strategies for average
	and weighted combination are analyzed respectively. We also experiment
	and compare our rejection strategies with handwritten digit recognition.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARYin2009.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.11.05}
}

@ARTICLE{PDB5Yu2004,
  author = {Lei Yu and Huan Liu},
  title = {Efficient Feature Selection via Analysis of Relevance and Redundancy.},
  journal = {Journal of Machine Learning Research},
  year = {2004},
  volume = {5},
  pages = {1205-1224},
  abstract = {Feature selection is applied to reduce the number of features in many
	applications where data has hundreds or thousands of features. Existing
	feature selection methods mainly focus on finding relevant features.
	In this paper, we show that feature relevance alone is insufficient
	for efficient feature selection of high-dimensional data. We define
	feature redundancy and propose to perform explicit redundancy analysis
	in feature selection. A new framework is introduced that decouples
	relevance analysis and redundancy analysis. We develop a correlation-based
	method for relevance and redundancy analysis, and conduct an empirical
	study of its efficiency and effectiveness comparing with representative
	methods.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://www.jmlr.org/papers/volume5/yu04a/yu04a.pdf},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB5Yu2004.pdf:PDF},
  keywords = {Feature selection},
  owner = {TOSHIBA},
  timestamp = {2009.10.03}
}

@ARTICLE{MCWang2008,
  author = {Yu-Chiang and Frank Wangand David Casasent},
  title = {New support vector-based design method for binary hierarchical classifiers
	for multi-class classification problems},
  journal = {Neural Networks},
  year = {2008},
  volume = {21},
  pages = {502–510},
  abstract = {We propose a new hierarchical design method, weighted support vector
	(WSV) k-means clustering, to design a binary hierarchical classification
	structure. This method automatically selects the classes to be separated
	at each node in the hierarchy, and allows visualization of clusters
	of highdimensional support vector data; no prior hierarchical designs
	address this. At each node in the hierarchy, we use an SVRDM (support
	vector representation and discrimination machine) classifier, which
	offers generalization and good rejection of unseen false objects
	(rejection is not achieved with the standard SVMs). We give the basis
	and new insight into why a Gaussian kernel provides good rejection.
	Recognition and rejection test results on a real IR (infrared) database
	show that our proposed method outperforms the standard one-vs-rest
	methods and the use of standard SVM classifiers.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCWang2008.pdf:PDF},
  keywords = {Automatic target recognition; Hierarchical classifier; Pattern recognition;
	Support vector machine},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{PDBYuan2005,
  author = {Quan Yuan and Ashwin Thangali and Stan Sclaroff},
  title = {Face Identification by a Cascade of Rejection Classifiers},
  booktitle = {Proceedings of the 2005 IEEE Computer Society Conference on Computer
	Vision and Pattern Recognition (CVPR’05)},
  year = {2005},
  abstract = {Nearest neighbor search is commonly employed in face recognition but
	it does not scale well to large dataset sizes. A strategy to combine
	rejection classifiers into a cascade for face identification is proposed
	in this paper. A rejection classifier for a pair of classes is defined
	to reject at least one of the classes with high confidence. These
	rejection classifiers are able to share discriminants in feature
	space and at the same time have high confidence in the rejection
	decision. In the face identification problem, it is possible that
	a pair of known individual faces are very dissimilar. It is very
	unlikely that both of them are close to an unknown face in the feature
	space. Hence, only one of them needs to be considered. Using a cascade
	structure of rejection classifiers, the scope of nearest neighbor
	search can be reduced significantly. Experiments on Face Recognition
	Grand Challenge (FRGC) version 1 data demonstrate that the proposed
	method achieves significant speed up and an accuracy comparable with
	the brute force Nearest Neighbor method. In addition, a graph cut
	based clustering technique is employed to demonstrate that the pairwise
	separability of these rejection classifiers is capable of semantic
	grouping},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBYuan2005.pdf:PDF},
  owner = {TOSHIBA},
  timestamp = {2009.10.21}
}

@MISC{PDB4Zadrozny2001,
  author = {Bianca Zadrozny},
  title = {Reducing Multiclass to Binary By Coupling Probability Estimates},
  year = {2001},
  abstract = {This paper presents a method for obtaining class membership probability
	estimates for multiclass classification problems by coupling the
	probability estimates produced by binary classifiers. This is an
	extension for arbitrary code matrices of a method due to Hastie and
	Tibshirani for pairwise coupling of probability estimates. Experimental
	results with Boosted Naive Bayes show that our method produces calibrated
	class membership probability estimates, while having similar classification
	accuracy as loss-based decoding, a method for obtaining the most
	likely class that does not generate probability estimates},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB4Zadrozny2001.pdf:PDF},
  keywords = {MultiClassifier Systems},
  owner = {TOSHIBA},
  timestamp = {2009.10.03}
}

@INPROCEEDINGS{ARZalewski2008,
  author = {Willian Zalewski and Huei Diana Lee and Adewole M.J.F. Caetano and
	Ana C. Lorena and Andr e G. Maletzke and Jo ao Jos e Fagundes and
	Cl´audio Saddy and Rodrigues Coy and Feng Chung Wu},
  title = {Evaluation of Models for the Recognition of Hadwritten Digits in
	Medical Forms},
  booktitle = {BSB '08: Proceedings of the 3rd Brazilian symposium on Bioinformatics},
  year = {2008},
  pages = {178--181},
  address = {Santo Andr\'{e}, Brazil},
  publisher = {Springer-Verlag},
  abstract = {edicine has benefited widely from the use of computational techniques,
	which are often employed in the analysis of data generated in medical
	clinics. Among the computational techniques used in these analyses
	are those from Knowledge Discovery in Databases (KDD). In order to
	apply KDD techniques in the analysis of clinical data, it is often
	necessary to map them into an adequate structured format. This paper
	presents an extension in a methodology to map medical forms into
	structured datasets, in which a sub-system for handwritten digit
	recognition is added to the overall mapping system.},
  doi = {http://dx.doi.org/10.1007/978-3-540-85557-6_19},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARZalewski2008.pdf:PDF},
  isbn = {978-3-540-85556-9},
  keywords = {Machine learning, digit recognition, medical forms.},
  location = {Berlin, Heidelberg},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@INPROCEEDINGS{DSZanchettin2006,
  author = {Zanchettin, C. and Cavalcanti, G.D.C. and Doria, R.C. and Silva,
	E.F.A. and Rabelo, J.C.B. and Bezerra, B.L.D.},
  title = {A neural architecture to identify courtesy amount delimiters},
  booktitle = {Neural Networks, 2006. IJCNN '06. International Joint Conference
	on},
  year = {2006},
  pages = {3210 -3217},
  month = {0-0 },
  abstract = {This paper deals with automatic recognition of real bank checks. A
	new approach is proposed to read the numerical amount field from
	bank checks, considering the numeric value and the different delimiters
	that might exist in that field. The proposal combines different neural
	networks classifiers to perform the recognition. Experimental results
	have shown that this approach is robust and efficient for automatic
	recognition of real Brazilian bank checks.},
  doi = {10.1109/IJCNN.2006.247306},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSZanchettin2006.pdf:PDF},
  keywords = {bank check image recognition;courtesy amount delimiter identification;neural
	network classifier;bank data processing;image recognition;neural
	net architecture;pattern classification;},
  owner = {Maha},
  review = {The system uses neural network architecutre to identify courtsey amounts.
	The system assume the digit comes segmented to the system and it
	is only concerned about classification. The features are used to
	discriminate between 10 digits and eight delimiters. The features
	are moment, concavity measurments and share represenation profile.
	The system startes by using a MLP18 network that can classify all
	18 classes after initial classification of symbol to be either digit
	or delimiter two different network are used to confirm this classification.
	One network for digits and another of delimiters. If both the inital
	classification and the second network abrove on a decission then
	the system moves to the next symbol in the courtsey amount otherwise
	it may reject the symbol and all the courtesy amounts based on some
	thresholds. The result was test on 10,871 images from a real data
	of barzillian banks. The recognition rate of the symbols has reached
	97% but the whole courtsey recognition rate is 67.8 %. Comparision
	where done between the probosed system and using a single classifier.},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSZhang2002,
  author = {Zhang, L.Q. and Suen, C.Y.},
  title = {Recognition of courtesy amounts on bank checks based on a segmentation
	approach},
  booktitle = {Frontiers in Handwriting Recognition, 2002. Proceedings. Eighth International
	Workshop on},
  year = {2002},
  pages = { 298 - 302},
  abstract = { A segmentation based courtesy amount recognition (CAR) system is
	presented in this paper. A two-stage segmentation module has been
	proposed, namely the global segmentation stage and the local segmentation
	stage. At the global segmentation stage, a courtesy amount is coarsely
	segmented into sub-images according to the spatial relationships
	of the connected components. These sub-images are then verified by
	the recognition module and the rejected sub-images are sequentially
	split using contour analysis at the local segmentation stage. Two
	neural network classifiers are combined into a recognition module.
	The isolated digit classifier divides the input patterns into ten
	numeral classes (0-9), while the holistic double zeros classifier
	recognizes the cursive and touching double zeros. Experimental results
	show that the system reads 66.5% bank checks correctly at 0% misreading
	rate.},
  doi = {10.1109/IWFHR.2002.1030926},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSZhang2002.pdf:PDF},
  issn = { },
  keywords = {CAR system; bank checks; connected components; cursive double zeros;
	global segmentation; holistic double zeros classifier; isolated digit
	classifier; local segmentation; neural network classifier combination;
	segmentation based courtesy amount recognition system; touching double
	zeros; two-stage segmentation module; bank data processing; handwritten,
	; image classification; image segmentation; neural nets;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@PHDTHESIS{MCZing2006,
  author = {Ping Zhang},
  title = {Reliable Recognition of Handwritten Digits Using A Cascade Ensemble
	Classifier System and Hybrid Features},
  school = {Concordia University},
  year = {2006},
  abstract = {In this thesis, many efforts were devoted to the recognition and verification
	of handwritten numeral recognitions. In pursuit of the highest recognition
	accuracy and the lowest misrecognition rate, we introduce a hybrid
	feature extraction strategy and a multimodal nonparametric analysis
	for feature dimensionality reduction (in order to obtain a faster
	and more stable classifier training procedure for verification).
	The design of a cascade ensemble classifier recognition system with
	rejection strategies is also introduced. From a practical perspective,
	the various recognizers and verifiers were designed and implemented
	using novel hybrid feature extraction algorithms and a newly designed
	ensemble cascade classifier system. The designed OCR engines were
	applied to handwritten numeral recognition. A summary of thesis contributions
	and discussions on future direction is also addressed.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCZing2006:PDF},
  owner = {Maha},
  timestamp = {2010.02.25}
}

@CONFERENCE{FE1Zhang2005,
  author = {P. Zhang and T.D. Bui and C. Y. Suen},
  title = {Hybrid Feature Extraction and Feature Selection for Improving Recognition
	Accuracy of Handwritten Numerals},
  booktitle = {Proceeding s of the 2005 Eight International Conference on Document
	Analysis and Recognition (ICDAR 05)},
  year = {2005},
  editor = {IEEE},
  organization = {IEEE},
  abstract = {The recognition of handwritten numerals is a challenging task in pattern
	recognition. It can be considered as one of the benchmarks in evaluating
	feature extraction methods and the performance of classifiers. In
	this paper, we propose a new method to improve the recognition accuracy
	of handwritten numerals by using hybrid feature extraction and random
	feature selection. First, we present seven feature extraction methods.
	A novel multi-class divergence criterion for large scale feature
	analysis is proposed and a random feature selection strategy is used
	to congregate three new hybrid feature sets. The new congregated
	features are complementary as they are formed from different original
	feature sets extracted by different means. Experiments conducted
	on MNIST database show that our proposed method can increase the
	recognition accuracy.},
  comment = {See the review},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\FE1Zhang2005.pdf:PDF},
  keywords = {Features Extraction, Handwritten digits},
  owner = {Maha},
  review = {Comments:
	
	7 different methods (features sets ) wavelet, MAT, Binary Gradianet
	, Median Gradient, geometrical 
	
	2 stage features selections , TS and Random FS. 
	
	Result to 99.12% on MNIST 
	
	combine classifiers can result in increase of accuracy till 99.25%},
  timestamp = {2009.10.02}
}

@ARTICLE{MCZhang2007,
  author = {Ping Zhang and Tien D. Bui and ChingY. Suen},
  title = {Anovel cascade ensemble classifier system with a high recognition
	performance on handwritten digits},
  journal = {Pattern Recognition},
  year = {2007},
  volume = {40},
  pages = {3415 – 3429},
  abstract = {This paper presents a novel cascade ensemble classifier system for
	the recognition of handwritten digits. This new system aims at attaining
	a very high recognition rate and a very high reliability at the same
	time, in other words, achieving an excellent recognition performance
	of handwritten digits. The trade-offs among recognition, error, and
	rejection rates of the new recognition system are analyzed. Three
	solutions are proposed: (i) extracting more discriminative features
	to attain a high recognition rate, (ii) using ensemble classifiers
	to suppress the error rate and (iii) employing a novel cascade system
	to enhance the recognition rate and to reduce the rejection rate.
	Based on these strategies, seven sets of discriminative features
	and three sets of random hybrid features are extracted and used in
	the different layers of the cascade recognition system. The novel
	gating networks (GNs) are used to congregate the confidence values
	of three parallel artificial neural networks (ANNs) classifiers.
	The weights of the GNs are trained by the genetic algorithms (GAs)
	to achieve the overall optimal performance. Experiments conducted
	on the MNIST handwritten numeral database are shown with encouraging
	results: a high reliability of 99.96% with minimal rejection, or
	a 99.59% correct recognition rate without rejection in the last cascade
	layer.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCZhang2007.pdf:PDF},
  keywords = {Handwritten digit recognition; Hybrid feature extraction; Cascade
	classifier system; Rejection criteria; Ensemble classifier; Gating
	networks;
	
	Neural networks; Genetic algorithms},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{MCZhang2006,
  author = {Xiaoxun Zhang and Yunde Jia},
  title = {Face recognition with local steerable phase feature},
  journal = {Pattern Recognition Letters},
  year = {2006},
  volume = {27},
  pages = {1927–1933},
  abstract = {In this paper, we propose a novel local steerable phase (LSP) feature
	extracted from the face image using steerable filters for face recognition.
	The new type of local feature is semi-invariant under common image
	deformations and distinctive enough to provide useful identity information.
	Phase information provided by steerable filters is locally stable
	with respect to scale changes, noise and brightness changes. Phase
	features from multiple scales and orientations are concatenated to
	an augmented feature vector which is used to evaluate similarity
	between face images. We use a nearest-neighbor classifier based on
	the local weighted phase-correlation for final classification. The
	experimental results on FERET dataset show an encouraging recognition
	performance.},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\MCZhang2006.pdf:PDF},
  keywords = {Steerable filters; Phase feature; Face recognition},
  owner = {TOSHIBA},
  timestamp = {2009.10.29}
}

@ARTICLE{PDB2Zhao2007,
  author = {Zhipeng Zhao and Akshay Vashist and Ahmed M. Elgammal and Ilya B.
	Muchnik and Casimir A. Kulikowski},
  title = {Combinatorial and statistical methods for part selection for object
	recognition.},
  journal = {Int. J. Comput. Math.},
  year = {2007},
  volume = {84},
  pages = {1285-1297},
  number = {9},
  abstract = {In an object recognition task where an image is represented as a constellation
	of image patches, often many patches correspond to the cluttered
	background. If such patches are used for object class recognition,
	they will adversely affect the recognition rate. In this paper, we
	present a statistical method for selecting the image patches which
	characterize the target object class and are capable of discriminating
	between the positive images containing the target objects and the
	complementary negative images. This statistical method select those
	images patches from the positive images which, when used individually,
	have the power of discriminating between the positive and negative
	images in the evaluation data. Another contribution of this paper
	is the part-based probabilistic method for object recognition. This
	Bayesian approach uses a common reference frame instead of reference
	patch to avoid the possible occlusion problem. We also explore different
	feature representation using PCA an 2D PCA. The experiment demonstrates
	our approach has outperformed most of the other known methods on
	a popular benchmark data set while approaching the best known results.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  booktitle = {Int. J. Comput. Math.},
  doi = {http://dx.doi.org/10.1080/00207160601167045},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB2Zhao2007.pdf:PDF},
  keywords = {Computer vision, Pattern representation and modeling, Object detection,
	
	Class recognition, Feature selection},
  owner = {TOSHIBA},
  timestamp = {2009.10.03}
}

@ARTICLE{PDB1Zhon2002,
  author = {Zhi-Hua Zhon and Jianxin Wu and Wei Tang},
  title = {Ensembling Neural Networks: Many Could Be Better Than All.},
  journal = {Artif. Intell.},
  year = {2002},
  volume = {137},
  pages = {239-263},
  number = {1-2},
  abstract = {Neural network ensemble is a learning paradigm where many neural networks
	are jointly used to solve a
	
	problem. In this paper, the relationship between the ensemble and
	its component neural networks is analyzed
	
	from the context of both regression and classification, which reveals
	that it may be better to ensemble many
	
	instead of all of the neural networks at hand. This result is interesting
	because at present, most approaches
	
	ensemble all the available neural networks for prediction. Then, in
	order to show that the appropriate neural
	
	networks for composing an ensemble can be effectively selected from
	a set of available neural networks, an
	
	approach named GASEN is presented. GASEN trains a number of neural
	networks at first. Then it assigns
	
	random weights to those networks and employs genetic algorithm to
	evolve the weights so that they can
	
	characterize to some extent the fitness of the neural networks in
	constituting an ensemble. Finally it selects
	
	some neural networks based on the evolved weights to make up the ensemble.
	A large empirical study shows
	
	that, comparing with some popular ensemble approaches such as Bagging
	and Boosting, GASEN can
	
	generate neural network ensembles with far smaller sizes but stronger
	generalization ability. Furthermore, in
	
	order to understand the working mechanism of GASEN, the bias-variance
	decomposition of the error is
	
	provided in this paper, which shows that the success of GASEN may
	lie in that it can significantly reduce the
	
	bias as well as the variance.},
  doi = {http://dx.doi.org/10.1016/S0004-3702(02)00190-X},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB1Zhon2002.pdf:PDF},
  institution = {National Laboratory for Novel Software Technology, Nanjing University,
	Nanjing 210093, P.R.China},
  keywords = {MultiClassifier Systems, Neural Networks, Bagging, Boosting, Bias
	- Variance Decomposition, Genetic Algorithm},
  owner = {TOSHIBA},
  timestamp = {2009.10.03}
}

@ARTICLE{DSZhou2002,
  author = {Jie Zhou and Adam Krzyzak and Ching Y. Suen},
  title = {Verification--a method of enhancing the recognizers of isolated and
	touching handwritten numerals},
  journal = {Pattern Recognition},
  year = {2002},
  volume = {35},
  pages = {1179 - 1189},
  number = {5},
  abstract = {This paper investigates verification schemes and their applications
	to the recognition of both isolated and touching handwritten numerals.
	Definitions and functionality analyses of the verifiers are given.
	The measurement of precision rate is used to assess the system reliability
	in a class-specific manner. Verification-enhanced systems are proposed
	with extensive experiments conducted on both isolated and touching
	numerals. Two databases for touching numerals are built/organized
	to serve as standard data sets. Experimental results indicate a substantial
	improvement in system precision rates by the verification scheme,
	which proves the effectiveness of the proposed systems and justifies
	the important role of verifiers in OCR systems.},
  doi = {DOI: 10.1016/S0031-3203(01)00109-1},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSZhou2002.pdf:PDF},
  issn = {0031-3203},
  keywords = {Handwritten numerals recognition},
  owner = {Maha},
  timestamp = {2010.2.24},
  url = {http://www.sciencedirect.com/science/article/B6V14-454609Y-K/2/1325f84ab8bc2015a1ff4639d7f3ccda}
}

@INPROCEEDINGS{DSZhou2001,
  author = {Jun Zhou and Suen, C.Y. and Ke Liu},
  title = {A feedback-based approach for segmenting handwritten legal amounts
	on bank cheques},
  booktitle = {Document Analysis and Recognition, 2001. Proceedings. Sixth International
	Conference on},
  year = {2001},
  pages = {887 -891},
  abstract = {The proposed feedback-based approach is implemented in two steps.
	In the first step, segmentation is done according to the structural
	features between the connected components in the legal amounts. In
	the second step, a feedback process is introduced to re-segment the
	parts that could not be identified in the first step. Then a multiple
	neural network classifier is used to verify the re-segmentation result.
	The confidence value produced by the classifier is used to determine
	the best segmentation points. This approach is tested on a CENPARMI
	database and the result indicates that the correct segmentation rate
	increased by 13.4% from the previous approach},
  doi = {10.1109/ICDAR.2001.953914},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSZhou2001.pdf:PDF},
  keywords = {CENPARMI database;bank cheques;confidence value;feedback-based approach;handwritten
	legal amounts;multiple neural network classifier;segmentation;structural
	features;cheque processing;document image processing;feedback;handwritten,
	;image classification;image segmentation;neural nets;},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{PDBZhu2006,
  author = {Qiang Zhu and Mei-Chen Yeh and Kwang-Ting Cheng and Shai Avidan},
  title = {Fast Human Detection Using a Cascade of Histograms of Oriented Gradients.},
  booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern
	Recognition (CVPR 2006)},
  year = {2006},
  pages = {1491-1498},
  address = {New York, NY, USA},
  month = { 17-22 June 2006},
  publisher = {IEEE Computer Society},
  abstract = {We integrate the cascade-of-rejectors approach with the Histograms
	of Oriented Gradients (HoG) features to achieve a fast and accurate
	human detection system. The features used in our system are HoGs
	of variable-size blocks that capture salient features of humans automatically.
	Using AdaBoost for feature selection, we identify the appropriate
	set of blocks, from a large set of possible blocks. In our system,
	we use the integral image representation and a rejection cascade
	which significantly speed up the computation. For a 320 × 280 image,
	the system can process 5 to 30 frames per second depending on the
	density in which we scan the image, while maintaining an accuracy
	level similar to existing methods.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  doi = {http://doi.ieeecomputersociety.org/10.1109/CVPR.2006.119},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDBZhu2006.pdf:PDF},
  isbn = {0-7695-2597-0},
  owner = {TOSHIBA},
  timestamp = {2009.10.21}
}

@INPROCEEDINGS{PDB3Zhu2003,
  author = {Xiaojin Zhu and Zoubin Ghahramani and John D. Lafferty},
  title = {Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions.},
  booktitle = {Machine Learning, Proceedings of the Twentieth International Conference
	(ICML 2003)},
  year = {2003},
  editor = {Tom Fawcett and Nina Mishra},
  pages = {912-919},
  address = {Washington, DC, USA},
  month = {August 21-24},
  publisher = {AAAI Press},
  abstract = {Active and semi-supervised learning are important techniques when
	labeled data are scarce. We combine the two under a Gaussian random
	field model. Labeled and unlabeled data are represented as vertices
	in a weighted graph, with edge weights encoding the similarity between
	instances. The semi-supervised learning problem is then formulated
	in terms of a Gaussian random field on this graph, the mean of which
	is characterized in terms of harmonic functions. Active learning
	is performed on top of the semisupervised learning scheme by greedily
	selecting queries from the unlabeled data to minimize the estimated
	expected classification error (risk); in the case of Gaussian fields
	the risk is efficiently computed using matrix methods. We present
	experimental results on synthetic data, handwritten digit recognition,
	and text classification tasks. The active learning scheme requires
	a much smaller number of queries to achieve high accuracy compared
	with random query selection.},
  bibsource = {DBLP,http://dblp.uni-trier.de},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\PDB3Zhu2003.pdf:PDF},
  isbn = {1-57735-189-4},
  owner = {TOSHIBA},
  timestamp = {2009.10.03}
}

@ARTICLE{ARZiaratban2008,
  author = {Majid Ziaratban and Karim Faez and Farshid Allahveiradi},
  title = {Novel Statistical Description for the Structure of Isolated Farsi/Arabic
	Handwritten Characters},
  year = {2008},
  abstract = {In this study, a structural method described with
	
	statistical features is proposed for isolated Farsi/Arabic
	
	handwritten character recognition. All the character’s dots
	
	are first removed, and the character’s body is thinned. A
	
	set of feature points are then extracted and the skeleton is
	
	decomposed into segments named primitives. Novel
	
	statistical features are extracted from each segment. Three
	
	basic characteristics behind the extracted features can
	
	accurately describe the direction and curvatures of each
	
	primitive. Since the numbers of primitives for characters
	
	are not the same, three algorithms are applied to equalize
	
	the lengths of features. Also another method is used for
	
	reducing the recognition time based on the size of the
	
	feature vectors.
	
	Experimental results show the prominence of the
	
	proposed features. The best recognition rate is obtained by
	
	using the PCA algorithm for equalizing the feature vectors
	
	that is 1.34% more than wavelet features. Our dataset
	
	includes 19118 samples. We used 11471 samples for
	
	training and the rest (7647) for test.},
  booktitle = { },
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\ARZiaratban2008.pdf:PDF},
  keywords = {Farsi handwritten, Statistical description, hybrid feature extraction.},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@INPROCEEDINGS{DSZiaratban2007,
  author = {Ziaratban, M. and Faez, K. and Ezoji, M.},
  title = {Use of Legal Amount to Confirm or Correct the Courtesy Amount on
	Farsi Bank Checks},
  booktitle = {Ninth International Conference on Document Analysis and Recognition,
	2007. ICDAR 2007. },
  year = {2007},
  volume = {2},
  pages = {1123 -1127},
  month = {sept.},
  abstract = {The study of handwritten words is tied to the development of recognition
	methods to be used in real-world applications involving handwritten
	words, such as handwritten texts, bank checks, and postal envelopes,
	among others. In this paper an approach for Farsi bank checks was
	proposed in which the legal amounts are used to aid the recognition
	results of courtesy amounts. The legal amounts which are set of some
	of 40 specified words are divided into their sub- words. Some if-then
	rules are extracted from validation set to validate the recognized
	digits. These rules are used to confirm, correct or reject the recognized
	digit. The experimental results reveal a recognition rate of 85.33%
	without the legal amounts, and a reliability rate 99.31% with a rejection
	rate of 3.67%.},
  doi = {10.1109/ICDAR.2007.4377090},
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSZiaratban2007.pdf:PDF},
  issn = {1520-5363},
  keywords = {Farsi bank checks;courtesy amount;handwritten texts;handwritten words;if-then
	rules;legal amount;recognition methods;bank data processing;handwritten,
	;},
  owner = {Maha},
  review = {The system uses the legal amount to confirm the correct recognition
	of the courtesy amount. The system first takes a bank image then
	binarize, remove background and find the courtsey amount in a pre
	defined location. The courtsey amount is split into component primitives
	by a segmentation module which atempts to segment courtesy amount
	into digits and delimiters. The legal part is divided into sub words.
	The digit are recognized using NN MLP (nearset neighbor multilayer
	percptron ) networks which are trained with staistical and structural
	features. The rate of this classifier is 98%. The legal amount is
	divided into 12 different classes of sub words (for simplicity) another
	MLP is used to correctly classify the subword into one of these classes.
	The system moves from right to left and for each digit in the legal
	amount it search for the crossponding word or subword. A set of if
	then else rule are extracted from the trained MLP to use to confirm
	or reject the classified digit. The legal amount confirmation module
	improved the recognition rate from 95% to 99% . The test was on 300
	courtesy amounts. no comparision to other systems.},
  timestamp = {2010.2.24}
}

@UNPUBLISHED{DSChan,
  title = {IMPROVING THE AUTOMATIC PROCESSING OF HANDWRITTEN BANK CHECKS},
  booktitle = { },
  file = {:D\:\\AUC\\papers\\PaperDBase\\papers\\DSChan.pdf:PDF},
  owner = {Maha},
  timestamp = {2010.2.24}
}

@comment{jabref-meta: selector_publisher:ACM;IEEE;Springer;}

@comment{jabref-meta: selector_author:Kia Fock Loe;Loo Nin Teow;P. Zha
ng;}

@comment{jabref-meta: selector_journal: Pattern Recognition;}

@comment{jabref-meta: selector_keywords:Arabic Characters;Arabic Handw
riting ;Bagging ;Bias - Variance Decomposition ;Boosting ;Character Re
cognition ;Cheque processing ;Classifier Cascade;Classifiers Ensemble;
Computer vision;Database;Feature selection;Features Extraction ;Geneti
c Algorithm ;Handwritten digits;HMM;IFN/INIT;Japanese or Chinese Chara
cters ;MNIST;MultiClassifier Systems;Neural Networks;Object detection;
OCR;Pattern Recognition ;Pattern representation and modeling;PCA ;Post
al Adress Design ;Probabilistic;Robustness design ;Survey;SVM;}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:EzzatPapers\;0\;;
1 ExplicitGroup:Important\;0\;;
1 ExplicitGroup:ReadingRelated\;2\;;
2 ExplicitGroup:ToRead\;0\;AR2Assaleha2009\;ARALEMAMI1990\;ARAbdullah2
008\;ARAbed2007\;ARAdankon2008\;ARAlamri2008\;ARAli2008\;ARAssaleh2009
\;ARBahlmann2005\;ARBall2009\;ARBasu2009\;ARBeldjehem2009\;ARBenjelil2
009\;ARBertolami2006\;ARBertolami2008\;ARBhattacharya2005\;ARBoussella
a2007\;ARBroumandnia2007\;ARBroumandnia2008\;ARCavalin2009\;ARCheriet2
009b\;ARCho2007\;ARCompt2009\;ARCowell2001\;ARDC2009\;ARDaifallah2009\
;ARDreuw2009\;ARElAbed2007\;ARElAbed2009\;ARElbaati2009\;ARFarah2006\;
ARFarooq2009\;ARFrias-Martinez2006\;ARGadat2007\;ARGolubitsky2008\;ARH
USSAIN2000\;ARHaboubi2009\;ARHachour2006\;ARHamdani2009\;ARHuang2006\;
ARICDAR2005\;ARICDAR2007\;ARJawahar2009\;ARJou2009\;ARKESSENTINI2009\;
ARKavianifar1999\;ARKherallah2009\;ARKhosravi2007\;ARKim2006\;ARKumara
2008\;ARLiu2008b\;ARLiu2009\;ARLiwicki2008\;ARLopresti2006\;ARLorigo20
06\;ARMaddouri2008\;ARMane2009\;ARMargner2006\;ARMargner2009\;ARMihov2
005\;ARMoussa2008\;ARMozaffari2006\;ARMozaffari2007\;ARNatarajan2009\;
AROUBAKER2009\;ARPal2008\;ARPechwitz2002\;ARPlamondon2006\;ARRhee2009\
;ARSLIMANE2009\;ARSaabni2009\;ARSaabni2009b\;ARSabri2006\;ARSadri2003\
;ARSrihari2008\;ARSternby2009\;ARSturgill2008\;ARSu2009\;ARSubramanian
2009\;ARVellasques2008\;ARWang2008\;ARWshah2009\;ARXiu2006\;ARYin2009\
;ARZalewski2008\;ARZiaratban2008\;ARbdAlmageed2009\;DSAgarwal1995\;DSA
lMuhtaseb2009\;DSAlamri2009\;DSChandra2008\;DSCheriet2007\;DSCheriet20
08\;DSDimauro2002\;DSDing2008\;DSElnagar2003\;DSFarah2005\;DSFarah2006
\;DSFeritas2000\;DSGorski1999\;DSGorski2001\;DSGu2006\;DSGuillevic1994
\;DSKapp2007\;DSKim2001\;DSKoch2005\;DSKornai1995\;DSLam1995\;DSLecce2
000\;DSLou2008\;DSMadasu2005\;DSMadasu2005b\;DSMarisa2002\;DSMarisa200
2b\;DSMelegy2007\;DSMello2007\;DSMorita2001\;DSMuntean2007\;DSNeves200
8\;DSOliveira2002\;DSPal2009\;DSPalacios2003\;DSPalacios2008\;DSRomero
2007\;DSSAMOUD2008\;DSSaabni2009\;DSSantos2002\;DSShah2008\;DSSouiciMe
slati2004\;DSTang2004\;DSTang2004c\;DSVellasques2008\;DSZanchettin2006
\;DSZhang2002\;DSZhou2001\;DSZhou2002\;DShonggang2005\;FEAbdullah2007\
;MCCao95\;MCGutta1997\;MCHotta2009\;MCHung2004\;MCKharma2001\;MCKussul
2005\;MCLecun1998a\;MCLecun1998b\;MCLee1999\;MCOliveira2007\;MCOuyang2
009\;MCRodriguez2002\;MCShieh2007\;MCSingh2005\;MCToygar2004\;MCWang20
08\;MCZhang2006\;MCZhang2007\;PBAlaei2009\;PBPan2009\;PDB10Bhattachary
a2003\;PDB12Prevost2003\;PDB13Morita2003\;PDB14Hamamura2003\;PDB15Golf
arelli1997\;PDB17Domeniconi2001\;PDB18Masuyama2002\;PDB19Madeed2004\;P
DB1Zhon2002\;PDB2Zhao2007\;PDB3Zhu2003\;PDB5Yu2004\;PDB6AlOhali2003\;P
DB7Chen2004\;PDB8AlOmari2004\;PDB9SVM1998\;PDBAthitsos2005\;PDBBritto2
004\;PDBChen2005\;PDBEid2007\;PDBFavat1994\;PDBFu2006\;PDBFumera2003\;
PDBGorgevik2004\;PDBHuang2004\;PDBKaczmar2005\;PDBLi2003\;PDBLiu2004\;
PDBLuo2005\;PDBMozaffari2004\;PDBMozaffari2005\;PDBNunes2004\;PDBPong2
006\;PDBTjan2006\;PDBYuan2005\;PDBZhu2006\;ThBurrow2004\;;
3 ExplicitGroup:Printed\;0\;;
3 ExplicitGroup:ToReadAgain\;0\;FE10Teow2002\;FE9Shi2002\;;
2 ExplicitGroup:ForFutureRefrenceOnly\;0\;DSChan\;MCZing2006\;PDB11He2
001\;PDB16Pudil1992\;PDB4Zadrozny2001\;ThesisAlOhali2002\;ThesisSaid19
97\;;
2 ExplicitGroup:HaveRead\;0\;AR3AlMuhtaseb2008\;AR4Awaidah2009\;ARAbde
lRaouf2008\;ARAbdulKader2008\;ARAburas2008\;ARBelaid2008\;ARBouchain20
06\;ARCheriet2007\;ARCheriet2009\;ARFujisawa2008\;ARMello2008\;DSBezer
ra2008\;DSChandra2008\;DSHussein1999\;DSMuntean2007\;DSShridhar2009\;D
SSolimanpour2006\;DSXu2003\;DSZiaratban2007\;FE11Hamamoto\;FE1Zhang200
5\;FE2Gader1996\;FE3Lauer2007\;FE4Liu2003\;FE5Yang2002\;FE6Tsymbal2002
\;FE7Oliveira2002\;FE8Trier1996\;MCChen2009\;MCSu2007\;PDBSRIKANTAN199
6\;;
1 ExplicitGroup:SubjectORTitleRelated\;0\;;
2 ExplicitGroup:Survey Or Tutorials Or Comparisons\;0\;;
2 ExplicitGroup:Features For OCR\;0\;;
2 ExplicitGroup:Latin Handwritten Digit Recognition\;0\;;
2 ExplicitGroup:Arabic and Persian Handwritten Digit Recognition\;0\;;
2 ExplicitGroup:Pairwise Classification\;0\;;
2 ExplicitGroup:Multi-Category Classification\;0\;;
2 ExplicitGroup:SVM\;0\;;
2 ExplicitGroup:Features Selection\;0\;;
2 ExplicitGroup:Multiclassifier Systems\;0\;;
2 ExplicitGroup:Thresholding and Rejection\;0\;;
2 ExplicitGroup:Object Detection\;0\;;
2 ExplicitGroup:Decision Trees\;0\;;
2 ExplicitGroup:Classification Cascades\;0\;;
2 ExplicitGroup:UnSorted\;0\;;
1 ExplicitGroup:BasedOnKeyword\;0\;;
2 KeywordGroup:Character Recognition\;0\;keywords\;Character Recogniti
on\;0\;0\;;
2 KeywordGroup:Feature Extraction\;0\;keywords\;Features Extraction\;0
\;0\;;
}

